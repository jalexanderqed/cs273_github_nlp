My rocksdb use over than 50G memory and I have no idea how much it will use. My machine ram is 64GB, so I have to use crontab to kill the process every few hours.
- data size: 280GB
- number of keys: 1,628,635,615
- most of key's length is between 30 and 80 bytes

rocksdb options

```
  options_.compression = rocksdb::kSnappyCompression;
  options_.create_if_missing        = false;

  options_.IncreaseParallelism();
  options_.OptimizeLevelStyleCompaction();

  options_.target_file_size_base    = (size_t)128 * 1024 * 1024;
  options_.max_bytes_for_level_base = (size_t)1024 * 1024 * 1024;
  options_.num_levels = 6;

  options_.write_buffer_size = (size_t)128 * 1024 * 1024;
  options_.max_write_buffer_number  = 8;

  // initialize BlockBasedTableOptions
  auto cache1 = rocksdb::NewLRUCache(1 * 1024 * 1024 * 1024LL);
  auto cache2 = rocksdb::NewLRUCache(1 * 1024 * 1024 * 1024LL);
  rocksdb::BlockBasedTableOptions bbt_opts;
  bbt_opts.block_cache = cache1;
  bbt_opts.block_cache_compressed = cache2;
  bbt_opts.cache_index_and_filter_blocks = 0;
  bbt_opts.block_size = 32 * 1024;
  bbt_opts.format_version = 2;
  options_.table_factory.reset(rocksdb::NewBlockBasedTableFactory(bbt_opts));
```

---

For testing, I stop write data, rocksdb is readonly now, after millions of read, the status is below. The process use is over than 20G memory.

```
// db_->GetProperty("rocksdb.estimate-table-readers-mem", &out);
index/filter: 724,702,240 

// db_->GetProperty("rocksdb.cur-size-all-mem-tables", &out);
memtable: 390
```

