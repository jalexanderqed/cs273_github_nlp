RocksDB is an amazing storage engine. We are impressed by the performance of RocksDB. However, We meet some unexpected phenomenons of RocksDB. The version of RocksDB is v2.7, and the latest commit hash value of the version is: 91c01485d1c0a4675e24b39ad2adb950a3a468e6
The phenomenons are described following:
1. Firstly, we call NewIterator to get a DBIter of a columnfamily
2. Then, we use the DBIter to scan all the k/v of the columnfamily
3. During the scan, we find that the DBiter falls into the dead cycle when we call DBIter->Next()

We analyze the data in sst. Some k/v in the same ssts are **OUT-OF-ORDER**. It is absolutely  unexpected!!!  **OUT-OF-ORDER** SSTs make the database can't be retrieved correctly since RocksDB use the binary search in SST. Dead cycle of the DBIter->Next() is also explicable if the ssts are disorder Since the we will replace the db_iter->Next( ) by the db_->Seek( ) in DBIter::FindNextUserEntryInternal if the num_skipped > max_skip_, Seek will use the binary search, it is a undefined behavior!!! 

We analyze the source code of RocksDB(v2.7). We conclude that there are three places will create new ssts. 
1. Recovery the WAL, 
2. Flush memtable 
3. The result of the compaction

We add some Log in these places to check the key/value added into the new ssts are disorder or not. We find that 1 and 2 will not generate disorder ssts, and 3 will generate disorder ssts. It is reasonable since ssts generated from 1 and 2 are come from skiplist however the input ssts of compaction are already disorder!

So, where is the source of the disorder sst!!! We can exclude the ssts generated by memtable flush since the memtable is organized as the skiplist. Thus, the only possible is that the input of the compaction may disorder!!! One possibility is that the ssts in level > 0 may overlap. 

I find a suspicious place which may make the ssts overlap in level > 0. 

DBImpl::WriteLevel0Table

```
  1507   int level = 0;
  1508   if (s.ok() && meta.fd.GetFileSize() > 0) {
  1509     const Slice min_user_key = meta.smallest.user_key();
  1510     const Slice max_user_key = meta.largest.user_key();
  1511     // if we have more than 1 background thread, then we cannot
  1512     // insert files directly into higher levels because some other
  1513     // threads could be concurrently producing compacted files for
  1514     // that key range.
  1515     if (base != nullptr && options_.max_background_compactions <= 1 &&
  1516         cfd->options()->compaction_style == kCompactionStyleLevel) {
  1517       level = base->PickLevelForMemTableOutput(min_user_key, max_user_key);
  1518     }
  1519     edit->AddFile(level, meta.fd.GetNumber(), meta.fd.GetPathId(),
  1520                   meta.fd.GetFileSize(), meta.smallest, meta.largest,
  1521                   meta.smallest_seqno, meta.largest_seqno);
  1522   }
```

We can conclude that the output level of the new sst generated from memtable may be put to a level higher than level 0. It seems ok since according the the PickLevelForMemTableOutput, there is no files overlapped with [meta.smallest, meta.largest] in level [0, level( > 0)] of the version of the columnfamilydata  pointed by the **base**. However, the **base** is **_not the version**_ we really apply the new generated sst! The actual version we apply the sst may has some sst which is overlapped with the generated sst! Which will make the rule of the lsm **FALSE**: the ssts in level > 0 are not overlapped with each other!
If the compaction pick the overlapped ssts in the same level(> 0) to process the compaction, it may output the disorder ssts!

