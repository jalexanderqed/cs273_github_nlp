We are using rocksdb(0.4.2) in our project. Currently we met some 'core dump' problem. Here is some information about it.
### The stack in error file

```
Stack: [0x00007fdedd6ff000,0x00007fdedd800000],  sp=0x00007fdedd7fdbb8,  free space=1018k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  0x00007fde00000001
C  [librocksdbjni7623879195517844425..so+0x22d6ee]  rocksdb::BlockIter::PrefixSeek(rocksdb::Slice const&, unsigned int*)+0x2e
C  [librocksdbjni7623879195517844425..so+0x22d791]  rocksdb::BlockIter::Seek(rocksdb::Slice const&)+0x61
C  [librocksdbjni7623879195517844425..so+0x228db0]  rocksdb::BlockBasedTable::Get(rocksdb::ReadOptions const&, rocksdb::Slice const&, rocksdb::GetContext*)+0x1d0
C  [librocksdbjni7623879195517844425..so+0x1e667b]  rocksdb::TableCache::Get(rocksdb::ReadOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, rocksdb::Slice const&, rocksdb::GetContext*, rocksdb::HistogramImpl*)+0x38b
C  [librocksdbjni7623879195517844425..so+0x1f57f0]  rocksdb::Version::Get(rocksdb::ReadOptions const&, rocksdb::LookupKey const&, std::string*, rocksdb::Status*, rocksdb::MergeContext*, bool*)+0x6a0
C  [librocksdbjni7623879195517844425..so+0x196cdd]  rocksdb::DBImpl::GetImpl(rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, std::string*, bool*)+0x60d
C  [librocksdbjni7623879195517844425..so+0x196ea9]  rocksdb::DBImpl::Get(rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, std::string*)+0x19
C  [librocksdbjni7623879195517844425..so+0x2c1610]  rocksdb::DBWithTTLImpl::Get(rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, std::string*)+0x20
C  [librocksdbjni7623879195517844425..so+0x17181a]  rocksdb::DB::Get(rocksdb::ReadOptions const&, rocksdb::Slice const&, std::string*)+0x4a
C  [librocksdbjni7623879195517844425..so+0x1485f1]  rocksdb_get_helper(JNIEnv_*, rocksdb::DB*, rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle*, _jbyteArray*, int)+0x1b1
C  [librocksdbjni7623879195517844425..so+0x14871f]  Java_org_rocksdb_RocksDB_get__J_3BI+0x3f
J 805  org.rocksdb.RocksDB.get(J[BI)[B (0 bytes) @ 0x00007fdf20da7673 [0x00007fdf20da7600+0x73]

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
J 805  org.rocksdb.RocksDB.get(J[BI)[B (0 bytes) @ 0x00007fdf20da7644 [0x00007fdf20da7600+0x44]
J 1562 C2 com.alibaba.yarn.blink.runtime.reader.StreamInputProcessor.processInput(Lorg/apache/flink/streaming/api/operators/OneInputStreamOperator;Ljava/util/concurrent/locks/Lock;)Z (325 bytes) @ 0x00007fdf20f5fdac [0x00007fdf20f5eea0+0xf0c]
J 889% C2 com.alibaba.yarn.blink.runtime.tasks.OneInputStreamTask.run()V (42 bytes) @ 0x00007fdf20d89168 [0x00007fdf20d89040+0x128]
j  org.apache.flink.streaming.runtime.tasks.StreamTask.invoke()V+336
j  com.alibaba.yarn.blink.runtime.taskexecutor.Task.run()V+537
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
```
### The options we use

```
Options opt = new Options();
opt.setUseFsync(false);
opt.setDisableDataSync(true);
opt.setCreateIfMissing(true);
opt.setMergeOperator(new StringAppendOperator());
BlockBasedTableConfig table_options = new BlockBasedTableConfig();
table_options.setBlockSize(32 * SizeUnit.KB);
table_options.setBlockCacheSize(DEFAULT_BLOCK_CACHE_SIZE * SizeUnit.MB);
table_options.setFilter(new BloomFilter(DEFAULT_BLOOM_FILTER_BITS, false));
table_options.setCacheIndexAndFilterBlocks(true);
table_options.setIndexType(IndexType.kHashSearch);
table_options.setWholeKeyFiltering(false);
opt.setMemTableConfig(new HashLinkedListMemTableConfig());
opt.useFixedLengthPrefixExtractor(Integer.SIZE / Byte.SIZE * 2);
opt.setMemtablePrefixBloomBits(10000000);
opt.setMemtablePrefixBloomProbes(6);
opt.setTableFormatConfig(table_options);
opt.createStatistics();
opt.setWriteBufferSize(32 * SizeUnit.MB);
opt.setTargetFileSizeBase(64 * SizeUnit.MB);
opt.setMaxWriteBufferNumber(4);
opt.setAllowOsBuffer(true);
opt.setMaxOpenFiles(-1);
opt.setMaxBackgroundFlushes(2);
opt.setMaxBackgroundCompactions(2);
opt.setCompactionStyle(CompactionStyle.LEVEL);
opt.setLevelZeroFileNumCompactionTrigger(4);
opt.setLevelZeroSlowdownWritesTrigger(20);
opt.setLevelZeroStopWritesTrigger(30);
opt.setNumLevels(4);
opt.setMaxBytesForLevelBase(64 * 4 * SizeUnit.MB);
```
- We only use put and get method, something like this

```
TtlDB db = TtlDB.open(opt, rocksDbPath.getAbsolutePath(),(int)TimeUnit.DAYS.toSeconds(DEFAULT_TTL_DAY_NUMBER), false);
db.put(writeOptions, key, value);
byte[] valueBytes = db.get(key);
```
### The situation is
- When we set opt.setCacheIndexAndFilterBlocks(false), there will be no problem
- When we not set prefix, there will be no problem(We need prefix in some other situation, such as seeking)

```
//table_options.setIndexType(IndexType.kHashSearch);
//table_options.setWholeKeyFiltering(false);
//opt.setMemTableConfig(new HashLinkedListMemTableConfig());
//opt.useFixedLengthPrefixExtractor(Integer.SIZE / Byte.SIZE * 2);
//opt.setMemtablePrefixBloomBits(10000000);
//opt.setMemtablePrefixBloomProbes(6);
```
- But when we put index into block cache and use prefix at the same time, it will crash after processing some data. I guess it may crash when index is not in block cache? We also tested in 4.5.1 version, but it still not work.

