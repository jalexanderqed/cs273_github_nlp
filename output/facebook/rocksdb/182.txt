Currently I was running the benchmark for RocksDB on SSD. The dataset is about 110G, every record size is about 1KB and operation ratio is 50% reading 50% writing. 

There is a situation always happning, at first the throughput will over ten thousand and then fall to several thousand even several hundred, at that time, I notice that the read latency will increase significantly, and I think this is because of compaction happening from reading the DB log. And this unstable situation will happen again and again.

I would like to get some suggestions for how to keep our throughput stable and make the performance optimized. Oh , I benchmark them using 32 read threads and 4 write threads. Here is my configuration as below:
 Options.block_cache_size : 48 GB
 Options.block_size : 32 KB
 Options.write_buffer_size : 1024MB
 Options.compression : 1
 Options.max_open_files: 500000
 Options.target_file_size_base : 512MB
 Options.max_bytes_for_level_base : 1024MB
 Options.level0_file_num_compaction_trigger: 4
 Options.level0_slowdown_writes_trigger: 16
 Options.level0_stop_writes_trigger: 64
 Options.disableDataSync: 0
 Options.max_background_compactions: 8
 Options.max_write_buffer_number: 16
 Options.rate_limit_delay_max_milliseconds: 1000
 Options.arena_block_size:12800KB
 Options.delete_obsolete_files_period_micros: 300000000
 Options.disable_seek_compaction: 0
 Options.min_write_buffer_number_to_merge: 4
 Options.max_background_flushes: 2

Thanks for your any comments!

