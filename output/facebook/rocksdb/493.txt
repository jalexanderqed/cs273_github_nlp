I'm working with data of around a terabyte. I can now efficiently load this into rocksdb using configuration as described in #490. The happy result is that the process is now bottlenecked at compaction.

The data load can run at nearly 100M/s, but I notice that compaction runs at only 10M/s. I have a lot of free cores, and at least 200M/s of available write, so I was wondering if there is a way to improve throughput.

In principle the db could be compacted in chunks in parallel, to reduce the number of input SSTs to the final merge-sort at the cost of a lot more compute and IO (I wouldn't mind either as I'm stuck waiting on iterations of this process). However, it's not clear that this gets at the underlying problem.

How can I improve compaction performance? Are there any settings in options which could be used, or is this a deeper limitation? What are the dominant factors in compaction runtime?

