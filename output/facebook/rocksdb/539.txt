Looking at the [official benchmarks](https://github.com/facebook/rocksdb/wiki/Read-Modify-Write-Benchmarks) for the merge operator, it seems that some important parameters and some details are missing.
## Issue with key size

Although not mentioned on the page, by looking at the code, my conclusion is that you were using 16 byte keys. Since you are using a 64bit uniform [I believe] random number generator, the are two problems:
1. Your keys always look like an 8byte random number followed by 8 bytes of zeros, B1...B800000000. This seems unnecessary. You could simply use an 8byte key.
2. More importantly, the possibility of collision over the course of 50M keys is very very small. Therefore, I am not sure how many of the merge statements actually caused two values to be merged as the probability of producing the same key is so small! So, does this mean that the 20 micros/ops difference can simply be the time it took the "update" benchmark to do the read before the write operation? Am I missing something here?
3. If my point regarding lack [or infrequency] of key collision is valid, I would suggest re-writing the key generator.
## Issue with reads after merge (or lack thereof)

I am still learning about the implementation details of RocksDB, so I might be wrong on this one. Let's assume that [the above mentioned issue is resolved and] the random number generator actually produces same keys often, which means the benchmarks will in fact perform "Read-Modify-Write" and merges using the merge operator. In the "update" benchmark, when we do a RMW operation, a future read on the same key would be quite cheap as it does not incur the cost of merging two values. However, in the "merge" benchmark, a `db.merge(k,v)` statement may not necessarily cause an immediate merge of two values. In fact, it is possible that a read will have to pay this cost. Alternatively, depending on when and how compaction is scheduled to happen, a future write of a different key may be stalled. I would suggest that it is important to improve both experiments by adding a read phase to each of them. This will help understand the performance impact of the merge operator on future read operations. For example, assuming a total of N entries for the "merge" benchmark, it should look like

```
for i in 1 to N
   generate random key
   generate random value
   db.merge(key, value)
end for
for i in 1 to N
   generate random key
   db.get(key, value)
end for
```
## Misc

Lastly, it would be great if the RMW benchmarks were executed on the same hardware config as the other official [performance benchmarks](https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks). This would make it easier to compare the times.

