We are having around 16 rocksdb databases with size of 200GB each.
Iterating through all the dbs in a loop is consuming nearly 8GB of memory which is much more.

For better clarity, we had conducted a test by creating a db with 1 record,
by connecting and iterating through db for 1million times in a loop, we observed memory usage has been increased linearly.
Please find the attached piece of code.

``` c
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <assert.h>

#include "rocksdb/c.h"

const char DBPath[] = "/tmp/test.db";

void test_db() {
  rocksdb_t *db;
  rocksdb_options_t *options = rocksdb_options_create();

  // open DB
  char *err = NULL;
  db = rocksdb_open_for_read_only(options, DBPath, 0, &err);
  assert(!err);

  rocksdb_readoptions_t *readoptions = rocksdb_readoptions_create();
  rocksdb_iterator_t *it = rocksdb_create_iterator(db, readoptions);
  rocksdb_iter_seek_to_first(it);
  rocksdb_iter_next(it);

  // cleanup
  rocksdb_readoptions_destroy(readoptions);
  rocksdb_options_destroy(options);
  rocksdb_close(db);
}

int main(int argc, char **argv) {
  int i;

  for (i=0; i<1000000L; i++) {
      test_db();
      printf("\r%d", i); 
  }

  return 0;
}
```

DB: https://www.dropbox.com/s/ujdrwtjojs4q9w5/test_db.zip?dl=1

Environment used:
OS - Linux
Distribution - Ubuntu 14.04.3 LTS
rocksdb version - 3.10

Expected memory leak from **rocksdb_iter_next(it);**
Observations:
Out of 1million for 100k (i.e connecting to db and doing iter_next) times alone the script consumed about 225MB of memory.
- What might be the issue and how can we overcome the same?

