My rocksdb use over than 50G memory and I have no idea how much it will use. My machine ram is 64GB, so I have to use crontab to kill the process every few hours.
- data size: 280GB
- number of keys: 1,628,635,615
- most of key's length is between 30 and 80 bytes

rocksdb options

```
  options_.compression = rocksdb::kSnappyCompression;
  options_.create_if_missing        = false;

  options_.IncreaseParallelism();
  options_.OptimizeLevelStyleCompaction();

  options_.target_file_size_base    = (size_t)128 * 1024 * 1024;
  options_.max_bytes_for_level_base = (size_t)1024 * 1024 * 1024;
  options_.num_levels = 6;

  options_.write_buffer_size = (size_t)128 * 1024 * 1024;
  options_.max_write_buffer_number  = 8;

  // initialize BlockBasedTableOptions
  auto cache1 = rocksdb::NewLRUCache(1 * 1024 * 1024 * 1024LL);
  auto cache2 = rocksdb::NewLRUCache(1 * 1024 * 1024 * 1024LL);
  rocksdb::BlockBasedTableOptions bbt_opts;
  bbt_opts.block_cache = cache1;
  bbt_opts.block_cache_compressed = cache2;
  bbt_opts.cache_index_and_filter_blocks = 0;
  bbt_opts.block_size = 32 * 1024;
  bbt_opts.format_version = 2;
  options_.table_factory.reset(rocksdb::NewBlockBasedTableFactory(bbt_opts));
```

---

For testing, I stop write data, rocksdb is readonly now, after millions of read, the status is below. The process use is over than 20G memory.

```
// db_->GetProperty("rocksdb.estimate-table-readers-mem", &out);
index/filter: 724,702,240 

// db_->GetProperty("rocksdb.cur-size-all-mem-tables", &out);
memtable: 390
```

Based on your config and stats, I don't think RocksDB should use more than 3GB. Are you sure that memory usage doesn't come from somewhere else? If you're using jemalloc you can see where your process is using memory by heap profiling: https://github.com/jemalloc/jemalloc/wiki/Use-Case:-Heap-Profiling

BTW can you try disabling `block_cache_compressed`? We don't use that option anywhere (AFAIK) so it might be untested.

ok, I will disabling `block_cache_compressed` and try to using memory by heap profiling.

I am using google tcmalloc and the pdf is generated by google-pprof.

[36.pdf](https://github.com/facebook/rocksdb/files/144013/36.pdf)

The profile only showed 2GB.

Yeah, the profile that you attached shows 2GB: 1GB on block cache, 500MB on indexes and 300MB on memtables. This is all as expected.

Here is another profile when run for a while:

```
Total MB: 10364.8
Focusing on: 10364.8
Dropped nodes with <= 51.8 abs(MB) Dropped edges with <= 10.4 MB

Snappy_Uncompress (inline) 4986.8 (48.1%)
rocksdb Arena AllocateNewBlock 3669.4 (35.4%) of 3671.4 (35.4%)
rocksdb NewArenaWrappedDbIterator 546.7 (5.3%)
rocksdb Block NewIterator 357.3  (3.4%)
```

[652.pdf](https://github.com/facebook/rocksdb/files/146532/652.pdf)

Is the new pfile 652.pdf with block_cache_compressed switched off?

> Is the new pfile 652.pdf with block_cache_compressed switched off?

@dhruba  I think it's switched off. Now just keep `bbt_opts.block_cache`. 

```
  auto cache1 = rocksdb::NewLRUCache(1 * 1024 * 1024 * 1024LL);
  rocksdb::BlockBasedTableOptions bbt_opts;
  bbt_opts.block_cache = cache1;
  bbt_opts.cache_index_and_filter_blocks = 0;
  bbt_opts.block_size = 32 * 1024;
  bbt_opts.format_version = 2;
  options_.table_factory.reset(rocksdb::NewBlockBasedTableFactory(bbt_opts));
```

Tomorrow I will try to call `MallocExtension::instance()->ReleaseFreeMemory();` to make sure tcmalloc release no-longer-used memory back to the kernel.

@bitkevin NewArenaWrappedDbIterator is taking significant amount of memory -- you probably never delete iterators once you're done using them, which means you're leaking memory.

I agree with @igorcanadi . How probably kept lots of iterators open. All of them will pin down some blocks from being released. If they pin down more memory than block cache capacity, we'll still allow you. We are adding an option to fail the request, instead of using more memory than user specified.

Thanks a lot! 
You guys are right, I have made a mistake which left lots of iterators open. Thank you guys, help me a lot.

