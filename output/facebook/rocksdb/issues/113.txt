Hey guys, 

I am trying to do a simple fillrandom test and can't seem to be achieving high write rates. The commands I use are

```
r=1000000000; t=24; bpl=1073741824;del=0;levels=4;ctrig=8; delay=12; stop=16; wbn=30; mbc=2; mb=67108864;wbs=15728640; dds=1; sync=0;  ks=20; vs=200; bs=65536; cs=21474836480; of=20000; si=100000;

./db_bench --benchmarks=fillrandom --disable_seek_compaction=1 --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t --key_size=$ks --value_size=$vs --block_size=$bs --cache_size=$cs --bloom_bits=10 --cache_numshardbits=6 --open_files=$of --verify_checksum=1 --db=./rdb-test --sync=$sync --disable_wal=1 --compression_type=none --stats_interval=$si --disable_data_sync=$dds --write_buffer_size=$wbs --target_file_size_base=$mb --max_write_buffer_number=$wbn --max_background_compactions=$mbc --level0_file_num_compaction_trigger=$ctrig --level0_slowdown_writes_trigger=$delay --level0_stop_writes_trigger=$stop --num_levels=$levels --delete_obsolete_files_period_micros=$del --stats_per_interval=1 --max_bytes_for_level_base=$bpl --memtablerep=vector --use_existing_db=0 --disable_auto_compactions=0 --max_grandparent_overlap_factor=10  --source_compaction_factor=10000000 --max_background_flushes=1
```

At a high level, I am working with 1B records (20 byte keys, 200 byte values), with 1GB in level 0, with 64MB files. 20Gb cache. 

The intention is not a bulk load kind of test, so I would like to keep compaction level based and more closer to online workloads.

I suspect am doing something incorrect with compactor triggering. Any ideas?

Sample stats output

```
2014/04/09-17:07:56  ... thread 10: (100000,300000) ops and (1015.9,984.2) ops/second in (98.430843,304.800916) seconds
                               Compactions
Level  Files Size(MB) Score Time(sec)  Read(MB) Write(MB)    Rn(MB)  Rnp1(MB)  Wnew(MB) RW-Amplify Read(MB/s) Write(MB/s)      Rn     Rnp1     Wnp1     NewW    Count   msComp   msStall  Ln-stall Stall-cnt
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0        4       58   0.1        16         0      1564         0         0      1564        0.0       0.0        98.8        0        0        0        0      108      145       0.0       0.0         0
  1       17     1045   1.0        84      9661      9652      1506      8155      1497       12.8     115.1       115.0      104      132      156       24       13     5992       0.0       0.0         0
  2        7      452   0.0         5       581       581       258       323       258        4.5     118.4       118.3        4        5        9        4        4      981       0.0       0.0         0
Uptime(secs): 304.8 total, 0.1 interval
Writes cumulative: 7209256 total, 580006 batches, 12.4 per batch, 1.51 ingest GB
WAL cumulative: 0 WAL writes, 0 WAL syncs, 0.00 writes per sync, 0.00 GB written
Compaction IO cumulative (GB): 1.51 new, 10.00 read, 11.52 write, 21.52 read+write
Compaction IO cumulative (MB/sec): 5.1 new, 33.6 read, 38.7 write, 72.3 read+write
Amplification cumulative: 7.6 write, 14.2 compaction
Writes interval: 2422 total, 198 batches, 12.2 per batch, 0.5 ingest MB
WAL interval: 0 WAL writes, 0 WAL syncs, 0.00 writes per sync, 0.00 MB written
Compaction IO interval (MB): 0.52 new, 0.00 read, 0.00 write, 0.00 read+write
Compaction IO interval (MB/sec): 4.9 new, 0.0 read, 0.0 write, 0.0 read+write
Amplification interval: 0.0 write, 0.0 compaction
Stalls(secs): 0.000 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 0.000 leveln_slowdown
Stalls(count): 0 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 0 leveln_slowdown
```

cc @ljinfb who is our performance expert

Do you know if you are cpu bound or io bound? are u running on a single disk or multiple disks? flash drives?

A casual glance showed that Max background threads  = 2 which might be too low to drive all the io to the device. A larger number (say 8) might make some difference.

There are no RocksDB stalls listed here:

```
Stalls(secs): 0.000 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 0.000 leveln_slowdown
Stalls(count): 0 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 0 leveln_slowdown
```

So it looks like compaction is keeping up.

Hey Dhruba, 

These configs are basically what we discussed over email a couple months back. 
Flash drives. Right now, I am neither. :) I have plenty of CPU and IOPS. 

In the meantime, I kicked off a 

```
wbs=268435456; r=1000000000; t=24; ks=20; vs=800; mb=67108864; cs=21474836480; of=20000; ./db_bench --benchmarks=fillrandom --num=$r --threads=$t --key_size=$ks  --value_size=$vs --cache_size=$cs --bloom_bits=10 --open_files=$of --db=./rdb-data --write_buffer_size=$wbs --use_existing_db=0 --target_file_size_base=$mb
```

and this seems much better. IO close to maxing out . and the default for max_background_compactions seems to be 1. So I am not sure either.  

From my experiences with other log structured storage engines, compaction tuning is the hardest part. So, do you guys have any pointers to code/docs (besides Options) to understand all the tuning knobs? May be I need to do some reading and come back to you guys

You may want to increase write buffer size, 16M is a bit small comparing to your L1 file size. Ideally, you may want to make the total size of L0 (ctrig=8 \* 16M = 128M) equal or larger than total L1 size (1G in your setting). Also increasing # of compaction threads will probably help as well.

@ljinfb Thanks. Will try that. 

In fact, the difference between the runs seems like the following additional params

```
bpl=1073741824;del=0;levels=4;ctrig=8; delay=12; stop=16; wbn=30; mbc=2; wbs=15728640; dds=1; sync=0; bs=65536;
```

and as you mentioned the increase in wbs to 256MB or so. 

Any code/pointers to understanding all these knobs would be great. I would like to do some digging and get back to you guys..

From your first post, I think wbn=30 is too large and wbs=15MB was too small. L0 file size \* ctrig should be about the size of the L1  (--target_file_size_base) so increase that if you increase wbs. When wbs is too small then your LSM with leveled compaction will have more levels leading to more write amplification and less write throughput. On pure flash hosts I have used between 128M and 1G for the L1 size.

Note that ctrig is --level0_file_num_compaction_trigger and "L0 file size" is approximately --write_buffer_size

@vinothechandar, If I am not mistaken, you are doing 24 threads with ~10k/s write per thread, that is about 240k/s write rate, with ~50M/s throughput. That is more or less inline with the numbers reported on wiki. Improving write performance is one of our goals for this quarter.

@mdcallag Let me redo my run and get back to you. Thanks for the explanation. 
@ljinfb I think its 1k per thread? 

```
(1015.9,984.2) ops/second in (98.430843,304.800916)
```

I did verify from sar though that it was not writing that much.. It seems like I need more digging into the compaction knobs.. you can close this for now and I will reopen again when I hit some issues

