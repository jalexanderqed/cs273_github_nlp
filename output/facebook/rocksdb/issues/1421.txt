Hi all!

My config is:
RocksDB version: 4.11.2
Has DB with few columns (3-5)
App have 3 different DB.
OS: Windows 7

I'm now got facing on issue, when  app is running for about a day and then stop and start, it is going into long DB::Open time. After first restart, the second restarting going definitely faster (don't delay at all).

```
    snmp_int.exe!rocksdb::DBImpl::RecoverLogFiles(const std::vector<unsigned __int64,std::allocator<unsigned __int64> > & log_numbers, unsigned __int64 * next_sequence, bool read_only) Line 1584  C++
    snmp_int.exe!rocksdb::DBImpl::Recover(const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, bool read_only, bool error_if_log_file_exist, bool error_if_data_exists_in_logs) Line 1312 C++
    snmp_int.exe!rocksdb::DB::Open(const rocksdb::DBOptions & db_options, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & dbname, const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, std::vector<rocksdb::ColumnFamilyHandle *,std::allocator<rocksdb::ColumnFamilyHandle *> > * handles, rocksdb::DB * * dbptr) Line 5717 C++
    snmp_int.exe!rocksdb::DBWithTTL::Open(const rocksdb::DBOptions & db_options, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & dbname, const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, std::vector<rocksdb::ColumnFamilyHandle *,std::allocator<rocksdb::ColumnFamilyHandle *> > * handles, rocksdb::DBWithTTL * * dbptr, std::vector<int,std::allocator<int> > ttls, bool read_only) Line 101    C++
```

see attach to full stack (stack get many times) :
[stack.txt](https://github.com/facebook/rocksdb/files/549607/stack.txt)
The option, set to db configure is:
[OPTIONS-000189.txt](https://github.com/facebook/rocksdb/files/549609/OPTIONS-000189.txt)
The log file from one of many db is:
[LOG.old.txt](https://github.com/facebook/rocksdb/files/549610/LOG.old.txt)

The last lines in LOG that db after delay a trigger compaction:

```
2016/10/25-10:47:29.335231 4c78 [1] Compaction start summary: Base version 14 Base level 0, inputs: [173(174KB) 163(1186B) 154(1027B)], [150(955KB)]
2016/10/25-10:47:29.335231 4c78 EVENT_LOG_v1 {"time_micros": 1477367249335231, "job": 3, "event": "compaction_started", "files_L0": [173, 163, 154], "files_L1": [150], "score": 1, "input_data_size": 1159024}
2016/10/25-10:47:30.489801 4c78 [1] [JOB 3] Generated table #180: 39542 keys, 1176579 bytes
2016/10/25-10:47:30.489801 4c78 EVENT_LOG_v1 {"time_micros": 1477367250489801, "cf_name": "1", "job": 3, "event": "table_file_creation", "file_number": 180, "file_size": 1176579, "table_properties": {"data_size": 1156720, "index_size": 32944, "filter_size": 0, "raw_key_size": 709132, "raw_average_key_size": 17, "raw_value_size": 4042366, "raw_average_value_size": 102, "num_data_blocks": 1149, "num_entries": 39542, "filter_policy_name": "", "kDeletedKeys": "0", "kMergeOperands": "0"}}
2016/10/25-10:47:30.490808 4c78 [1] [JOB 3] Compacted 3@0 + 1@1 files to L1 => 1176579 bytes
2016/10/25-10:47:30.494834 4c78 (Original Log Time 2016/10/25-10:47:30.494834) [1] compacted to: base level 1 max bytes base 268435456 files[1 1 0 0 0 0 0] max score 0.25, MB/sec: 1.0 rd, 1.0 wr, level 1, files in(3, 1) out(1) MB in(0.2, 0.9) out(1.1), read-write-amplify(12.9) write-amplify(6.5) OK, records in: 6567, records dropped: 0
2016/10/25-10:47:30.494834 4c78 (Original Log Time 2016/10/25-10:47:30.494834) EVENT_LOG_v1 {"time_micros": 1477367250494834, "job": 3, "event": "compaction_finished", "compaction_time_micros": 1154570, "output_level": 1, "num_output_files": 1, "total_output_size": 1176579, "num_input_records": 39542, "num_output_records": 39542, "num_subcompactions": 1, "lsm_state": [1, 1, 0, 0, 0, 0, 0]}
2016/10/25-10:47:30.496847 4c78 [DEBUG] [JOB 3] Delete var/db/r_data/1/000173.sst type=2 #173 -- OK

```

My question is how to improve db open performance, and maybe I'm need to reduce some options (see attach) to prevent long db open?

In your case, you have multiple column families and probably at least one of them is seldom updated. In DB restart, most of the time was spent on replaying the log files. Set options.max_total_wal_size to reduce the size of the logs. Some value like 1GB or 2GB will be a good value.

Thank you for your answer.
Unfortunately, this solution did not help to improve the startup time.
start time is left on the old values:
first start: (59 seconds)

```
2016/10/27-09:16:10.831540 342c RocksDB version: 4.11.2
2016/10/27-09:17:09.696540 4df0 EVENT_LOG_v1 {"time_micros": 1477534629696540, "job": 3, "event": "table_file_deletion", "file_number": 287}

```

second start: (less than one second)

```
2016/10/27-09:24:21.095540 87c RocksDB version: 4.11.2
2016/10/27-09:24:21.284540 87c DB pointer 000000000288F7F0

```

log of first start:
[LOG_data.txt](https://github.com/facebook/rocksdb/files/554866/LOG_data.txt)
log of second start (after restart): 
[LOG_data_2.txt](https://github.com/facebook/rocksdb/files/554867/LOG_data_2.txt)

most of the time of the first start spent here:

```
2016/10/27-09:16:10.861540 342c Recovering log #309 mode 2
2016/10/27-09:16:54.898540 342c Creating manifest 316

```

App CPU eating on the work time is less than 4%:
![sysmonproccpu-20161027t02-58-30](https://cloud.githubusercontent.com/assets/9152787/19753054/15364398-9c2c-11e6-918d-739d087ce5d0.png)

May be I'm need to change code somewhere else?

Hi! Can someone help me?

