RocksDB has a very big number of different configuration options that we added to make it flexible and suitable for various workloads and environments (in-memory, SSD, point lookups, etc). Configuring all these options to get the most out of RocksDB is not easy and requires very high familiarity with RocksDB internals.

On the other hand, RocksDB's default options are inherited from LevelDB and not suitable for any high-performance server workload.

We should invest some time in making RocksDB's options easier to configure and understand. For starting proposals, see: https://github.com/facebook/rocksdb/wiki/Proposals-on-Improving-Rocksdb's-Options

We're welcoming all contributions and thoughts in this effort.

Hi, @igorcanadi ... thanks for opening this issue!

The options issue is definitely a reality. Currently, in our use of RocksDB at my startup, we have about 20-25 lines of options being set (we're using the C API). And yes, it took reading through a lot of RocksDB's internals to understand the majority of them.

Here's what I'm thinking ...

While there's no way we can make it possible to have things pre-configured for all use cases, I think maybe having some pre-configured "building blocks" for different use cases would be great. Now, the question is, how can this be approached and how do we figure out the appropriate settings for these building blocks? Well, I'm thinking maybe we could reach out to the community already using RocksDB and learn how they're using it.

Another possible route is maybe having a tool, which takes some input parameters about an environment and potential workload, and returns suggestions on what might need configuring and what their values should be (I think this was suggested in the wiki).

\- Jonathan

+1 for simplifying RocksDB options. Currently, its very hard to configure rocksDB due to large number of options. Couple of suggestions which might help in improving understanding of these options:
1. Add documentation for universal compaction and explanation of its options.
2. If possible, add documentation for each option which describes how can performance be potentially affected by changing the option. If its not possible for individual option, then we can add this doc for combination of options.
3. If possible, add documentation for "how to configure RocksDB" which can contain explanation of which options to configure by looking at the log files.

@ankgup87 Agreed. I also think that if it proves too difficult to change or modify the existing APIs in order to simplify them, at least having some great documentation would help. Currently, there is a fair amount of documentation in the wiki, but it's actually quite lacking in practicality. I've found myself reading more source code than I ever intended. Having some documentation around every option would be great. I'm willing to help with some documentation based on my knowledge and understanding of RocksDB so far.

\- Jonathan

Hi, 

for me it is more important that users get a better understanding of the option itself.
When I initially started with rocksdb it was hard to find out which options should be set to squeeze out more performance from rocksdb according to my scenario.

Here is a rough list of thoughts I had at the beginning:
- How many levels should I choose
- How big should a individual level be
- Is it better to have more and possible smaller levels, or less levels having bigger size
- Level or Universal compaction. 
- What is a good 'target_file_size_base' and 'target_file_size_multiplier'
- When do I need bloom_filters. Do they make only sense with 'prefix_extractor'
- When is the cache filled (for example the block_cache is also filled on 'write')
- What is a good value for 'max_background_compactions'
- Setting allow_us_buffer = false but having big caches (to avoid a another process stealing my buffer cache). However #111  prevents this anyway :)

No need to answer the questions here. My point is that a configuration guide and documentation would help a lot. At the start I often looked at the 'benchmark' pages to get an impression how good configuration looks like.

I like the idea of having 'building blocks for typical scenarios': 
A scenario description could look like:
- rocksdb-version
- Type of operations (put, get, del, merge, ....)
- (Rough) numbers how ops/sec, separated by operation type
- Keysize
- Valuesize
- Size of the whole database
- IO-System (Memory only, SSD, Spinning Disk)
- Of course the options itself, but only that one different from the default.
  Maybe such a description could be semi-automatically generated by parsing the current log file ?

Having an easier API is of course a good thing, but from my point of view it has not the highest priority.
Take for example the "OptimizeLevelStyleCompaction" call. Sure, it makes configuring 20 lines less, but the logic/meaning could also be documented. It is the practical experience which makes the call so worthy.
To be honest, I prefer putting the 'raw' options into my code. So a reader of the code (including me) could directly see all the options as a whole. If I would only call 'OptimizedLevelStyleCompaction' a reader has to go to the rocksdb source code to understand what options are set. However this may be specific to me. Other developers like it to call one method and then magically everything is faster, without caring much about the details.

Regards,
 Stephan

Thanks for the great discussion guys.

I agree with @stephan-hof that writing a good documentation about configuration options is the most important first step. We can also have some examples of good configurations for different workloads that clients can easily c/p if they want effortless setup. I'll try to write something up in next few weeks.

@stephan-hof :+1:.

@igorcanadi I'd love to see what you write up :smile:! Thanks! Like I said, if you need any help, I'd love to help in whatever way possible. Thanks again.

\- Jonathan

@stephan-hof  I agree with u !
@igorcanadi  Actually I am doing some benchmark on Rocksdb now, and confused and tested so many configurations, I would like to see the output documents which explains the option parameters, that would help the basic workloads as well. I want to help if you need any help. Thx again

Hey guys! I have drafted some parts of the Tuning guide: https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide

There is obviously much more work to be done, but would appreciate your feedback on structure and content.

@igorcanadi

Hey! Checking it out now!
Thanks!

\- Jonathan

Would you guys consider donating your RocksDB options and workload for "Example Configuration" section? I expect this section to be the most useful when new users try to make sense of the options.

Finally I found some time to respond.  First of all, thanks for the new page.
I would like to start by describing how I'm using rocksdb and then just the options. 
- running on spinning disks. Usually 2 x 7200K disks in a RAID-1
- rocksdb version 3.1
- key size: ~30 bytes
- value size: 800 KB - 2 MB
- number of records: ~10.000.000
- number of writes: Writes happen randomly over the entire dataset.
  - there is a steady rate of 200-400 writes per second
  - very rarely a 20 minutes burst of 2000-3000 writes per second
- number of GETs:
  - there is a stead rate of 5-10 random GETs per second.
  - once in a while (once a day) there is a bulk GET of 30000 entries. Which is not related to bursts of writes above. => Not happening at the same time.

A write itself works like this: If a key is inserted initially, a PUT with the
entire size is done. All further updates are done with a 'Merge'. The size of a
'Merge' is typically 1-2K. So the main load are small random writes over a big dataset.

I know the number of reads/writes is not so impressive, but this comes from the
application and my goal was to use less disk IO as possible.
Before rocksdb I just used flat files on ext4 and these random writes killed the
IO-system. Now with rocksdb the IO-wait dropped to 3% (hourly average). Even during the 20 minutes bursts, IO-wait is under 10%.

Important for these numbers. I measured 'Merge' only, because PUTs
happening mainly during the beginning of a database lifetime. So I ignored them
for benchmarks.

The random GET performance is a disk benchmark only, matching my expectation.
My only fear is here the heavy usage of "Merge". To get a entry all the 'Merges', possibly
spread over many levels, need to be combined. Leading to more disk seeks.
I guess this is the tradeoff for the fantastic write performance. 

Later this year I will play around with universal compaction, in the hope that
the same key from several files is compacted at once. Hence a single GET doesn't has to read too many files. Another test would be less but bigger levels. Also the block_size is currently tuned for write performance. I'm planning here tests with a smaller size.
However I'm currently happy with the performance and saw no need to spend more
time on benchmarking/tuning. So for now I'm working with the following settings:

``` cpp

rocksdb::Options options;
options.create_if_missing = true;

options.max_open_files = 1000000;

options.filter_policy = rocksdb::NewBloomFilterPolicy(10);
options.block_cache = rocksdb::NewLRUCache(1073741825);
options.block_size = 64 * 1024;

options.write_buffer_size = 64 * 1024 * 1024;
options.max_write_buffer_number = 7;

options.target_file_size_base = 64 * 1024 * 1024;
options.target_file_size_multiplier = 1;

options.max_bytes_for_level_base = 512 * 1024 * 1024;
options.max_bytes_for_level_multiplier = 4;

options.num_levels = 10;
options.max_bytes_for_level_multiplier_additional.assign({1, 1, 1, 1, 1, 1, 1, 1, 1, 1});

options.max_background_flushes = 1;
options.max_background_compactions = 3;

options.max_log_file_size = 50 * 1024 * 1024;
options.keep_log_file_num = 25;

options.merge_operator.reset(new MyMerge());
rocksdb::DB::Open(options, this->dbpath, &this->db);

this->db->GetEnv()->SetBackgroundThreads(4, rocksdb::Env::LOW);
this->db->GetEnv()->SetBackgroundThreads(1, rocksdb::Env::HIGH);
```

If I have a dramatic change to the options I will report again.

To contribute to the documentation I'm pasting here my private notes made for the log file.
I was tired to look every time in the code to understand the meaning of 'Compactions' inside the log file.
You can reuse/correct them if you want and add them to a wiki page. If something is completely stupid feel free to edit my post directly here, before somebody picks up a wrong description. 
- Files: Number of files in this level
- Size: Size of uncompressed data
- Score: (current level size) / (max level size) => If score is greater than 1 it is a candidate for compaction
- Time: Home many seconds spend in compacting this level
- Read: Total bytes read during compaction between levels N and N+1
- Write: Total bytes written during compaction between levels N and N+1
- Rn: Bytes read from level N during compaction between levels N and N+1
- Rnp1: Bytes read from level N+1 during compaction between levels N and N+1
- Wnew: (Rnp1 - Write): New bytes written in this level
- RW-Amplify: Ratio of how many data has been read to write new data. So if this number is big, a lot of data was read during compaction. However the effective data written was small. Smaller is better. Everything under 60 is OK
- Read (MB/s): Read speed of the current IO-system
- Write (MB/s): Write speed of the current IO-system
- Rn: Files read from level N during compaction between levels N and N+1
- Rnp1: Files read from level N+1 during compaction between levels N and N+1
- Wnp1: Files written during compaction between levels N and N+1
- NewW: (Wnp1 - Rnp1) New files creating during compaction within in this level
- Count: Number of compactions done
- msCom: Average duration of a single compaction (in milliseconds)
- msStall: Average duration of a single stall (in milliseconds)
- Ln-stall: Total time (in milliseconds) of stalls. In this time writes to that level were blocked.
- Stall-cnt: Number of stalls triggered in this level. A stall is triggered if the the number of files within a certain level reach its maximum. => Writes are forbidden until a compaction runs.

Regards, Stephan

@stephan-hof thanks for donating your configuration! That's a great idea of explaining our Compaction Stats output. I added the explanations to the Guide.

How does one go about setting these options?

``` c++
options.filter_policy = rocksdb::NewBloomFilterPolicy(10);
options.block_cache = rocksdb::NewLRUCache(1073741825);
```

The API has changed and they are now attached to BlockBasedTableOptions.

But you can't just do:

``` c++
rocksdb::BlockBasedTableOptions topt;
topt.filter_policy = rocksdb::NewBloomFilterPolicy(10);
topt.block_cache = rocksdb::NewLRUCache(block_cache_size, 7);
options.table_factory.reset(NewBlockBasedTableFactory(topt));
```

It seems this configuration has become much more complex, and there is no obvious place where it has been documented.

Thank you for reporting this issue and appreciate your patience. We've  notified the core team for an update on this issue. We're looking for a response within the next 30 days or the issue may be closed.

