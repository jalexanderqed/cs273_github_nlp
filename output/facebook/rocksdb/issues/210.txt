Like many people suffers, write amplification becomes our bottleneck in some senarios.

_It’s a long post_
#### Backgroud

We're using LevelDB on a box with 700GB SSD(24 CPUs, 128G RAM). It's pre-shared to 16 smaller LevelDBs. Currently, total data size is about 160GB. Data size of every shard is about 10GB, occupied L0 ~ L3. Write-amp is between 8-10.
Write throughput from application is about 12MB/s (4K ops, avg value size 3K). Because of write-amp, DISK IO write throughput is 100MB/s, read throughput is 40MB/s(mainly from compaction). DISK is very very busy right now.
Write-amp will increase as the data growth to level larger than L3. The entire DISK situation will be worser.
#### First Trial

I tried many ways to reduce write amplication, including #19. Thanks to the RocksDB, I adjusted many parameter of compaction of Level Sytle Compaction, and also tried Universal Style Compaction. Write-amp is still not satisfied.
Our test on Universal Sytle Compaction: because it only picks up a few files that are chronologically adjacent, theses files probably don't have too much overlap. So the compaction may be not highly-efficient.

Here I want to skip over many details, since these tests cannot reduce write-amp dramaticly.
#### Finaly Trial

I switched back to Level Style Compaction again. Write-amp becomes larger after data flushed to level higher than 1. So I set the max level to 2 (means only L0 and L1) and split total data input many small Column Families to control the data size of L0 and L1.

Here are the details:
Assume total data size is 512GB, `R`(=1) RocksDB, pre-splited to `N`(=128) CFs, 4GB per CF, L0 size is set to `SL0`(=2GB), L1 size `SL1` will expected to be 2GB.
max_total_wal_size is set to `W`(=4GB), file size in L0 is expected to `W/N`(=32MB), file number in L0 will be `SL0/(W/N)` (=SL0*N/W=64MB).

**Write-amp**: 2~3 (1 + 1 + X). 1 for WAL, 1 for memtable to L0, X for L0 to L1. if we set size of L0 larger than L1, X will less than 1, which sacrified space-amp. So write-amp can be **limited and predictable** to **2~3**, if we tune those parameters carefully.
**Read-amp**: mainly depends on the file num of L0.
**Space-amp**: >2, can be tuned. It can be about 2, if we set total data size of L0 is actual data size. But it’s hard to estimate and adjust real-time.

I write a demo after the idea. Bulk load 100M items with avg item size 2K.
Then start to measure and write 200M items randomly.
Write throughput from demo is 400GB(200M writes \* 2K), DISK READ is 424GB, DISK WRITE is 1070GB. write-amp is about 2.5.

A good news is RocksDB supports to write WAL seperatly, which I'll write to a HDD on production environment. It will also reduce work load on SSD.

As you can see, parameters R, N, SL0, W can be tuned to precisely control write-amp, which will scarify read-amp and space-amp.
#### Problem

Here are the problems I've met.

After restart, RocksDB need to recover from 128 rolling WALs to all CFs.

**Problem A**: it may cost several minutes because there’re several GBs of WALs.

**Problem B**: L0 file num in every CF will burst to a number larger than `level0_file_num_compaction_trigger`, which I set to 64. Then RocksDB starts to compact L0 and L1 of all CFs, which is about hundreds of GBs, and RocksDB slow down all write operations during a very long time.

This may be because I use too many CFs.
#### Candidate Solutions

RocksDB is still my best choice.

**Problem A**:
1. I can trigger a Flush of all memtable before restart, expected to reduce the WAL need to recover next restart. I do find db->Flush in the API.
   So the problem is not so big.
   Maybe there are other bad situations, the program doesn’t have a chance to Flush before restart, i.e. segment fault, but they’re rarely happened after the program is stable.
2. reduce `max_total_wal_size` or adjust other parameters

**Problem B**:

I want to try a new compaction strategy. Maybe called Mixed Style Compaction.
1. Try to compact small files in L0 which are smaller than target_file_size_base, and, we can pick up as many as we can, expected to generate a file larger than target_file_size_base. Then new file is put to L0 again. This step should control every file in L0 is compacted only once.
2. Compact all files in L0 with L1, only when total data size of L0 exceeds our limit.

Brief idea is, try to merge small files from L0 to L0, if total file number in L0 exceed our limit. But if total data size of L0 exceeds our limit, do a compaction from L0 to L1.
There still remain many more ideas in step 1 to control the write-amp.

After that, I realized that this is much like the minor and major compaction of HBase.
#### Any Comments?
1. Does anyone have the same idea? May be a plan or even mature solution?
2. Does the Mixed Style Compaction have any bad case, which I haven’t catched?

I planed to test it by myself if no better solution is available.

Thanks for your patient to reach the last word. Any comment is welcome and appreciate.

Another potential issue is, if db starts from empty, all CFs may need to do L0 to L1 compact at the same time, which is resource consuming. So background compaction number need to be tuned, and give L0 to L0 compaction more priority.

@allenlz This is a great post, thanks!

When running with lots of column families you will see lots of small WALs (since we roll the WAL as soon as a single column family does the flush). With bunch of small WALs, on recovery, we will recover each WAL into a single L0 table file. This will trigger `level0_file_num_compaction_trigger` and slow down writes.

Here are some ideas:
1. **Speed up recovery**. Instead of recovering each WAL into a single table file, we can recover multiple WALs into a single table file (flush when it's bigger than `write_buffer_size`). This will also reduce number of fsync()s we're waiting for (we fsync every file serially). We could also implement multi-threaded recovery.
2. Add an API call that will support compacting small files from L0 to L0. Why would you want this as a general compaction strategy? It would only be useful after recovery, when we create a bunch of small files in L0. You could manually compact those files into a bigger L0 and your writes won't be slowed down. We are trying to define a general-purpose pluggable compaction algorithm API. Feel free to join the discussion at: https://reviews.facebook.net/D19143
3. Instead of running with 128 column families, run with 128 rocksdb instances (this won't work if you need atomic writes across shards). That way, you can run recoveries in parallel and WAL sizes will be bigger in general, creating less L0 files.

Thanks for quick response.
1. It's a good idea to 'recover multiple WALs into a single table file'
2. If there're more CompactXXX API to used, I think it may not need to write another compaction strategy. But It seems need more time to be fully supported.
3. Runing with 128 rocksdb instances will be more complex to manage, such as the replication. So I prefer to use column families if possible.

P.S. It's a little bit surprised to me that we can see your code review records inside Facebook. A good way to known your progress and discussion.

Hi allenlz, 

rocksdb code is open source and the development process is also in the open. The engineering discussion happen here where anybody can participate:
https://www.facebook.com/groups/rocksdb.dev/

You can contribute any of your code back to the rocksdb open source code base if you so desire.

"After restart, RocksDB need to recover from 128 rolling WALs to all CFs. Problem A: it may cost several minutes because there’re several GBs of WALs." what is your write buffer size? I want to figure out whether it takes a long time because the total mem table sizes across all CFs are too large, or because of something else.

Having too many L0 files after recovery is something we should fix.

@allenlz do you now make use of options.level0_slowdown_writes_trigger and options.level0_stop_writes_trigger? If not, you can try to work it around by setting the two parameters to large enough so that your write will not be blocked while compaction is triggered.

B(1) is an interesting idea. Options.min_write_buffer_number_to_merge tries to achieves similar thing as B(1)? But it will trade memory space for better WAMP.

@igorcanadi : the following is the value I could see to compact smaller L0 files into L0 files.  Correct me if I am wrong.

When the total size of L1 is sufficiently larger than L0 files, compacting smaller L0 files into L0 files may reduce write amplification --- you could think of this strategy as having L0 use universal compaction, and having level > L0 use level compaction.

You can achieve similar thing with universal compaction by setting min_merge_width to N (and adjust max_size_amplification_percent accordingly), where N is the ratio between the last big file and the flushed smaller L0 file. I actually would consider it as a universal compaction because they have similar space amp of 2. That also improves the WAL recovery issue since smaller files would merge together instead of merging into the big one

@siying 

> what is your write buffer size? I want to figure out whether it takes a long time because the total mem table sizes across all CFs are too large, or because of something else.

I ran it again, here are my options:
max_total_wal_size: 8G(this time I increase it from 4G), write_buffer_size:64M, target_file_size_base: 64M, level0_file_num_compaction_trigger: 64, level0_slowdown_writes_trigger: 128, min_write_buffer_number_to_merge: 1, max_write_buffer_number: 4
And some log sizes:

```
...
7.5G Jul 24 00:42 001579.log
2.3M Jul 24 00:42 001685.log
...
8.0M Jul 24 00:43 001837.log
1.9G Jul 24 00:44 001839.log (last one)
```

It took 1 min 45 seconds to recover.

@allenlz my suggestion of larger level0_slowdown_writes_trigger and level0_stop_writes_trigger are to make sure writes can go through even if compaction didn't finish. It should be irrelevant to recovery speed. Did you see writes still blocking after starting up?

> you can try to work it around by setting the two parameters to large enough so that your write will not be blocked while compaction is triggered.

@siying Yes, you're right. I set level0_slowdown_writes_trigger to 128, it's too small for this senario. 
The last test is the one _before I saw your post_. Thanks for point it out, I'll fix it in the next test. I believe it will not slow the writes if I set larger, since I saw the '[25] Compacting 129@0 + 0@1 files'

I ran it once more with level0_slowdown_writes_trigger to 512. Writes are not slowed down.

@yhchiang I agree with you.
The name Mixed Style in my first post, comes from combining the Universal Style in L0 and Level Style.
Right now, I think it's better that we **take L0 -> L0 as a optimization of Level Style**, instead of another new strategy.

@ljinfb 
Universal Style to me is a little bit harder to tune and to predict the write-amp.

@allenlz
That is true. Assume newly flushed L0 file size is M, the last big L0 file size is N, then setting min_merge_width to N/M + 1 can make sure write-amp < 2 IF no compaction is triggered before the current compaction completes. The IF condition is guaranteed in your strategy by locking down L1 files, however, universal style does not provide such guarantee. I feel the write-amp will still be close to 2 in reality since 2 compactions probably won't happen in the same ColumnFamily at the same time, given you use 128 CFs.
One side-benefit of universal-style is that it sort of mimics the new strategy you described in B. When lots of small files are generated in a short period, they would get compacted together instead of going into the big file. But it does not guarantee "only once" as you mentioned.

FYI, we are working on https://reviews.facebook.net/D20661 , with which there shouldn't be lots of L0 files generated during recovery.

Thank you guys for the improving during recovery.

In the past weeks, I implemented the small L0 files compaction inside the Level Style Compaction (https://github.com/allenlz/rocksdb/commit/faa4792ebb789b6ad19afc7f00e4333c64da2c93)
I also write tests to make sure the data is not lost or corrupted. I met several problems and after solve them, it works good to me.

Any comments is welcome.

_I will post my whole solution and test results of write-amplication in near future._

With leveled compaction limited to L0 and L1 files, this is very similar to universal where L1 is the max sorted run. Only the L1 is partitioned by key range here and it is a single file for universal. 

Can universal be enhanced to support this workload?

I don't think there is a benefit (yet) from partitioning the L1 into many files in this case. L0 -> L1 compaction is all data from L0 to all data from L1 at this point.

Thank you for reporting this issue and appreciate your patience. We've  notified the core team for an update on this issue. We're looking for a response within the next 30 days or the issue may be closed.

