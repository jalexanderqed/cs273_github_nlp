Hi, I seem to be getting really poor ingest performance with the vector memtable. I'm running the 'Bulk load database into L0..." command from [Performance Benchmarks](https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks), changing only the database path:

```
bpl=10485760;overlap=10;mcz=2;del=300000000;levels=2;ctrig=10000000; delay=10000000; stop=10000000; wbn=30; mbc=20; mb=1073741824;wbs=268435456; dds=1; sync=0; r=1000000000; t=1; vs=800; bs=65536; cs=1048576; of=500000; si=1000000; ./db_bench --benchmarks=fillrandom --disable_seek_compaction=1 --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t --value_size=$vs --block_size=$bs --cache_size=$cs --bloom_bits=10 --cache_numshardbits=4 --open_files=$of --verify_checksum=1 --db=/mnt/db_bench --sync=$sync --disable_wal=1 --compression_type=zlib --stats_interval=$si --compression_ratio=50 --disable_data_sync=$dds --write_buffer_size=$wbs --target_file_size_base=$mb --max_write_buffer_number=$wbn --max_background_compactions=$mbc --level0_file_num_compaction_trigger=$ctrig --level0_slowdown_writes_trigger=$delay --level0_stop_writes_trigger=$stop --num_levels=$levels --delete_obsolete_files_period_micros=$del --min_level_to_compress=$mcz --max_grandparent_overlap_factor=$overlap --stats_per_interval=1 --max_bytes_for_level_base=$bpl --memtablerep=vector --use_existing_db=0 --disable_auto_compactions=1 --source_compaction_factor=10000000
```

After 5 minutes the statistics are:

```
2013/12/23-00:00:32  ... thread 0: (1000000,5000000) ops and (14495.0,15005.6) ops/second in (68.989201,333.209596) seconds
                               Compactions
Level  Files Size(MB) Score Time(sec)  Read(MB) Write(MB)    Rn(MB)  Rnp1(MB)  Wnew(MB) RW-Amplify Read(MB/s) Write(MB/s)      Rn     Rnp1     Wnp1     NewW    Count  Ln-stall Stall-cnt
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0       16     3015 301.5       119         0      3015         0         0      3015        0.0       0.0        25.2        0        0        0        0       16       0.0         0
Uptime(secs): 333.2 total, 69.0 interval
Writes cumulative: 4999999 total, 4999999 batches, 1.0 per batch, 3.87 ingest GB
WAL cumulative: 0 WAL writes, 0 WAL syncs, 0.00 writes per sync, 0.00 GB written
Compaction IO cumulative (GB): 3.87 new, 0.00 read, 2.94 write, 2.94 read+write
Compaction IO cumulative (MB/sec): 11.9 new, 0.0 read, 9.0 write, 9.0 read+write
Amplification cumulative: 0.8 write, 0.8 compaction
Writes interval: 1000000 total, 1000000 batches, 1.0 per batch, 793.5 ingest MB
WAL interval: 0 WAL writes, 0 WAL syncs, 0.00 writes per sync, 0.00 MB written
Compaction IO interval (MB): 793.46 new, 0.00 read, 565.31 write, 565.31 read+write
Compaction IO interval (MB/sec): 11.5 new, 0.0 read, 8.2 write, 8.2 read+write
Amplification interval: 0.7 write, 0.7 compaction
Stalls(secs): 0.000 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 0.000 leveln_slowdown
Stalls(count): 0 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 0 leveln_slowdown
```

When I rm -rf that and then run exactly the same command but with `--memtablerep=skip_list`, ingest is 6x faster over the first five minutes:

```
2013/12/23-00:08:09  ... thread 0: (1000000,28000000) ops and (76160.0,90472.0) ops/second in (13.130248,309.487848) seconds
                               Compactions
Level  Files Size(MB) Score Time(sec)  Read(MB) Write(MB)    Rn(MB)  Rnp1(MB)  Wnew(MB) RW-Amplify Read(MB/s) Write(MB/s)      Rn     Rnp1     Wnp1     NewW    Count  Ln-stall Stall-cnt
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0       87    15938 1593.8      2438         0     15938         0         0     15938        0.0       0.0         6.5        0        0        0        0       87       0.0         0
Uptime(secs): 309.5 total, 13.1 interval
Writes cumulative: 27999999 total, 27999999 batches, 1.0 per batch, 21.70 ingest GB
WAL cumulative: 0 WAL writes, 0 WAL syncs, 0.00 writes per sync, 0.00 GB written
Compaction IO cumulative (GB): 21.70 new, 0.00 read, 15.56 write, 15.56 read+write
Compaction IO cumulative (MB/sec): 71.8 new, 0.0 read, 51.5 write, 51.5 read+write
Amplification cumulative: 0.7 write, 0.7 compaction
Writes interval: 1000000 total, 1000000 batches, 1.0 per batch, 793.5 ingest MB
WAL interval: 0 WAL writes, 0 WAL syncs, 0.00 writes per sync, 0.00 MB written
Compaction IO interval (MB): 793.46 new, 0.00 read, 549.57 write, 549.57 read+write
Compaction IO interval (MB/sec): 60.4 new, 0.0 read, 41.8 write, 41.8 read+write
Amplification interval: 0.7 write, 0.7 compaction
Stalls(secs): 0.000 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 0.000 leveln_slowdown
Stalls(count): 0 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 0 leveln_slowdown

```

I'm running b26dc9562801d on ubuntu on an i2.xlarge EC2 instance. The db_bench prelude looks like this:

```
LevelDB:    version 2.0
Date:       Sun Dec 22 23:54:59 2013
CPU:        4 * Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz
CPUCache:   25600 KB
Keys:       16 bytes each
Values:     800 bytes each (40000 bytes after compression)
Entries:    1000000000
RawSize:    778198.2 MB (estimated)
FileSize:   38162231.4 MB (estimated)
Write rate limit: 0
Compression: zlib
Memtablerep: vector
WARNING: Assertions are enabled; benchmarks unnecessarily slow
```

Looked in the code, with assertions turned on the vector memtable has O(n<sup>2</sup>) insertion time...guess I should have heeded the warning :)

