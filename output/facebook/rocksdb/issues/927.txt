After inserting about a billion keys (rough estimate), I am hitting the maximum file size supported by `PlainTableReader`. In my logs it says:

``` json
2016/01/06-22:41:30.390947 7f52427fc700 [WARN] Compaction error: Not implemented: File is too large for PlainTableReader!
2016/01/06-22:41:30.390964 7f52427fc700 (Original Log Time 2016/01/06-22:41:30.390396) [default] compacted to: files[10 0 0 0 0 0 2264] max score 2.00, MB/sec: 5.3 rd, 5.3 wr, level
 0, files in(0, 3) out(1) MB in(0.0, 2177.6) out(2160.9), read-write-amplify(inf) write-amplify(inf) Not implemented: File is too large for PlainTableReader!, records in: 103049259,
 records dropped: 0
2016/01/06-22:41:30.390968 7f52427fc700 (Original Log Time 2016/01/06-22:41:30.390462) EVENT_LOG_v1 {"time_micros": 1452120090390429, "job": 18890, "event": "compaction_finished", "
output_level": 0, "num_output_files": 1, "total_output_size": 2265820389, "num_input_records": 102019867, "num_output_records": 102019867, "lsm_state": [10, 0, 0, 0, 0, 0, 2264]}
2016/01/06-22:41:30.390974 7f52427fc700 [ERROR] Waiting after background compaction error: Not implemented: File is too large for PlainTableReader!, Accumulated background error counts: 1
```

The part `"total_output_size": 2265820389` already shows the error - `PlainTableIndex::kMaxFileSize` is 2Gb (minus 1 byte), `total_output_size` is about 2.1Gb. So far so clear.

What's not clear is why this error happens. I used the configuration values from https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide#in-memory-prefix-database, with some parameters from https://github.com/facebook/rocksdb/wiki/RocksDB-In-Memory-Workload-Performance-Benchmarks#test-1-point-lookup :

``` cpp
  // Plaintable
  rocksdb::PlainTableOptions plain_table_options;
  plain_table_options.user_key_len = 8;
  plain_table_options.bloom_bits_per_key = 10;
  plain_table_options.hash_table_ratio = 0.75;
  options.table_factory.reset(rocksdb::NewPlainTableFactory(plain_table_options));

  options.allow_mmap_reads = true;
  options.allow_mmap_writes = false;
  options.compression = rocksdb::kNoCompression;

  // create prefix extractor over entire key
  options.prefix_extractor.reset(rocksdb::NewFixedPrefixTransform(8));

  // Use hash link list memtable to change binary search to hash lookup in mem table
  // in write mode, use vector memtable
  if(mode == Mode::WriteOnly) {
    options.memtable_factory.reset(new rocksdb::VectorRepFactory);    
  } else {
    options.memtable_factory.reset(rocksdb::NewHashSkipListRepFactory(1024*1024));
  }

  // Enable bloom filter for hash table to reduce memory accesses (usually means
  // CPU cache misses) when reading from mem table to one, for the case where key
  // is not found in mem tables
  options.memtable_prefix_bloom_bits = 10000000;
  options.memtable_prefix_bloom_probes = 6;

  // Tune compaction so that, a full compaction is kicked off as soon as we have
  // two files. We hack the parameter of universal compaction:
  options.compaction_style = rocksdb::kCompactionStyleUniversal;
  options.compaction_options_universal.size_ratio = 10;
  options.compaction_options_universal.min_merge_width = 2;
  options.compaction_options_universal.max_size_amplification_percent = 1;
  options.level0_file_num_compaction_trigger = 1;
  options.level0_slowdown_writes_trigger = 16;
  options.level0_stop_writes_trigger = 24;

  // Tune bloom filter to minimize memory accesses:
  options.bloom_locality = 1;

  // Use one mem table at one time. Its size is determined by the full compaction
  // interval we want to pay. We tune compaction such that after every flush, a
  // full compaction will be triggered, which costs CPU. The larger the mem table
  // size, the longer the compaction interval will be, and at the same time, we
  // see less memory efficiency, worse query performance and longer recovery time
  // when restarting the DB.
  options.write_buffer_size = 32 << 20;  // 32Mb
  options.max_write_buffer_number = 2;
  options.min_write_buffer_number_to_merge = 1;

  // Multiple DBs sharing the same compaction pool of `num_threads`.
  // we have one thread for messaging, one thread for flushing, and
  // at least one thread for compacting. If num_threads > 3, put all 
  // remaining threads into compaction.
  int num_background_threads = num_threads > 3 ? num_threads - 2 : 1;
  options.max_background_compactions = num_background_threads;
  options.max_background_flushes = 1;
  options.env->SetBackgroundThreads(1, rocksdb::Env::Priority::HIGH);
  options.env->SetBackgroundThreads(num_background_threads, rocksdb::Env::Priority::LOW);

  // Settings for WAL logs:
  options.disableDataSync = 1;
  options.bytes_per_sync = 2 << 20;
```

(yes, this is the same configuration as in my other bug report today).

If it's something trivial, I would suggest adding this information to the documentation.

