[{
	"body": "Example code\r\n```\r\n#include <cstdio>\r\n#include <iostream>\r\n#include <string>\r\n\r\n#include \"rocksdb/db.h\"\r\n#include \"rocksdb/slice.h\"\r\n#include \"rocksdb/options.h\"\r\n#include \"rocksdb/merge_operator.h\"\r\n#include \"rocksdb/env.h\"\r\n\r\nusing namespace rocksdb;\r\n\r\nstd::string kDBPath = \"/ssd1/data/test\";\r\n\r\nclass RocksDbMerger : public MergeOperator {\r\n public:\r\n  bool FullMerge(const Slice& key,\r\n                 const Slice* existing_value,\r\n                 const std::deque<std::string>& operand_list,\r\n                 std::string* new_value,\r\n                 Logger* logger) const override {\r\n    Log(InfoLogLevel::INFO_LEVEL, logger, \"FullMerge new_value size:%ld\", new_value->size());\r\n    if (existing_value) {\r\n      new_value->append(existing_value->data(), existing_value->size());\r\n    }\r\n    for (auto& operand : operand_list) {\r\n      new_value->append(operand);\r\n    }\r\n    return true;\r\n  }\r\n\r\n  bool PartialMerge(const Slice& key,\r\n                    const Slice& left_operand,\r\n                    const Slice& right_operand,\r\n                    std::string* new_value,\r\n                    Logger* logger) const override {\r\n    return false;\r\n  }\r\n\r\n  const char* Name() const override {\r\n    return \"rocksdb_merger\";\r\n  }\r\n};\r\n\r\nint main() {\r\n  DB* db;\r\n  Options options;\r\n  // Optimize RocksDB. This is the easiest way to get RocksDB to perform well\r\n  options.IncreaseParallelism();\r\n  options.OptimizeLevelStyleCompaction();\r\n  // create the DB if it's not already present\r\n  options.create_if_missing = true;\r\n\r\n  options.merge_operator.reset(new RocksDbMerger());\r\n\r\n  // open DB\r\n  Status s = DB::Open(options, kDBPath, &db);\r\n  assert(s.ok());\r\n\r\n  // Put key-value\r\n  s = db->Merge(WriteOptions(), \"key1\", \"value1\");\r\n  assert(s.ok());\r\n\r\n  s = db->Merge(WriteOptions(), \"key2\", \"value2\");\r\n  assert(s.ok());\r\n\r\n  s = db->Merge(WriteOptions(), \"key3\", \"value3\");\r\n  assert(s.ok());\r\n\r\n  auto itr = db->NewIterator(ReadOptions());\r\n  itr->SeekToFirst();\r\n  while (itr->Valid()) {\r\n    std::cout << itr->value().ToString() << std::endl;\r\n    itr->Next();\r\n  }\r\n  delete itr;\r\n\r\n  delete db;\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\nOutput\r\n```\r\nvalue1\r\nvalue1value2\r\nvalue1value2value3\r\n```\r\n\r\nLOG from FullMerge\r\n```\r\n2016/11/24-17:17:40.904639 7fa94cb5c940 FullMerge new_value size:0\r\n2016/11/24-17:17:40.904704 7fa94cb5c940 FullMerge new_value size:6\r\n2016/11/24-17:17:40.904723 7fa94cb5c940 FullMerge new_value size:12\r\n```",
	"number": 1567,
	"title": "MergeOperator::FullMerge() param : new_value is not empty when use rocksdb::Iterator"
}, {
	"body": "Our java application open 12 db for writable mode and open serveral db for read only mode.\r\nEach DB has following configuration.\r\n\r\nmax_backfground_compactions=1\r\nblock_cache=null\r\nmax_open_files=5000\r\nblock_size=4k\r\nwrite_buffer_size=512mb\r\nmax_write_buffer_number=3\r\nmax_bytes_for_level_base=2048mb\r\ntarget_file_size_base=512mb\r\n\r\nMax Heap size for java application is 16gb.\r\nAfter startup application, Memory usage for application is growing up to 42gb and we got OOM.\r\n1. Anybody explain why such big memory used in application in spite of parameter ?\r\n2. How can we limit memory usage for rocksdb ?\r\n\r\nThanks in advance.\r\nRegards, Arnold.",
	"number": 1566,
	"title": "Memory use for rocksdb are growing and we got OOM."
}, {
	"body": "Here is the document of the API\r\n\r\n```C++\r\n  // Sets iter to an iterator that is positioned at a write-batch containing\r\n  // seq_number. If the sequence number is non existent, it returns an iterator\r\n  // at the first available seq_no after the requested seq_no\r\n  // Returns Status::OK if iterator is valid\r\n  // Must set WAL_ttl_seconds or WAL_size_limit_MB to large values to\r\n  // use this api, else the WAL files will get\r\n  // cleared aggressively and the iterator might keep getting invalid before\r\n  // an update is read.\r\n  virtual Status GetUpdatesSince(\r\n      SequenceNumber seq_number, unique_ptr<TransactionLogIterator>* iter,\r\n      const TransactionLogIterator::ReadOptions&\r\n          read_options = TransactionLogIterator::ReadOptions()) = 0;\r\n```\r\n\r\nIt is clear that the iterator should be *after* the requested seq_no if the requested seq_no is not existing.  I have a  db which currently holds two sequence number 1, 4.  However, the following programming emits \r\n```\r\n1\r\n4\r\n```\r\nPer the document, only `4` should be emitted.  Do I misunderstand the doc ? \r\n\r\nHere is the program:\r\n\r\n```C++\r\n#include <iostream>\r\n#include \"rocksdb/db.h\"\r\n\r\nusing namespace std;\r\n\r\nint main() {\r\n    rocksdb::DB* db;\r\n    rocksdb::Options options;\r\n    rocksdb::Status status = rocksdb::DB::OpenForReadOnly(options, \"/tmp/kv\", &db);\r\n\r\n    unique_ptr<rocksdb::TransactionLogIterator> iter;\r\n    status = db->GetUpdatesSince(2, &iter);\r\n    rocksdb::WriteBatch wb;\r\n    if (status.ok()) {\r\n        for (; iter->Valid(); iter->Next()) {\r\n            auto batch_result = iter->GetBatch();\r\n\r\n            wb = *batch_result.writeBatchPtr;\r\n            cout << batch_result.sequence << endl;\r\n        }\r\n    }\r\n\r\n    delete db;\r\n\r\n    return 0;\r\n}\r\n```",
	"number": 1565,
	"title": "The behaviour of GetUpdatesSince  is not the same as documented"
}, {
	"body": "rocksdb direct io support\r\n\r\n```\r\n[gzh@dev11575.prn2 ~/rocksdb] ./db_bench -benchmarks=fillseq --num=1000000\r\nInitializing RocksDB Options from the specified file\r\nInitializing RocksDB Options from command-line flags\r\nRocksDB:    version 5.0\r\nDate:       Wed Nov 23 13:17:43 2016\r\nCPU:        40 * Intel(R) Xeon(R) CPU E5-2660 v2 @ 2.20GHz\r\nCPUCache:   25600 KB\r\nKeys:       16 bytes each\r\nValues:     100 bytes each (50 bytes after compression)\r\nEntries:    1000000\r\nPrefix:    0 bytes\r\nKeys per prefix:    0\r\nRawSize:    110.6 MB (estimated)\r\nFileSize:   62.9 MB (estimated)\r\nWrite rate: 0 bytes/second\r\nCompression: Snappy\r\nMemtablerep: skip_list\r\nPerf Level: 1\r\nWARNING: Assertions are enabled; benchmarks unnecessarily slow\r\n------------------------------------------------\r\nInitializing RocksDB Options from the specified file\r\nInitializing RocksDB Options from command-line flags\r\nDB path: [/tmp/rocksdbtest-112628/dbbench]\r\nfillseq      :       4.393 micros/op 227639 ops/sec;   25.2 MB/s\r\n\r\n[gzh@dev11575.prn2 ~/rocksdb] ./db_bench -use_direct_writes -benchmarks=fillseq --num=1000000\r\nInitializing RocksDB Options from the specified file\r\nInitializing RocksDB Options from command-line flags\r\nRocksDB:    version 5.0\r\nDate:       Wed Nov 23 13:18:10 2016\r\nCPU:        40 * Intel(R) Xeon(R) CPU E5-2660 v2 @ 2.20GHz\r\nCPUCache:   25600 KB\r\nKeys:       16 bytes each\r\nValues:     100 bytes each (50 bytes after compression)\r\nEntries:    1000000\r\nPrefix:    0 bytes\r\nKeys per prefix:    0\r\nRawSize:    110.6 MB (estimated)\r\nFileSize:   62.9 MB (estimated)\r\nWrite rate: 0 bytes/second\r\nCompression: Snappy\r\nMemtablerep: skip_list\r\nPerf Level: 1\r\nWARNING: Assertions are enabled; benchmarks unnecessarily slow\r\n------------------------------------------------\r\nInitializing RocksDB Options from the specified file\r\nInitializing RocksDB Options from command-line flags\r\nDB path: [/tmp/rocksdbtest-112628/dbbench]\r\nfillseq      :       7.621 micros/op 131222 ops/sec;   14.5 MB/s",
	"number": 1564,
	"title": "direct io write support"
}, {
	"body": "I want to use rocksdb in java application on x86 windows pc. is it possible or it will require too much changes?",
	"number": 1563,
	"title": "is it possible to build x86 windows version?"
}, {
	"body": "",
	"number": 1560,
	"title": "Blob storage"
}, {
	"body": "DBTestDynamicLevel.MigrateToDynamicLevelMaxBytesBase keeps failing in Travis. Disable it for now to unblock the test.\r\nAlso add an assert to help debug another failure.",
	"number": 1559,
	"title": "Disable DBTestDynamicLevel.MigrateToDynamicLevelMaxBytesBase"
}, {
	"body": "I have a rather large database of about 3Tb in size and billions of rows. It was created using `PrepareForBulkLoad()` options set.\r\n\r\nTrying to reopen database (even in read-only mode) and either positioning iterator to the first key or reading any key ends up in quick (minutes) allocation of the whole RAM (32Gb) and OOM.\r\n\r\nI experimented with `BlockBasedTableOptions` but it doesn't seem to help.\r\n\r\nAttached LOG dumped every 60 seconds. It was made with `disable_auto_compactions = true;` option when trying to position iterator to the first key. Iterator's options are:\r\n```\r\n                ro.managed = true;\r\n                ro.fill_cache = false;\r\n                ro.verify_checksums = false;\r\n```\r\nbut I tried many other combinations without any luck.\r\n[LOG.txt](https://github.com/facebook/rocksdb/files/603857/LOG.txt)\r\n\r\nWhat are the best options to read large database without getting into OOM? I use current rocksdb version as of commit a0deec960f3a8190831c673e5ba998fe6fb7ea90\r\n",
	"number": 1555,
	"title": "Huge memory leak in large database when doing read or iterator positioning"
}, {
	"body": "My program crashed, and it seems the database is now corrupted! Next time I try to open it, I get an error:\r\n\r\n```\r\n/var/local/foo-db/MANIFEST-000008: No such file or directory\r\n```\r\n\r\nWhat's going on here? How do I recover from this crash?",
	"number": 1554,
	"title": "Database corrupted after crash"
}, {
	"body": "Looking at https://github.com/facebook/rocksdb/commit/20699df8438d14568915bb3a8e7038ce224f1e1c\r\n\r\nThe commit claims to deprecate memtable_prefix_bloom_bits and memtable_prefix_bloom_probes. It removes support for them and \"not supported\" is stronger than \"deprecated\", at least in the MySQL community. Deprecated is the step before removing support where you warn people that an option will not be supported in the future and hopefully provide the new option so the migration can begin.\r\n\r\noptions_helper now parses and ignores uses of the old options. Parse but ignored is unfriendly to users and we are familar with that in MySQL, http://mysqlha.blogspot.com/2009/01/parsed-but-ignored.html\r\n\r\nI think it should \r\n1) log a warning for the use of memtable_prefix_bloom_probes given that there is no alternative options\r\n2) for the use of memtable_prefix_bloom_bits -- return an error or do both of log a warning and convert the value to a use of memtable_prefix_bloom_bits_ratio\r\n\r\nFinally, if prefix_extractor is set, can the default for memtable_prefix_bloom_bits_ratio be something greater than 0. Good defaults means we have to set few things in my.cnf",
	"number": 1551,
	"title": "memtable_prefix_bloom_bits, memtable_prefix_bloom_bits_ratio and memtable_prefix_bloom_probes"
}, {
	"body": "Made delete_obsolete_files_period_micros option dynamic. It can be updating using DB::SetDBOptions().",
	"number": 1549,
	"title": "Made delete_obsolete_files_period_micros option dynamic."
}, {
	"body": "By \"mid\" concurrency I mean when the number of clients is less than the number of HW threads. By \"high\" concurrency I mean when the number of clients is much larger than the number of HW threads. And this assumes clients don't stall on storage reads. In my tests, results are not public yet, I see RocksDB (or MyRocks) with the concurrent memtable helping QPS at mid concurrency and hurting at high concurrency.\r\n\r\nIt would be nice to explain why it hurts at high concurrency.",
	"number": 1544,
	"title": "Explain why concurrent memtable helps at \"mid\" concurrency but hurts at \"high\" concurrency"
}, {
	"body": "First of all, it's pretty cool that Facebook released RocksDB under a BSD license. It also comes with a patent grant, which, although [slightly](https://news.ycombinator.com/item?id=9356508) [controversial](https://news.ycombinator.com/item?id=9111849), seems to generally be received quite well - many thanks for that, too!\r\n\r\nHaving thought about it, I became a little curious which patents are actually related to RocksDB. After all, its underlying data structures are quite well-understood. I did a quick search on google patents, and also looked at some wikipedia pages (for example for LSM trees and bloom filters), but couldn't really find anything. Is there a list of patents that are related to RocksDB?",
	"number": 1543,
	"title": "List of Patents"
}, {
	"body": "Issue: facebook/rocksdb#1541",
	"number": 1542,
	"title": "User (Rocks JNI) needs to delete/dispose family handle. Otherwise files are not deleted."
}, {
	"body": "Dropping family is just marking family as dropped internally. But files are deleted after handle is disposed. JNI leaks the pointer when user calls dropFamily.",
	"number": 1541,
	"title": "Rocks JNI does not dispose family handle"
}, {
	"body": "I encountered an error: \"File is too large for PlainTableReader!\" when using universal compaction and table factory is PlainTableFactory, and target_file_size_mulitplier is 2, and level_num is 7.\r\nWhy PlainTableBuilder can build a table which can not be read?",
	"number": 1538,
	"title": "File is too large for PlainTableReader"
}, {
	"body": "When using universal compaction and multiple dbpathes(two in my case, and on two different SSD), just first dbpath is used, when the first SSD is full(target_size was configured as half of available space), insertion always failed, and the second SSD is never used.",
	"number": 1524,
	"title": "universal compaction never use second dbpath"
}, {
	"body": "Currently, the Multiget method [returns a Map<byte[], byte[]>](https://github.com/facebook/rocksdb/blob/master/java/src/main/java/org/rocksdb/RocksDB.java#L860). \r\n\r\nGenerally, an API should never return a `Map`/`Set` with `byte[]` as key. This is because `hashCode()` for `byte[]` is reference based. It just happens to work in this case because the reference is never changed. I think we should deprecate the existing multiget method, and have a new one which just returns a `List<byte[]>` with possible nulls, just like the native interface. This also preserves maximum performance. It is generally not a good practice to return `null` in `List`, but since this part of code should be highly performant, I think it is a reasonable trade-off as long as it is documented.",
	"number": 1523,
	"title": "RocksDB Java multiget method should not return Map with byte[] as key"
}, {
	"body": "Summary:\r\n- \"rocksdb.compaction.key.drop.range_del\" - number of keys dropped during compaction due to a range tombstone covering them\r\n- \"rocksdb.compaction.range_del.drop.obsolete\" - number of range tombstones dropped due to compaction to bottom level and no snapshot saving them\r\n- s/CompactionIteratorStats/CompactionIterationStats/g since this class is no longer specific to CompactionIterator -- it's also updated for range tombstone iteration during compaction\r\n- Move the above class into a separate .h file to avoid circular dependency.\r\n\r\nTest Plan: test together with https://reviews.facebook.net/D63927, which uses these stats",
	"number": 1520,
	"title": "DeleteRange compaction statistics"
}, {
	"body": "Hello everyone,\r\n\r\nCan RocksDB work on Ramdisk. Today I tried to store rocksdb in RAMdisk (8GB / Macbook Pro 2014 with 16GB RAM), and the program always crashes (segmentation fault).\r\n\r\nI did not changed anything. Before, one line of code is:\r\n\r\n``path += \"/testdb/\";``\r\n\r\nand now it is:\r\n\r\n``path += \"/Volumes/ram_disk/testdb/\";``\r\n\r\nI tested my RAMdisk in command line, indeed I can read/write on that.\r\n\r\nCan rocksdb work with ramdisk, or something is wrong with my programming?\r\n\r\nThank you very much,",
	"number": 1517,
	"title": "Storing RocksDB in RAMdisk"
}, {
	"body": "in: https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks\r\nrocksdb performance is shown.\r\n\r\ni have a couple of questions regarding the setup:\r\n\r\n1.   2 FusionIO devices in SW RAID 0 that can do ~200k 4kb read/second at peak\r\n      What is the write performance of the devices? \r\n       is 200k 4kb read/second at peak is for each device or for both of them?\r\n\r\n2.  Machine has 144 GB of RAM\r\n     1G rocksdb block cache\r\n     Is the whole 144 GB can be used for data caching ? if not,  what is the difference between the 2 lines\r\n\r\n3.  total database size is 800GB\r\n    but in Test1: total data size 481 GB \r\n    why there is a missmatch?\r\n\r\n4. Difference in performance between Test1 and Test2\r\n    In first Scenario the Write amplification is 2 (first the DB is being written to disc. Then it is being read          again, merge sorted, and being writtent again). is it true?\r\n   In the second scenario, the WA is at least 2, but merge sort is very quick, becaese there are no overlaps.\r\n   the difference in Throughput is 80MB/s vs 370MB/s.\r\n    So the whole difference is because the SW merge-sort is very slow?\r\n    Also , what are the DB parametrs? (compaction type, number of levels, MaxLevelRatio etc')\r\n\r\nThanks a lot.\r\n\r\n\r\n\r\n",
	"number": 1510,
	"title": "Rocksdb performance quiestions"
}, {
	"body": "char is unsigned on power by default causing this test to fail with the FF case. ppc64 return 255 while x86 returned -1. Casting works on both platforms.",
	"number": 1500,
	"title": "cast to signed char in ldb_cmd_test for ppc64le"
}, {
	"body": "Some places autodetected. These are the two places that didn't.\r\n\r\ncloses #1498\r\n\r\nStill unsure if the following instances of 4 * 1024 need fixing in:\r\nutil/io_posix.h\r\ninclude/rocksdb/table.h (appears to be blocksize and different)\r\nutilities/persistent_cache/block_cache_tier.cc\r\nutilities/persistent_cache/persistent_cache_test.h\r\ninclude/rocksdb/env.h\r\nutil/env_posix.cc\r\ndb/column_family.cc",
	"number": 1499,
	"title": "Page size isn't always 4k on linux"
}, {
	"body": "The page size util/env_test.c:NewAligned is a fixed 4K size rather than using sysconf(_SC_PAGESIZE) to determine this like the rockdb does.\r\n\r\nThis results in an assertion during make check on non-4K page size Linux kernels.",
	"number": 1498,
	"title": "test failure: util/env_test.c incorrect assumption about page size"
}, {
	"body": "During a make check:\r\n\r\n<pre>\r\n08:37:11 + g++ --version\r\n08:37:11 g++ (Ubuntu 5.4.1-2ubuntu1~16.04) 5.4.1 20160904\r\n08:37:11 Copyright (C) 2015 Free Software Foundation, Inc.\r\n08:37:11 This is free software; see the source for copying conditions.  There is NO\r\n08:37:11 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n08:37:11 make -j\"${CPUS}\" --output-sync=target V=1 \"${BUILD_TYPE}\"\r\n08:37:11 + make -j24 --output-sync=target V=1 check\r\n</pre>\r\n\r\n<pre>\r\nroot@1a9fff0208e0:/build# ps -ef\r\nUID        PID  PPID  C STIME TTY          TIME CMD\r\nroot         1     0  0 21:37 ?        00:00:00 /bin/sh -c /usr/bin/sudo -E -H -u build CC=${CC} BUILD_TYPE=${BUILD_TYPE} /scripts/build_rocksdb.sh\r\nroot         7     1  0 21:37 ?        00:00:00 /usr/bin/sudo -E -H -u build CC=gcc BUILD_TYPE=check /scripts/build_rocksdb.sh\r\nbuild        8     7  0 21:37 ?        00:00:00 /bin/bash /scripts/build_rocksdb.sh\r\nbuild       15     8  0 21:37 ?        00:00:00 make -j24 --output-sync=target V=1 check\r\nroot      2437     0  0 21:56 ?        00:00:00 bash\r\nroot      2447  2437  0 21:56 ?        00:00:00 ps -ef\r\nbuild     3409    15  0 21:40 ?        00:00:00 /bin/bash -c if test \"100%\" != 1                                  \\     && (build_tools/gnu_parallel --gnu --help 2>/dev/null) |   \r\nbuild     3415  3409  0 21:40 ?        00:00:00 make T= TMPD=/dev/shm/rocksdb.usPg check_0\r\nbuild     3600  3415  0 21:40 ?        00:00:00 /bin/bash -c export TEST_TMPDIR=/dev/shm/rocksdb.usPg; \\ printf '%s\\n' ''......\\   'To monitor subtest <duration,pass/fail,name>,'.\r\nbuild     3606  3600  0 21:40 ?        00:00:02 perl build_tools/gnu_parallel -j100% --plain --joblog=LOG --gnu {} >& t/log-{/}\r\nbuild     8498  3606  0 21:40 ?        00:00:00 /bin/bash -c ./db_test2 >& t/log-db_test2\r\nbuild     8499  8498  0 21:40 ?        00:00:00 ./db_test2\r\n</pre>\r\n\r\n<pre>\r\nbuild@1a9fff0208e0:~$ gdb -p 8499\r\ngdb) thread apply all bt full\r\n\r\nThread 42 (Thread 0x2b701600f700 (LWP 9096)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\nNo locals.\r\n#1  0x00000000006a3ca3 in rocksdb::(anonymous namespace)::ConditionWait (lock=..., condition=...) at util/threadpool_imp.cc:97\r\nNo locals.\r\n#2  rocksdb::ThreadPoolImpl::BGThread (this=this@entry=0x19b1000, thread_id=thread_id@entry=39) at util/threadpool_imp.cc:175\r\n        uniqueLock = @0x19b1010: {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 39, __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}},\r\n          __size = '\\000' <repeats 12 times>, \"'\", '\\000' <repeats 26 times>, __align = 0}\r\n        function = <optimized out>\r\n        arg = <optimized out>\r\n        decrease_io_priority = <optimized out>\r\n        low_io_priority = false\r\n#3  0x00000000006a3ed3 in rocksdb::BGThreadWrapper (arg=0x19ba0d0) at util/threadpool_imp.cc:253\r\n        meta = 0x19ba0d0\r\n        thread_id = 39\r\n        tp = 0x19b1000\r\n#4  0x00002b700756470a in start_thread (arg=0x2b701600f700) at pthread_create.c:333\r\n        __res = <optimized out>\r\n        pd = 0x2b701600f700\r\n        now = <optimized out>\r\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {47760405493504, 8172254731406293992, 0, 140723184957023, 47760405494208, 26980152, 2848951592097037288, 2848985044752546792},\r\n              mask_was_saved = 0}}, priv = {pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}\r\n        not_first_call = <optimized out>\r\n        pagesize_m1 = <optimized out>\r\n        sp = <optimized out>\r\n        freesize = <optimized out>\r\n        __PRETTY_FUNCTION__ = \"start_thread\"\r\n#5  0x00002b700896282d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\nNo locals.\r\n\r\n(others similar to thread 42)\r\n\r\n\r\nThread 3 (Thread 0x2b7009026700 (LWP 8668)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\nNo locals.\r\n#1  0x00002b700806956c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\nNo symbol table info available.\r\n\r\n\r\n\r\n\r\n#2  0x000000000069d219 in rocksdb::SyncPoint::Process (this=0xa8de80 <rocksdb::SyncPoint::GetInstance()::sync_point>, point=\"DBImpl::NotifyOnCompactionCompleted::UnlockMutex\", \r\n    cb_arg=cb_arg@entry=0x0) at util/sync_point.cc:141\r\n        lock = {_M_device = 0xa8df98 <rocksdb::SyncPoint::GetInstance()::sync_point+280>, _M_owns = true}\r\n        thread_id = {_M_thread = 47760187483904}\r\n        marker_iter = <optimized out>\r\n        callback_pair = <optimized out>\r\n#3  0x0000000000545bb4 in rocksdb::DBImpl::NotifyOnCompactionCompleted (this=this@entry=0x19baf00, cfd=0x19cb5b0, c=0x2b7010003720, st=..., compaction_job_stats=..., job_id=12)\r\n    at db/db_impl.cc:2359\r\nNo locals.\r\n#4  0x0000000000547609 in rocksdb::DBImpl::BackgroundCompaction (this=this@entry=0x19baf00, made_progress=made_progress@entry=0x2b70090250af, \r\n    job_context=job_context@entry=0x2b7009025120, log_buffer=log_buffer@entry=0x2b7009025320, arg=arg@entry=0x0) at db/db_impl.cc:3620\r\n        manual_compaction = <optimized out>\r\n        is_manual = false\r\n        trivial_move_disallowed = <optimized out>\r\n        compaction_job_stats = {elapsed_micros = 559, num_input_records = 100, num_input_files = 10, num_input_files_at_output_level = 0, num_output_records = 100, \r\n          num_output_files = 1, is_manual_compaction = false, total_input_bytes = 11867, total_output_bytes = 3425, num_records_replaced = 0, total_input_raw_key_bytes = 1800, \r\n          total_input_raw_value_bytes = 1000, num_input_deletion_records = 0, num_expired_deletion_records = 0, num_corrupt_keys = 0, file_write_nanos = 0, \r\n          file_range_sync_nanos = 0, file_fsync_nanos = 0, file_prepare_write_nanos = 0, static kMaxPrefixLength = 8, smallest_output_key_prefix = \" M[2=Lt-\", \r\n          largest_output_key_prefix = \"|HT!Xlfs\", num_single_del_fallthru = 0, num_single_del_mismatch = 0}\r\n        status = {code_ = rocksdb::Status::kOk, subcode_ = rocksdb::Status::kNone, state_ = 0x0, static msgs = {0x7d085a \"\", 0x7e8737 \"Timeout Acquiring Mutex\", \r\n            0x7e874f \"Timeout waiting to lock key\", 0x7e8770 \"Failed to acquire lock due to max_num_locks limit\", 0x7a7d2c \"No space left on device\", 0x0}}\r\n        c = std::unique_ptr<rocksdb::Compaction> containing 0x2b7010003720\r\n        __PRETTY_FUNCTION__ = \"rocksdb::Status rocksdb::DBImpl::BackgroundCompaction(bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, void*)\"\r\n#5  0x0000000000555b36 in rocksdb::DBImpl::BackgroundCallCompaction (this=this@entry=0x19baf00, arg=arg@entry=0x0) at db/db_impl.cc:3302\r\n        l = {mutex_ = 0x19bb3c0}\r\n        pending_outputs_inserted_elem = <optimized out>\r\n        s = {code_ = rocksdb::Status::kOk, subcode_ = rocksdb::Status::kNone, state_ = 0x0, static msgs = <same as static member of an already seen type>}\r\n        made_progress = true\r\n---Type <return> to continue, or q <return> to quit---\r\n        m = 0x0\r\n        job_context = {job_id = 12, full_scan_candidate_files = std::vector of length 0, capacity 0, sst_live = std::vector of length 0, capacity 0, \r\n          sst_delete_files = std::vector of length 0, capacity 0, log_delete_files = std::vector of length 0, capacity 0, \r\n          log_recycle_files = std::vector of length 0, capacity 0, manifest_delete_files = std::vector of length 0, capacity 0, memtables_to_free = {num_stack_items_ = 0, \r\n            values_ = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, vect_ = std::vector of length 0, capacity 0}, superversions_to_free = {num_stack_items_ = 1, values_ = {\r\n              0x2b700c005710, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, vect_ = std::vector of length 0, capacity 0}, logs_to_free = {num_stack_items_ = 0, values_ = {0x19b3be0, 0x0, \r\n              0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, vect_ = std::vector of length 0, capacity 0}, new_superversion = 0x0, manifest_file_number = 0, pending_manifest_file_number = 0, \r\n          log_number = 0, prev_log_number = 0, min_pending_output = 0, prev_total_log_size = 0, num_alive_log_files = 0, size_log_to_delete = 0}\r\n        log_buffer = {log_level_ = <incomplete type>, info_log_ = 0x19ad250, arena_ = {<rocksdb::Allocator> = {_vptr.Allocator = 0x7db860 <vtable for rocksdb::Arena+16>}, \r\n            static kInlineSize = 2048, static kMinBlockSize = 4096, static kMaxBlockSize = 2147483648, \r\n            inline_block_ = \"G\\351$X\\000\\000\\000\\000\\375z\\f\\000\\000\\000\\000\\000[default] compacted to: base level 1 max bytes base 10485760 files[0 1 0 0 0 0 0] max score 0.00, MB/sec: 21.2 rd, 6.1 wr, level 1, files in(10, 0) out(1) MB in(0.0, 0.0) out(0.0), rea\"..., kBlockSize = 4096, blocks_ = std::vector of length 0, capacity 0, \r\n            huge_blocks_ = std::vector of length 0, capacity 0, irregular_block_num = 0, unaligned_alloc_ptr_ = 0x2b7009025b38 \"\", aligned_alloc_ptr_ = 0x2b7009025738 \"G\\351$X\", \r\n            alloc_bytes_remaining_ = 1024, hugetlb_size_ = 0, blocks_memory_ = 2048}, logs_ = {num_stack_items_ = 2, values_ = {0x2b7009025338, 0x2b7009025538, 0x2b7009025c01, \r\n              0x7a046d, 0x19baf00, 0x2b7009025c90, 0x0, 0x19b1038}, vect_ = std::vector of length 0, capacity 0}}\r\n        __PRETTY_FUNCTION__ = \"void rocksdb::DBImpl::BackgroundCallCompaction(void*)\"\r\n#6  0x0000000000556028 in rocksdb::DBImpl::BGWorkCompaction (arg=<optimized out>) at db/db_impl.cc:3101\r\n        ca = {db = 0x19baf00, m = 0x0}\r\n#7  0x00000000006a3da1 in rocksdb::ThreadPoolImpl::BGThread (this=this@entry=0x19b1000, thread_id=thread_id@entry=0) at util/threadpool_imp.cc:229\r\n        uniqueLock = @0x19b1010: {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 39, __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}}, \r\n          __size = '\\000' <repeats 12 times>, \"'\", '\\000' <repeats 26 times>, __align = 0}\r\n        function = 0x555fa0 <rocksdb::DBImpl::BGWorkCompaction(void*)>\r\n        arg = <optimized out>\r\n        decrease_io_priority = <optimized out>\r\n        low_io_priority = false\r\n#8  0x00000000006a3ed3 in rocksdb::BGThreadWrapper (arg=0x2b700c001b90) at util/threadpool_imp.cc:253\r\n        meta = 0x2b700c001b90\r\n        thread_id = 0\r\n        tp = 0x19b1000\r\n#9  0x00002b700756470a in start_thread (arg=0x2b7009026700) at pthread_create.c:333\r\n        __res = <optimized out>\r\n        pd = 0x2b7009026700\r\n        now = <optimized out>\r\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {47760187483904, 8172254731406293992, 0, 47760185373631, 2097152, 27145600, 2849015383635672040, 2848985044752546792}, \r\n              mask_was_saved = 0}}, priv = {pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}\r\n        not_first_call = <optimized out>\r\n        pagesize_m1 = <optimized out>\r\n        sp = <optimized out>\r\n        freesize = <optimized out>\r\n        __PRETTY_FUNCTION__ = \"start_thread\"\r\n#10 0x00002b700896282d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\nNo locals.\r\n\r\n---Type <return> to continue, or q <return> to quit---\r\nThread 2 (Thread 0x2b7008e25700 (LWP 8516)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\nNo locals.\r\n#1  0x00000000006a3ca3 in rocksdb::(anonymous namespace)::ConditionWait (lock=..., condition=...) at util/threadpool_imp.cc:97\r\nNo locals.\r\n#2  rocksdb::ThreadPoolImpl::BGThread (this=this@entry=0x19b10e8, thread_id=thread_id@entry=0) at util/threadpool_imp.cc:175\r\n        uniqueLock = @0x19b10f8: {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 1, __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}}, \r\n          __size = '\\000' <repeats 12 times>, \"\\001\", '\\000' <repeats 26 times>, __align = 0}\r\n        function = <optimized out>\r\n        arg = <optimized out>\r\n        decrease_io_priority = <optimized out>\r\n        low_io_priority = false\r\n#3  0x00000000006a3ed3 in rocksdb::BGThreadWrapper (arg=0x19ca4a0) at util/threadpool_imp.cc:253\r\n        meta = 0x19ca4a0\r\n        thread_id = 0\r\n        tp = 0x19b10e8\r\n#4  0x00002b700756470a in start_thread (arg=0x2b7008e25700) at pthread_create.c:333\r\n        __res = <optimized out>\r\n        pd = 0x2b7008e25700\r\n        now = <optimized out>\r\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {47760185382656, 8172254731406293992, 0, 140723184959775, 47760185383360, 0, 2849018957585333224, 2848985044752546792}, \r\n              mask_was_saved = 0}}, priv = {pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}\r\n        not_first_call = <optimized out>\r\n        pagesize_m1 = <optimized out>\r\n        sp = <optimized out>\r\n        freesize = <optimized out>\r\n        __PRETTY_FUNCTION__ = \"start_thread\"\r\n#5  0x00002b700896282d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\nNo locals.\r\n\r\nThread 1 (Thread 0x2b7007366600 (LWP 8499)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\nNo locals.\r\n#1  0x00002b700806956c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\nNo symbol table info available.\r\n#2  0x000000000069d219 in rocksdb::SyncPoint::Process (this=0xa8de80 <rocksdb::SyncPoint::GetInstance()::sync_point>, point=\"DBTest2::CompactionStall:1\", cb_arg=cb_arg@entry=0x0)\r\n    at util/sync_point.cc:141\r\n        lock = {_M_device = 0xa8df98 <rocksdb::SyncPoint::GetInstance()::sync_point+280>, _M_owns = true}\r\n        thread_id = {_M_thread = 47760157337088}\r\n        marker_iter = <optimized out>\r\n        callback_pair = <optimized out>\r\n#3  0x00000000004ad571 in rocksdb::DBTest2_CompactionStall_Test::TestBody (this=0x19b0b90) at db/db_test2.cc:1192\r\n        options = {<rocksdb::DBOptions> = {create_if_missing = true, create_missing_column_families = false, error_if_exists = false, paranoid_checks = true, env = 0x19b6120, \r\n---Type <return> to continue, or q <return> to quit---\r\n            rate_limiter = std::shared_ptr (empty) 0x0, sst_file_manager = std::shared_ptr (empty) 0x0, info_log = std::shared_ptr (empty) 0x0, \r\n            info_log_level = <incomplete type>, max_open_files = 5000, max_file_opening_threads = 16, max_total_wal_size = 0, statistics = std::shared_ptr (empty) 0x0, \r\n            disableDataSync = false, use_fsync = false, db_paths = std::vector of length 0, capacity 0, db_log_dir = \"\", wal_dir = \"\", \r\n            delete_obsolete_files_period_micros = 21600000000, base_background_compactions = -1, max_background_compactions = 40, max_subcompactions = 1, \r\n            max_background_flushes = 1, max_log_file_size = 0, log_file_time_to_roll = 0, keep_log_file_num = 1000, recycle_log_file_num = 0, \r\n            max_manifest_file_size = 18446744073709551615, table_cache_numshardbits = 6, WAL_ttl_seconds = 0, WAL_size_limit_MB = 0, manifest_preallocation_size = 4194304, \r\n            allow_os_buffer = true, allow_mmap_reads = false, allow_mmap_writes = false, use_direct_reads = false, allow_fallocate = true, is_fd_close_on_exec = true, \r\n            skip_log_error_on_recovery = false, stats_dump_period_sec = 600, advise_random_on_open = true, db_write_buffer_size = 0, \r\n            write_buffer_manager = std::shared_ptr (empty) 0x0, access_hint_on_compaction_start = rocksdb::DBOptions::NORMAL, new_table_reader_for_compaction_inputs = false, \r\n            compaction_readahead_size = 0, random_access_max_buffer_size = 1048576, writable_file_max_buffer_size = 1048576, use_adaptive_mutex = false, bytes_per_sync = 0, \r\n            wal_bytes_per_sync = 0, listeners = std::vector of length 1, capacity 1 = {std::shared_ptr (count 7, weak 0) 0x19b9d90}, enable_thread_tracking = false, \r\n            delayed_write_rate = 2097152, allow_concurrent_memtable_write = false, enable_write_thread_adaptive_yield = false, write_thread_max_yield_usec = 100, \r\n            write_thread_slow_yield_usec = 3, skip_stats_update_on_db_open = false, wal_recovery_mode = rocksdb::WALRecoveryMode::kTolerateCorruptedTailRecords, \r\n            allow_2pc = false, row_cache = std::shared_ptr (empty) 0x0, wal_filter = 0x0, fail_if_options_file_error = true, dump_malloc_stats = false, \r\n            avoid_flush_during_recovery = false, avoid_flush_during_shutdown = false}, <rocksdb::ColumnFamilyOptions> = {comparator = \r\n    0xa8c780 <rocksdb::BytewiseComparator()::bytewise>, merge_operator = std::shared_ptr (empty) 0x0, compaction_filter = 0x0, \r\n            compaction_filter_factory = std::shared_ptr (empty) 0x0, write_buffer_size = 16752640, max_write_buffer_number = 2, min_write_buffer_number_to_merge = 1, \r\n            max_write_buffer_number_to_maintain = 0, compression = <incomplete type>, compression_per_level = std::vector of length 0, capacity 0, \r\n            bottommost_compression = <incomplete type>, compression_opts = {window_bits = -14, level = -1, strategy = 0, max_dict_bytes = 0}, \r\n            prefix_extractor = std::shared_ptr (empty) 0x0, num_levels = 7, level0_file_num_compaction_trigger = 4, level0_slowdown_writes_trigger = 20, \r\n            level0_stop_writes_trigger = 24, max_mem_compaction_level = 146930464, target_file_size_base = 2097152, target_file_size_multiplier = 1, \r\n            max_bytes_for_level_base = 10485760, level_compaction_dynamic_level_bytes = false, max_bytes_for_level_multiplier = 10, \r\n            max_bytes_for_level_multiplier_additional = std::vector of length 7, capacity 7 = {1, 1, 1, 1, 1, 1, 1}, max_compaction_bytes = 0, soft_rate_limit = 0, \r\n            hard_rate_limit = 0, soft_pending_compaction_bytes_limit = 68719476736, hard_pending_compaction_bytes_limit = 274877906944, rate_limit_delay_max_milliseconds = 1000, \r\n            arena_block_size = 0, disable_auto_compactions = false, purge_redundant_kvs_while_flush = true, compaction_style = rocksdb::kCompactionStyleLevel, \r\n            compaction_pri = rocksdb::kByCompensatedSize, verify_checksums_in_compaction = true, compaction_options_universal = {size_ratio = 1, min_merge_width = 2, \r\n              max_merge_width = 4294967295, max_size_amplification_percent = 200, compression_size_percent = -1, stop_style = rocksdb::kCompactionStopStyleTotalSize, \r\n              allow_trivial_move = false}, compaction_options_fifo = {max_table_files_size = 1073741824}, max_sequential_skip_in_iterations = 8, memtable_factory = \r\n    std::shared_ptr (count 3, weak 0) 0x19df800, table_factory = std::shared_ptr (count 3, weak 0) 0x19d2f40, \r\n            table_properties_collector_factories = std::vector of length 0, capacity 0, inplace_update_support = false, inplace_update_num_locks = 10000, inplace_callback = 0x0, \r\n            memtable_prefix_bloom_size_ratio = 0, memtable_huge_page_size = 0, bloom_locality = 0, max_successive_merges = 0, min_partial_merge_operands = 2, \r\n            optimize_filters_for_hits = false, paranoid_file_checks = false, force_consistency_checks = false, report_bg_io_stats = false}, <No data fields>}\r\n        listener = 0x19b9d90\r\n        rnd = {seed_ = 1130133498}\r\n#4  0x000000000079ed97 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (location=0x806523 \"the test body\", method=<optimized out>, \r\n    object=<optimized out>) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3824\r\nNo locals.\r\n#5  testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=object@entry=0x19b0b90, \r\n    method=(void (testing::Test::*)(testing::Test * const)) 0x4acdf0 <rocksdb::DBTest2_CompactionStall_Test::TestBody()>, location=location@entry=0x806523 \"the test body\")\r\n    at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3860\r\nNo locals.\r\n#6  0x0000000000795b6c in testing::Test::Run (this=this@entry=0x19b0b90) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3897\r\n        impl = 0x184e900\r\n---Type <return> to continue, or q <return> to quit---\r\n#7  0x0000000000795d55 in testing::Test::Run (this=0x19b0b90) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3888\r\nNo locals.\r\n#8  testing::TestInfo::Run (this=0x184f9b0) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:4072\r\n        impl = 0x184e900\r\n        test = 0x19b0b90\r\n#9  0x0000000000795ed5 in testing::TestInfo::Run (this=<optimized out>) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:4047\r\nNo locals.\r\n#10 testing::TestCase::Run (this=0x184ecd0) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:4190\r\n        i = 10\r\n        impl = 0x184e900\r\n#11 0x000000000079634d in testing::TestCase::Run (this=<optimized out>) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:6100\r\nNo locals.\r\n#12 testing::internal::UnitTestImpl::RunAllTests (this=0x184e900) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:6062\r\n        test_index = 0\r\n        i = 0\r\n        should_shard = <optimized out>\r\n        has_tests_to_run = true\r\n        forever = false\r\n        in_subprocess_for_death_test = <optimized out>\r\n        failed = false\r\n        repeat = 1\r\n        this = 0x184e900\r\n#13 0x000000000079f2a7 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (\r\n    location=0x805a08 \"auxiliary test code (environments or event listeners)\", method=<optimized out>, object=<optimized out>)\r\n    at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3824\r\nNo locals.\r\n#14 testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x184e900,\r\n    method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0x795fc0 <testing::internal::UnitTestImpl::RunAllTests()>,\r\n    location=location@entry=0x805a08 \"auxiliary test code (environments or event listeners)\") at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3860\r\nNo locals.\r\n#15 0x000000000079665f in testing::UnitTest::Run (this=0xa91ba0 <testing::UnitTest::GetInstance()::instance>) at third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:5680\r\n        in_death_test_child_process = <optimized out>\r\n        premature_exit_file = <optimized out>\r\n#16 0x000000000040a55d in RUN_ALL_TESTS () at ./third-party/gtest-1.7.0/fused-src/gtest/gtest.h:20722\r\nNo locals.\r\n#17 main (argc=1, argv=0x7ffcab739008) at db/db_test2.cc:2192\r\nNo locals.\r\n\r\n\r\n\r\n(gdb) up\r\n#1  0x00002b700806956c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n(gdb) up\r\n#2  0x000000000069d219 in rocksdb::SyncPoint::Process (this=0xa8de80 <rocksdb::SyncPoint::GetInstance()::sync_point>, point=\"DBTest2::CompactionStall:1\", cb_arg=cb_arg@entry=0x0)\r\n    at util/sync_point.cc:141\r\n141         cv_.wait(lock);\r\n(gdb) p cv_\r\n$1 = {_M_cond = {__data = {__lock = 0, __futex = 102, __total_seq = 52, __wakeup_seq = 50, __woken_seq = 50,\r\n      __mutex = 0xa8df98 <rocksdb::SyncPoint::GetInstance()::sync_point+280>, __nwaiters = 4, __broadcast_seq = 50},\r\n    __size = \"\\000\\000\\000\\000f\\000\\000\\000\\064\\000\\000\\000\\000\\000\\000\\000\\062\\000\\000\\000\\000\\000\\000\\000\\062\\000\\000\\000\\000\\000\\000\\000\\230\\337\\250\\000\\000\\000\\000\\000\\004\\000\\000\\000\\062\\000\\000\", __align = 438086664192}}\r\n(gdb) p cv_.__mutex\r\nThere is no member or method named __mutex.\r\n(gdb) p cv_._M_cond\r\n$2 = {__data = {__lock = 0, __futex = 102, __total_seq = 52, __wakeup_seq = 50, __woken_seq = 50, __mutex = 0xa8df98 <rocksdb::SyncPoint::GetInstance()::sync_point+280>,\r\n    __nwaiters = 4, __broadcast_seq = 50},\r\n  __size = \"\\000\\000\\000\\000f\\000\\000\\000\\064\\000\\000\\000\\000\\000\\000\\000\\062\\000\\000\\000\\000\\000\\000\\000\\062\\000\\000\\000\\000\\000\\000\\000\\230\\337\\250\\000\\000\\000\\000\\000\\004\\000\\000\\000\\062\\000\\000\", __align = 438086664192}\r\n\r\n</pre>\r\n\r\n\r\n<pre>\r\nbuild@1a9fff0208e0:~$ cat /source/t/log-db_test2\r\n[==========] Running 37 tests from 5 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 19 tests from DBTest2\r\n[ RUN      ] DBTest2.IteratorPropertyVersionNumber\r\n[       OK ] DBTest2.IteratorPropertyVersionNumber (2 ms)\r\n[ RUN      ] DBTest2.CacheIndexAndFilterWithDBRestart\r\n[       OK ] DBTest2.CacheIndexAndFilterWithDBRestart (17 ms)\r\n[ RUN      ] DBTest2.MaxSuccessiveMergesChangeWithDBRecovery\r\n[       OK ] DBTest2.MaxSuccessiveMergesChangeWithDBRecovery (4 ms)\r\n[ RUN      ] DBTest2.SharedWriteBufferLimitAcrossDB\r\n[       OK ] DBTest2.SharedWriteBufferLimitAcrossDB (10 ms)\r\n[ RUN      ] DBTest2.WalFilterTest\r\nTesting with complete WAL processing\r\nTesting with ignoring record 1 only\r\nTesting with stopping replay from record 1\r\nTesting with complete WAL processing\r\n[       OK ] DBTest2.WalFilterTest (34 ms)\r\n[ RUN      ] DBTest2.WalFilterTestWithChangeBatch\r\n[       OK ] DBTest2.WalFilterTestWithChangeBatch (10 ms)\r\n[ RUN      ] DBTest2.WalFilterTestWithChangeBatchExtraKeys\r\n[       OK ] DBTest2.WalFilterTestWithChangeBatchExtraKeys (9 ms)\r\n[ RUN      ] DBTest2.WalFilterTestWithColumnFamilies\r\n[       OK ] DBTest2.WalFilterTestWithColumnFamilies (8 ms)\r\n[ RUN      ] DBTest2.PresetCompressionDict\r\n[       OK ] DBTest2.PresetCompressionDict (144 ms)\r\n[ RUN      ] DBTest2.CompressionOptions\r\n[       OK ] DBTest2.CompressionOptions (36 ms)\r\n[ RUN      ] DBTest2.CompactionStall\r\n</pre>",
	"number": 1497,
	"title": "test failure: ./db_test2 - all threads in pthread_cond_wait"
}, {
	"body": "It may be possible for a scenario to arise where stale WAL logs are deleted out of order. This has the potential to create a gap in the logs read from on recovery.\r\n\r\nconsider scenario:\r\ntransaction is prepared in log A.\r\ntransaction is committed in log B.\r\ncurrent log is C, A and B are release.\r\nlog B is deleted.\r\nrecovery happens.\r\n\r\nIn this scenario the transaction would be recovered as if is had not been committed because we cannot have reliable knowledge that only log C was needed for recovery. It IS possible to detect that log B was missing and that all logs before must have been released in the previous incarnation. We use this knowledge to clear all recovered transactions when we detect a gap.",
	"number": 1495,
	"title": "delete transactions recovered from stale logs"
}, {
	"body": "as ubuntu-16.04 defaults to link /bin/sh to /bin/dash the following compile error is generated for the make check target.\r\n\r\n<pre>\r\n14:56:27 make[2]: Leaving directory '/source'\r\n14:56:27 make[2]: Entering directory '/source'\r\n14:56:27 TEST_BINARY=db_universal_compaction_test; \\\r\n14:56:27   TEST_NAMES=` \\\r\n14:56:27     ./$TEST_BINARY --gtest_list_tests \\\r\n14:56:27     | perl -n \\\r\n14:56:27       -e 's/ *\\#.*//;' \\\r\n14:56:27       -e '/^(\\s*)(\\S+)/; !$1 and do {$p=$2; break};'\t\\\r\n14:56:27       -e 'print qq! $p$2!'`; \\\r\n14:56:27 for TEST_NAME in $TEST_NAMES; do \\\r\n14:56:27 \tTEST_SCRIPT=t/run-$TEST_BINARY-${TEST_NAME//\\//-}; \\\r\n14:56:27 \techo \"  GEN     \" $TEST_SCRIPT; \\\r\n14:56:27     printf '%s\\n' \\\r\n14:56:27       '#!/bin/sh' \\\r\n14:56:27       \"d=\\/dev/shm/rocksdb.x2Z6$TEST_SCRIPT\" \\\r\n14:56:27       'mkdir -p $d' \\\r\n14:56:27       \"TEST_TMPDIR=\\$d  ./$TEST_BINARY --gtest_filter=$TEST_NAME\" \\\r\n14:56:27 \t> $TEST_SCRIPT; \\\r\n14:56:27 \tchmod a=rx $TEST_SCRIPT; \\\r\n14:56:27 done\r\n14:56:27 /bin/sh: 9: Bad substitution\r\n</pre>\r\n\r\nA SHELL=/bin/bash at the top of the Makefile corrects this.",
	"number": 1494,
	"title": "Makefile with SHELL = dash (/bin/sh symlink) causes \"/bin/sh: 9: Bad substitution\" make error"
}, {
	"body": "This didn't occur with 3.8.\r\n\r\nclang-3.9 is from\r\n<pre>\r\ndeb http://apt.llvm.org/xenial/ llvm-toolchain-xenial-3.9 main\r\napt-get install clang-3.9\r\n</pre>\r\n\r\nBuild:\r\n<pre>\r\n${CC} --version\r\n+ clang-3.9 --version\r\nclang version 3.9.1-svn281634-1~exp1 (branches/release_39)\r\nTarget: x86_64-pc-linux-gnu\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\n${CXX} --version\r\n+ clang++-3.9 --version\r\nclang version 3.9.1-svn281634-1~exp1 (branches/release_39)\r\nTarget: x86_64-pc-linux-gnu\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\n${AR:-ar} --version\r\n+ ar --version\r\nGNU ar (GNU Binutils for Ubuntu) 2.26.1\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis program is free software; you may redistribute it under the terms of\r\nthe GNU General Public License version 3 or (at your option) any later version.\r\nThis program has absolutely no warranty.\r\n\r\nsed -i -e '1s/^/SHELL=\\/bin\\/bash/' Makefile\r\n+ sed -i -e '1s/^/SHELL=\\/bin\\/bash/' Makefile\r\n\r\nmake -j\"${CPUS}\" --output-sync=target V=1 \"${BUILD_TYPE}\"\r\n+ make -j24 --output-sync=target V=1 check\r\nMakefile:101: Warning: Compiling in debug mode. Don't use the resulting binary in production\r\nIn file included from utilities/column_aware_encoding_exp.cc:22:\r\nIn file included from ./table/format.h:19:\r\nIn file included from ./table/persistent_cache_helper.h:10:\r\nIn file included from ./util/statistics.h:17:\r\n./util/thread_local.h:205:5: error: macro expansion producing 'defined' has undefined behavior [-Werror,-Wexpansion-to-defined]\r\n#if ROCKSDB_SUPPORT_THREAD_LOCAL\r\n    ^\r\n./util/thread_local.h:22:4: note: expanded from macro 'ROCKSDB_SUPPORT_THREAD_LOCAL'\r\n  !defined(OS_WIN) && !defined(OS_MACOSX) && !defined(IOS_CROSS_COMPILE)\r\n   ^\r\n./util/thread_local.h:205:5: error: macro expansion producing 'defined' has undefined behavior [-Werror,-Wexpansion-to-defined]\r\n./util/thread_local.h:22:24: note: expanded from macro 'ROCKSDB_SUPPORT_THREAD_LOCAL'\r\n  !defined(OS_WIN) && !defined(OS_MACOSX) && !defined(IOS_CROSS_COMPILE)\r\n                       ^\r\n./util/thread_local.h:205:5: error: macro expansion producing 'defined' has undefined behavior [-Werror,-Wexpansion-to-defined]\r\n./util/thread_local.h:22:47: note: expanded from macro 'ROCKSDB_SUPPORT_THREAD_LOCAL'\r\n  !defined(OS_WIN) && !defined(OS_MACOSX) && !defined(IOS_CROSS_COMPILE)\r\n                                              ^\r\n3 errors generated.\r\n</pre>",
	"number": 1493,
	"title": "clang-3.9 compile error: error: macro expansion producing 'defined' has undefined behavior "
}, {
	"body": "Another error from far too recent compiler.\r\n\r\n<pre>\r\n${CXX} --version\r\n+ x86_64-unknown-linux-gnu-g++ --version\r\nx86_64-unknown-linux-gnu-g++ (crosstool-NG crosstool-ng-1.22.0-201-g11cb2dd) 7.0.0 20161109 (experimental)\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n...\r\n\r\nx86_64-unknown-linux-gnu-g++ -I/usr/include -I/usr/include/x86_64-linux-gnu -m64 -O3 -g -mtune=native -std=c++11  -g -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -Werror -I. -I./include -std=c++11 -I/usr/include -I/usr/include/x86_64-linux-gnu -m64 -O3 -g -mtune=native -std=c++11 -DROCKSDB_PLATFORM_POSIX -DROCKSDB_LIB_IO_POSIX -I/usr/include -I/usr/include/x86_64-linux-gnu -m64 -O3 -g -mtune=native -DOS_LINUX -fno-builtin-memcmp -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS=gflags -DZLIB -DBZIP2 -DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PTHREAD_ADAPTIVE_MUTEX -DROCKSDB_BACKTRACE -march=native   -isystem ./third-party/gtest-1.7.0/fused-src -O2 -fno-omit-frame-pointer -momit-leaf-frame-pointer -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -c db/db_iter.cc -o db/db_iter.o \r\nIn file included from ./util/statistics.h:17:0,\r\n                 from ./util/stop_watch.h:8,\r\n                 from ./db/merge_helper.h:19,\r\n                 from db/db_iter.cc:19:\r\n./util/thread_local.h:66:16: error: 'function' in namespace 'std' does not name a template type\r\n   typedef std::function<void(void*, void*)> FoldFunc;\r\n                ^~~~~~~~\r\n./util/thread_local.h:71:13: error: 'FoldFunc' has not been declared\r\n   void Fold(FoldFunc func, void* res);\r\n             ^~~~~~~~\r\n./util/thread_local.h:142:28: error: 'FoldFunc' has not been declared\r\n     void Fold(uint32_t id, FoldFunc func, void* res);\r\n                            ^~~~~~~~\r\nMakefile:1472: recipe for target 'db/db_iter.o' failed\r\n</pre>",
	"number": 1492,
	"title": "gcc-7.0.0 compile error: error: 'function' in namespace 'std' does not name a template type"
}, {
	"body": "Using really new compiler:\r\n<pre>\r\n+ x86_64-unknown-linux-gnu-g++ --version\r\nx86_64-unknown-linux-gnu-g++ (crosstool-NG crosstool-ng-1.22.0-201-g11cb2dd) 7.0.0 20161109 (experimental)\r\n..\r\n..\r\nx86_64-unknown-linux-gnu-g++ -I/usr/include -I/usr/include/x86_64-linux-gnu -m64 -O3 -g -mtune=native -std=c++11  -g -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -Werror -I. -I./include -std=c++11 -I/usr/include -I/usr/include/x86_64-linux-gnu -m64 -O3 -g -mtune=native -std=c++11 -DROCKSDB_PLATFORM_POSIX -DROCKSDB_LIB_IO_POSIX -I/usr/include -I/usr/include/x86_64-linux-gnu -m64 -O3 -g -mtune=native -DOS_LINUX -fno-builtin-memcmp -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS=gflags -DZLIB -DBZIP2 -DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PTHREAD_ADAPTIVE_MUTEX -DROCKSDB_BACKTRACE -march=native   -isystem ./third-party/gtest-1.7.0/fused-src -O2 -fno-omit-frame-pointer -momit-leaf-frame-pointer -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -c db/internal_stats.cc -o db/internal_stats.o \r\ndb/internal_stats.cc: In member function 'void rocksdb::InternalStats::DumpCFStats(std::__cxx11::string*)':\r\ndb/internal_stats.cc:719:6: error: '%.2f' directive output truncated writing between 4 and 317 bytes into a region of size 0 [-Werror=format-length=]\r\n void InternalStats::DumpCFStats(std::string* value) {\r\n      ^~~~~~~~~~~~~\r\ndb/internal_stats.cc:854:42: note: format output between 98 and 1664 bytes into a destination of size 1000\r\n            compact_micros / kMicrosInSec);\r\n                                          ^\r\ndb/internal_stats.cc:719:6: error: '%.2f' directive output truncated writing between 4 and 317 bytes into a region of size 0 [-Werror=format-length=]\r\n void InternalStats::DumpCFStats(std::string* value) {\r\n      ^~~~~~~~~~~~~\r\ndb/internal_stats.cc:873:46: note: format output between 96 and 1662 bytes into a destination of size 1000\r\n       interval_compact_micros / kMicrosInSec);\r\n                                              ^\r\ncc1plus: all warnings being treated as errors\r\n</pre>\r\n\r\nChanging buf size from 1k -> 2k would be sufficient.",
	"number": 1491,
	"title": "internal_stats.cc - buf not big enough for potential maximum sprintf size"
}, {
	"body": "Summary:\r\nAdd a parameter to allow users to continue using the iterator even after committing a transaction.\r\n\r\nTest Plan: Add a unit test",
	"number": 1489,
	"title": "Transaction::Commit() adds a parameter for not cleaning the write batch"
}, {
	"body": "Added a tombstone-collapsing mode to RangeDelAggregator, which eliminates overlap in the TombstoneMap. In this mode, we can check whether a tombstone covers an internal key using upper_bound() (i.e., binary search). However, the tradeoff is the overhead to add tombstones is now higher, so at first I've only enabled it for range scans (compaction/flush/user iterators), where we expect a high number of calls to ShouldDelete() for the same tombstones. Point queries like Get() will still use the linear scan approach.",
	"number": 1481,
	"title": "Collapse range deletions for faster range scans"
}, {
	"body": "In dynamic leveled compaction, when calculating read bytes, output level bytes may be wronglyl calculated as input level inputs. Fix it.",
	"number": 1475,
	"title": "Fix mis-reporting of compaction read bytes to the base level"
}, {
	"body": "Hello and thank you for RocksDB,\r\n \r\nRather than `sst_dump --show_compression_sizes` just showing the compression sizes:\r\n\r\n    Compression: kNoCompression Size:     65040323 \r\n    Compression: kSnappyCompression Size: 64355577 \r\n    Compression: kZlibCompression Size:   63778913 \r\n    Compression: kLZ4Compression Size:    64209486\r\n    Compression: kLZ4HCCompression Size:  64041757\r\n\r\nI found also showing the number of blocks compressed and not compressed to be valuable:\r\n \r\n    Compression: kNoCompression     Size: 65040323 Blocks compressed:   0 Blocks not compressed:    0\r\n    Compression: kSnappyCompression Size: 64355577 Blocks compressed: 212 Blocks not compressed: 1849\r\n    Compression: kZlibCompression   Size: 63778913 Blocks compressed: 358 Blocks not compressed: 1703\r\n    Compression: kLZ4Compression    Size: 64209486 Blocks compressed: 249 Blocks not compressed: 1812\r\n    Compression: kLZ4HCCompression  Size: 64041757 Blocks compressed: 292 Blocks not compressed: 1769\r\n \r\nI am testing compression options on values that are already compressed with LZ4, and have bumped up against the 12.5% threshold for `GoodCompressionRatio` (originally from ldb) as I have datasets where many blocks compress by 10%.\r\n \r\nCurrently `NUMBER_BLOCK_NOT_COMPRESSED` is only incremented when compression is aborted because the block is too big or did not pass verification ( added in https://github.com/facebook/rocksdb/commit/f43c8262c264b3aa2ff571ff63e4cdb5eb3288cd ) , it does not require `ShouldReportDetailedTime(r->ioptions.env, r->ioptions.statistics)` to be true, it is not incremented when a block is not compressed because it does not compress by 12.5%.  `NUMBER_BLOCK_COMPRESSED` is incremented when a compressed block is used/written, it requries `ShouldReportDetailedTime(r->ioptions.env, r->ioptions.statistics)` to be true, it was added in https://github.com/facebook/rocksdb/commit/9430333f84635ffb2586f6b5e4debc6051258943 .\r\n \r\nWhen the code ( with `NUMBER_BLOCK_NOT_COMPRESSED` (aborted compression) ) had `NUMBER_BLOCK_COMPRESSED` (compression meets threshold) added, I suspect counting blocks that fail to meet the the threshold was overlooked, or maybe just not a priority.\r\n \r\nThe output above with the number of blocks not compressed uses a change that changes `NUMBER_BLOCK_NOT_COMPRESSED` to also be incremented when a block does not compress by 12.5%.  This is a change in behaviour that is likely not acceptable:\r\n \r\n    > diff -du tools/sst_dump_tool.cc.orig tools/sst_dump_tool.cc\r\n    --- tools/sst_dump_tool.cc.orig 2016-11-04 09:33:20.505992575 -0400\r\n    +++ tools/sst_dump_tool.cc      2016-11-08 11:37:45.503881967 -0500\r\n    @@ -178,6 +178,8 @@\r\n     int SstFileReader::ShowAllCompressionSizes(size_t block_size) {\r\n       ReadOptions read_options;\r\n       Options opts;\r\n    +  opts.statistics = rocksdb::CreateDBStatistics();\r\n    +  opts.statistics->stats_level_ = StatsLevel::kAll;\r\n       const ImmutableCFOptions imoptions(opts);\r\n       rocksdb::InternalKeyComparator ikc(opts.comparator);\r\n       std::vector<std::unique_ptr<IntTblPropCollectorFactory> >\r\n    @@ -206,7 +208,9 @@\r\n                                       false /* skip_filters */, column_family_name);\r\n           uint64_t file_size = CalculateCompressedTableSize(tb_opts, block_size);\r\n           fprintf(stdout, \"Compression: %s\", i.second);\r\n    -      fprintf(stdout, \" Size: %\" PRIu64 \"\\n\", file_size);\r\n    +      fprintf(stdout, \" Size: %\" PRIu64, file_size);\r\n    +      fprintf(stdout, \" Number of blocks compressed: %\" PRIu64, opts.statistics->getAndResetTickerCount(NUMBER_BLOCK_COMPRESSED));\r\n    +      fprintf(stdout, \" Number of blocks not compressed: %\" PRIu64 \"\\n\", opts.statistics->getAndResetTickerCount(NUMBER_BLOCK_NOT_COMPRESSED));\r\n         } else {\r\n           fprintf(stdout, \"Unsupported compression type: %s.\\n\", i.second);\r\n         }\r\n \r\n    > diff -du4 table/block_based_table_builder.cc.orig table/block_based_table_builder.cc\r\n    --- table/block_based_table_builder.cc.orig     2016-11-08 11:39:50.682322192 -0500\r\n    +++ table/block_based_table_builder.cc  2016-11-08 07:46:55.875903674 -0500\r\n    @@ -706,17 +706,19 @@\r\n       if (abort_compression) {\r\n         RecordTick(r->ioptions.statistics, NUMBER_BLOCK_NOT_COMPRESSED);\r\n         type = kNoCompression;\r\n         block_contents = raw_block_contents;\r\n    -  }\r\n    -  else if (type != kNoCompression &&\r\n    -            ShouldReportDetailedTime(r->ioptions.env,\r\n    -              r->ioptions.statistics)) {\r\n    -    MeasureTime(r->ioptions.statistics, COMPRESSION_TIMES_NANOS,\r\n    -      timer.ElapsedNanos());\r\n    -    MeasureTime(r->ioptions.statistics, BYTES_COMPRESSED,\r\n    -      raw_block_contents.size());\r\n    -    RecordTick(r->ioptions.statistics, NUMBER_BLOCK_COMPRESSED);\r\n    +  } else if (ShouldReportDetailedTime(r->ioptions.env,\r\n    +                                     r->ioptions.statistics)) {\r\n    +    if (type != kNoCompression) {\r\n    +      MeasureTime(r->ioptions.statistics, COMPRESSION_TIMES_NANOS,\r\n    +                 timer.ElapsedNanos());\r\n    +      MeasureTime(r->ioptions.statistics, BYTES_COMPRESSED,\r\n    +                 raw_block_contents.size());\r\n    +      RecordTick(r->ioptions.statistics, NUMBER_BLOCK_COMPRESSED);\r\n    +    } else if (type != r->compression_type) {\r\n    +      RecordTick(r->ioptions.statistics, NUMBER_BLOCK_NOT_COMPRESSED);\r\n    +    }\r\n       }\r\n    \r\n       WriteRawBlock(block_contents, type, handle);\r\n       r->compressed_output.clear();\r\n \r\nAlternatively `NUMBER_BLOCK_NOT_COMPRESSED` could be left unchanged and a new metric, `NUMBER_BLOCK_COMPRESSION_RATIO_FAIL` added.  Avoiding any change in behaviour (for existing counters) for consumers.\r\n \r\nOr `NUMBER_BLOCK_ABORT_COMPRESSION` could be added and used as `NUMBER_BLOCK_NOT_COMPRESSED` is used now (aborts), and `NUMBER_BLOCK_NOT_COMPRESSED` used for ratio fails (when ShouldReportDetailedTime) and aborted compression due to size or failed verify.",
	"number": 1474,
	"title": "Show number of blocks compressed and not compressed when using \"sst_dump --show_compression_sizes\"."
}, {
	"body": "The tools are built, but `install` does not install them, nor is there a separate `install-tools` target.",
	"number": 1473,
	"title": "Makefile does not install tools"
}, {
	"body": "When there are a lot of tombstone keys, we will benefit a lot from iterator's upper bound when run forward scan operation. Is there any plan to support lower bound for iterator, it is useful when doing reverse scan.",
	"number": 1471,
	"title": "Iterator with lower bound"
}, {
	"body": "",
	"number": 1469,
	"title": "Improve comments related to compression levels."
}, {
	"body": "It is helpful when we want to get a specified statistics in these languages that limited to call C api like rust. The `rocksdb_options_statistics_get_string` return the whole statistics in the form of a string is not we want. ",
	"number": 1468,
	"title": "Add C api to get specified statistics"
}, {
	"body": "Hi,\r\n\r\nI find a strange behavior that if I ingest external file with move_files set to false, the snapshot can't read the ingest value; but if I set move_files to true, it always see it. Following is a simple example:\r\n\r\n```\r\n#include <vector>\r\n#include <unistd.h>\r\n#include <iostream>\r\n#include \"rocksdb/db.h\"\r\n\r\nusing namespace rocksdb;\r\n\r\nvoid assert_ok(const Status& s) {\r\n    if (!s.ok()) {\r\n        std::cout << s.ToString() << std::endl;\r\n        exit(1);\r\n    }\r\n}\r\n\r\nint main() {\r\n    Options options;\r\n    ReadOptions read_opt;\r\n    WriteOptions write_opt;\r\n    std::string db_path = \"/tmp/testingest_db\";\r\n    std::string sst_file_path = \"/tmp/testingest.sst\";\r\n    std::string origin_val, new_val;\r\n\r\n    DestroyDB(db_path, options);\r\n    options.create_if_missing = true;\r\n    DB* db;\r\n    assert_ok(DB::Open(options, db_path, &db));\r\n\r\n    assert_ok(db->Put(write_opt, Slice(\"key1\"), Slice(\"origin val\")));\r\n    const Snapshot* snap = db->GetSnapshot();\r\n    read_opt.snapshot = snap;\r\n    assert_ok(db->Get(read_opt, Slice(\"key1\"), &origin_val));\r\n\r\n    unlink(sst_file_path.c_str());\r\n    SstFileWriter sst_file_writer(EnvOptions(), options, options.comparator);\r\n    assert_ok(sst_file_writer.Open(sst_file_path));\r\n    assert_ok(sst_file_writer.Add(Slice(\"key1\"), Slice(\"new val\")));\r\n    assert_ok(sst_file_writer.Finish());\r\n\r\n    IngestExternalFileOptions ifo;\r\n    ifo.move_files = true;\r\n    assert_ok(db->IngestExternalFile({sst_file_path}, ifo));\r\n\r\n    assert_ok(db->Get(read_opt, Slice(\"key1\"), &new_val));\r\n    std::cout << origin_val << std::endl << new_val << std::endl;\r\n    assert(origin_val == new_val);\r\n    db->ReleaseSnapshot(snap);\r\n    delete db;\r\n    return 0;\r\n}\r\n```\r\n",
	"number": 1463,
	"title": "snapshot consistency is broken when IngestExternalFile with move_files set to true"
}, {
	"body": "The C++ new operator implicitly uses `malloc` of course, but here I want to link to a version of jemalloc that has the `je_` prefixes (complex long-winded reason). Is this possible?",
	"number": 1462,
	"title": "Way to add je_ prefix to malloc calls?"
}, {
	"body": "There is a trouble with me, about rocksdb core. The core-dump shows below. Pls help me, any suggestion is welcome. The version is _**4.8**_\r\n![image](https://cloud.githubusercontent.com/assets/19619615/19968178/69cecab4-a20e-11e6-90ce-05fae92a07c0.png)\r\n\r\n<img width=\"1175\" alt=\"2016-11-03 21 43 37\" src=\"https://cloud.githubusercontent.com/assets/19619615/19968222/a449b190-a20e-11e6-9b52-212a3552c6ab.png\">\r\n",
	"number": 1461,
	"title": "Program terminated with signal SIGSEGV, Segmentation fault about rocksdb 4.8"
}, {
	"body": "Fixes #1442.\r\n\r\nNo longer add -ljemalloc to library link lines.\r\n\r\nNo longer call malloc_print_stats() unless we're actually using jemalloc (use weak linking).\r\n",
	"number": 1453,
	"title": "Loosen dependency on jemalloc"
}, {
	"body": "I think It would be better to check expiration before return it.\r\n\r\nWe could check the timestamp of data on `SanityCheckTimestamp`. and.. honestly I don't know why it doesn't now\r\nhttps://github.com/facebook/rocksdb/blob/master/utilities/ttl/db_ttl_impl.cc#L192",
	"number": 1445,
	"title": "non intuitive behavior of DBWithTTL:get for staled data"
}, {
	"body": "I have a question with the memory rocksdb use.\r\nthe question is described as following:\r\n1. I use java-rocksdb\r\n2. I open about 8 rocksdb use in one jvm.\r\n3. I find the rocksdb::Arena::AllocateNewBlock use a lot of off-heap memory and it cant be gc with the java system. At last it will run out of physical memory and the programme will be very slow.\r\n\r\nThen what's the gc strategy the Arena use and how can i gc or reduce the memory rocksdb::Arena::AllocateNewBlock use.\r\n\r\nHere is some options i see and the memory of the off-heap memory the jvm use:\r\noptions:\r\n        BlockBasedTableConfig table_options = new BlockBasedTableConfig();\r\n        table_options.setBlockSize(256 * SizeUnit.KB);\r\n        table_options.setBlockCacheSize(320 / n * SizeUnit.MB);//we have n rocksdb in one jvm\r\n        table_options.setFilter(new BloomFilter(10, false));\r\n        table_options.setCacheIndexAndFilterBlocks(false);\r\n        options.setCreateIfMissing(true);\r\n        options.setTableFormatConfig(table_options);\r\n        options.setMaxWriteBufferNumber(2);\r\n        options.setWriteBufferSize(32 / n * SizeUnit.MB);\r\n        options.setTargetFileSizeBase(32 * SizeUnit.MB);\r\n        options.setMaxBackgroundFlushes(1);\r\n        options.setMaxBackgroundCompactions(2);\r\n        options.setLevelZeroFileNumCompactionTrigger(4);\r\n        options.setLevelZeroSlowdownWritesTrigger(10);\r\n        options.setLevelZeroStopWritesTrigger(20);\r\n        options.setNumLevels(7);\r\n        options.setAllowOsBuffer(true);\r\n\r\noff-heap memory :\r\nTotal: 3627.3 MB\r\n  3076.0  84.8%  84.8%   3076.0  84.8% rocksdb::Arena::AllocateNewBlock\r\n   348.1   9.6%  94.4%    348.1   9.6% rocksdb::UncompressBlockContents\r\n   149.3   4.1%  98.5%    497.3  13.7% rocksdb::ReadBlockContents\r\n    27.8   0.8%  99.3%     27.8   0.8% os::malloc\r\n    18.8   0.5%  99.8%     18.8   0.5% init\r\n     1.9   0.1%  99.8%      1.9   0.1% ObjectSynchronizer::omAlloc\r\n     1.9   0.1%  99.9%      1.9   0.1% readCEN\r\n     0.8   0.0%  99.9%      0.8   0.0% rocksdb::WritableFileWriter::WritableFileWriter\r\n     0.6   0.0%  99.9%      0.6   0.0% rocksdb::VersionSet::LogAndApply\r\n     0.5   0.0%  99.9%      0.5   0.0% updatewindow\r\n     0.4   0.0% 100.0%      0.4   0.0% tls_get_addr_tail\r\n     0.2   0.0% 100.0%      0.2   0.0% rocksdb::Cache::~Cache\r\n     0.2   0.0% 100.0%      0.2   0.0% rocksdb::NewLRUCache\r\n\r\n",
	"number": 1444,
	"title": "rocksdb::Arena::AllocateNewBlock use a lot of off-heap memory in java-rocksdb"
}, {
	"body": "Hi, I see adamretter has implement the API, see: https://github.com/adamretter/rocksdb/blob/java-transactions/java/src/main/java/org/rocksdb/OptimisticTransactionDB.java\n\nWhen you merge into the repository and release new version?\n",
	"number": 1443,
	"title": "When TransactionDB/OptimisticTransactionDB add to the Java API?"
}, {
	"body": "RocksDB (via `build_tools/build_detect_platform`) determines at build time whether to use jemalloc, tcmalloc, or none. This is wrong for two reasons:\n1. The choice of memory allocator should be left to the main program, not a library, so RocksDB shouldn't add `-ljemalloc` (or `-ltcmalloc`) to its own link line. Until recently, Linux would propagate a shared library's DT_NEEDED entries to the parent (see `--add-needed` or `--copy-dt-needed-entries` in `man ld`), so RocksDB's actions would force the main program to use jemalloc / tcmalloc. If we want to use `jemalloc` / `tcmalloc` in RocksDB **executables** (such as `ldb` and `db_bench`), then `-ljemalloc` / `-ltcmalloc` should be added to those executables' link lines.\n2. If RocksDB determines that `jemalloc` is available, it assumes it will only be linked into binaries that use it as its memory allocator, and thus it calls `jemalloc`-specific functions, see the call to `malloc_stats_print` in https://github.com/facebook/rocksdb/blob/c90c48d3c89141bc8839a0fa69a6b5b2d4f03d08/db/db_impl.cc#L599. This leads to a link error if you try to link RocksDB into an executable that doesn't use `jemalloc`. The proper thing to do is to declare `malloc_stats_print` as weak, and check for its existence at runtime, but even that's technically not enough -- it should check that `jemalloc` is actually in use. I solved this in `folly` in a horrible way that I'm not at all proud of: https://github.com/facebook/folly/blob/1a48bcd9c77353535820239dff3e7f5f7c1e93bb/folly/Malloc.h#L143\n",
	"number": 1442,
	"title": "RocksDB shouldn't determine at build time whether to use jemalloc / tcmalloc"
}, {
	"body": "FreeBSD 11.0 with cmake clang 3.8.0\n\n```\n/home/anan/projects/rocksdb/util/env_test.cc:1067:22: error: use of undeclared identifier 'O_DIRECT'\n```\n",
	"number": 1440,
	"title": "freebsd build error with cmake "
}, {
	"body": "Hi,\n\nTogether with @thatsafunnyname we've noticed optimistic transactions failing to commit straight after reopening a RocksDB. We were able to reduce the problem to a simple reproducer and add it as a unit test to optimistic_transactions_test.cc.\n\nWe believe this is a regression introduced in a400336, and can be reproduced reliably.\n\nThe transaction fails with `Operation failed. Try again.: Transaction could not\ncheck for conflicts for operation at SequenceNumber 1 as the MemTable\nonly contains changes newer than SequenceNumber 2.  Increasing the value\nof the max_write_buffer_number_to_maintain option could reduce the\nfrequency of this error` error.\n\nWe believe the problem is caused by creating a memtable with a wrong sequence number (an off by one error).\n\nmdlugajczyk/rocksdb@5ebfd2623a01e69a4cbeae3ed2b788f2a84056ad includes the test demonstrating the problem and a proposed solution.\n",
	"number": 1431,
	"title": "Wrong sequence number used for creating memtable"
}, {
	"body": "32 bit build is broken due to use of pedantic /WX flag in compile options.\nThere is a couple(not too many)warnings about casts from int64 to int or std::size_t. Naturally, GCC never warns about conversion errors, you can cast long longs to  chars, so this only pops up on Windows.\n\nThe easiest way to fix broken build seems to be to provide way to compile with less pedantic settings. Ideally, CMakeLists.txt could provide a way not to force own compiler flags, except the absolutely necessary ones (for gcc, this seems to be --std=c++11)\n\nAlternatively, one could fix the warnings and use appropriate data types instead of int64 everywhere.\n",
	"number": 1430,
	"title": "Windows x86 build broken due to warning treated as error"
}, {
	"body": "Also fixes #1286\n",
	"number": 1423,
	"title": "Fix JNI build on FreeBSD"
}, {
	"body": "Hi,\n\nWe have a use case of doing sync writes (each write ~ 1 Kb, SSD page size = 4Kb) with the following settings to alleviate stalling,\n\nrocksdb_options_set_write_buffer_size(options,33554432);\nenv = rocksdb_create_default_env();\nrocksdb_env_set_background_threads(env,2);\nrocksdb_env_set_high_priority_background_threads(env,1);\nrocksdb_options_set_max_background_compactions(options, 2);\nrocksdb_options_set_max_background_flushes(options,1);\nrocksdb_options_set_env(options,env);\n\nEverything else is default (default read cache settings). After ~ 45 million rows were loaded, we got a crash on one of the compaction threads (the build was made from the current repository last week):\n\n(gdb) bt\n#0  0x00007f7f574a29e4 in UnalignedCopy64 (decompressor=0x7f7f45d23790, writer=0x7f7f45d237c0, uncompressed_len=<value optimized out>) at snappy-stubs-internal.h:195\n#1  IncrementalCopyFastPath (decompressor=0x7f7f45d23790, writer=0x7f7f45d237c0, uncompressed_len=<value optimized out>) at snappy.cc:147\n#2  AppendFromSelf (decompressor=0x7f7f45d23790, writer=0x7f7f45d237c0, uncompressed_len=<value optimized out>) at snappy.cc:1209\n#3  DecompressAllTagssnappy::SnappyArrayWriter (decompressor=0x7f7f45d23790, writer=0x7f7f45d237c0, uncompressed_len=<value optimized out>) at snappy.cc:779\n#4  snappy::InternalUncompressAllTagssnappy::SnappyArrayWriter (decompressor=0x7f7f45d23790, writer=0x7f7f45d237c0, uncompressed_len=<value optimized out>) at snappy.cc:865\n#5  0x00007f7f574a35e0 in InternalUncompresssnappy::SnappyArrayWriter (compressed=<value optimized out>, uncompressed=<value optimized out>) at snappy.cc:855\n#6  snappy::RawUncompress (compressed=<value optimized out>, uncompressed=<value optimized out>) at snappy.cc:1234\n#7  0x00007f7f574a3632 in snappy::RawUncompress (compressed=<value optimized out>, n=<value optimized out>, uncompressed=<value optimized out>) at snappy.cc:1229\n#8  0x00007f7f5795111d in Snappy_Uncompress (data=0x7f7efa0afea0 \"\\305\\335-\\220\", n=215560, contents=0x7f7f45d24ee0, format_version=2, compression_dict=..., compression_type=<incomplete type>, ioptions=...)\n\n```\nat ./util/compression.h:179\n```\n#9  rocksdb::UncompressBlockContentsForCompressionType (data=0x7f7efa0afea0 \"\\305\\335-\\220\", n=215560, contents=0x7f7f45d24ee0, format_version=2, compression_dict=..., compression_type=<incomplete type>,\n\n```\nioptions=...) at table/format.cc:434\n```\n#10 0x00007f7f57951631 in rocksdb::UncompressBlockContents (data=Unhandled dwarf expression opcode 0xf3\n\n) at table/format.cc:540\n#11 0x00007f7f5795206c in rocksdb::ReadBlockContents (file=Unhandled dwarf expression opcode 0xf3\n\n) at table/format.cc:388\n#12 0x00007f7f5793a628 in rocksdb::(anonymous namespace)::ReadBlockFromFile (file=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_reader.cc:77\n#13 0x00007f7f5793d38d in Create (this=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_reader.cc:196\n#14 rocksdb::BlockBasedTable::CreateIndexReader (this=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_reader.cc:1743\n#15 0x00007f7f579439d5 in rocksdb::BlockBasedTable::Open (ioptions=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_reader.cc:788\n#16 0x00007f7f57938c5b in rocksdb::BlockBasedTableFactory::NewTableReader (this=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_factory.cc:59\n#17 0x00007f7f578f31c5 in rocksdb::TableCache::GetTableReader (this=Unhandled dwarf expression opcode 0xf3\n\n) at db/table_cache.cc:111\n#18 0x00007f7f578f376d in rocksdb::TableCache::FindTable (this=0x1962cb0, env_options=..., internal_comparator=..., fd=..., handle=0x7f7f45d25678, no_io=false, record_read_stats=true, file_read_hist=0x19619e0,\n\n```\nskip_filters=false, level=-1, prefetch_index_and_filter_in_cache=true) at db/table_cache.cc:148\n```\n#19 0x00007f7f578f3d1d in rocksdb::TableCache::NewIterator (this=0x1962cb0, options=..., env_options=..., icomparator=..., fd=..., table_reader_ptr=0x0, file_read_hist=0x19619e0, for_compaction=false, arena=\n\n```\n0x0, skip_filters=false, level=-1, range_del_agg=0x0, is_range_del_only=false) at db/table_cache.cc:218\n```\n#20 0x00007f7f5786943c in rocksdb::CompactionJob::FinishCompactionOutputFile (this=0x7f7f45d26ca0, input_status=Unhandled dwarf expression opcode 0xf3\n\n) at db/compaction_job.cc:1012\n#21 0x00007f7f5786be23 in rocksdb::CompactionJob::ProcessKeyValueCompaction (this=0x7f7f45d26ca0, sub_compact=0x7f7ef89ea9f0) at db/compaction_job.cc:864\n#22 0x00007f7f5786ca2f in rocksdb::CompactionJob::Run (this=0x7f7f45d26ca0) at db/compaction_job.cc:535\n#23 0x00007f7f578943c8 in rocksdb::DBImpl::BackgroundCompaction (this=0x19510e0, made_progress=0x7f7f45d270fe, job_context=0x7f7f45d27120, log_buffer=0x7f7f45d27320, arg=0x0) at db/db_impl.cc:3616\n#24 0x00007f7f578a4d68 in rocksdb::DBImpl::BackgroundCallCompaction (this=0x19510e0, arg=0x0) at db/db_impl.cc:3314\n#25 0x00007f7f579c0771 in rocksdb::ThreadPoolImpl::BGThread (this=0x194ab40, thread_id=0) at util/threadpool_imp.cc:229\n#26 0x00007f7f579c0853 in rocksdb::BGThreadWrapper (arg=0x1950b90) at util/threadpool_imp.cc:253\n#27 0x00000034c0007851 in start_thread () from /lib64/libpthread.so.0\n#28 0x00000034bf8e890d in clone () from /lib64/libc.so.6\n\nThanks,\n\nEthan.\n",
	"number": 1422,
	"title": "Compaction thread crash on sync writes"
}, {
	"body": "Hi all!\n\nMy config is:\nRocksDB version: 4.11.2\nHas DB with few columns (3-5)\nApp have 3 different DB.\nOS: Windows 7\n\nI'm now got facing on issue, when  app is running for about a day and then stop and start, it is going into long DB::Open time. After first restart, the second restarting going definitely faster (don't delay at all).\n\n```\n    snmp_int.exe!rocksdb::DBImpl::RecoverLogFiles(const std::vector<unsigned __int64,std::allocator<unsigned __int64> > & log_numbers, unsigned __int64 * next_sequence, bool read_only) Line 1584  C++\n    snmp_int.exe!rocksdb::DBImpl::Recover(const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, bool read_only, bool error_if_log_file_exist, bool error_if_data_exists_in_logs) Line 1312 C++\n    snmp_int.exe!rocksdb::DB::Open(const rocksdb::DBOptions & db_options, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & dbname, const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, std::vector<rocksdb::ColumnFamilyHandle *,std::allocator<rocksdb::ColumnFamilyHandle *> > * handles, rocksdb::DB * * dbptr) Line 5717 C++\n    snmp_int.exe!rocksdb::DBWithTTL::Open(const rocksdb::DBOptions & db_options, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & dbname, const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, std::vector<rocksdb::ColumnFamilyHandle *,std::allocator<rocksdb::ColumnFamilyHandle *> > * handles, rocksdb::DBWithTTL * * dbptr, std::vector<int,std::allocator<int> > ttls, bool read_only) Line 101    C++\n```\n\nsee attach to full stack (stack get many times) :\n[stack.txt](https://github.com/facebook/rocksdb/files/549607/stack.txt)\nThe option, set to db configure is:\n[OPTIONS-000189.txt](https://github.com/facebook/rocksdb/files/549609/OPTIONS-000189.txt)\nThe log file from one of many db is:\n[LOG.old.txt](https://github.com/facebook/rocksdb/files/549610/LOG.old.txt)\n\nThe last lines in LOG that db after delay a trigger compaction:\n\n```\n2016/10/25-10:47:29.335231 4c78 [1] Compaction start summary: Base version 14 Base level 0, inputs: [173(174KB) 163(1186B) 154(1027B)], [150(955KB)]\n2016/10/25-10:47:29.335231 4c78 EVENT_LOG_v1 {\"time_micros\": 1477367249335231, \"job\": 3, \"event\": \"compaction_started\", \"files_L0\": [173, 163, 154], \"files_L1\": [150], \"score\": 1, \"input_data_size\": 1159024}\n2016/10/25-10:47:30.489801 4c78 [1] [JOB 3] Generated table #180: 39542 keys, 1176579 bytes\n2016/10/25-10:47:30.489801 4c78 EVENT_LOG_v1 {\"time_micros\": 1477367250489801, \"cf_name\": \"1\", \"job\": 3, \"event\": \"table_file_creation\", \"file_number\": 180, \"file_size\": 1176579, \"table_properties\": {\"data_size\": 1156720, \"index_size\": 32944, \"filter_size\": 0, \"raw_key_size\": 709132, \"raw_average_key_size\": 17, \"raw_value_size\": 4042366, \"raw_average_value_size\": 102, \"num_data_blocks\": 1149, \"num_entries\": 39542, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}\n2016/10/25-10:47:30.490808 4c78 [1] [JOB 3] Compacted 3@0 + 1@1 files to L1 => 1176579 bytes\n2016/10/25-10:47:30.494834 4c78 (Original Log Time 2016/10/25-10:47:30.494834) [1] compacted to: base level 1 max bytes base 268435456 files[1 1 0 0 0 0 0] max score 0.25, MB/sec: 1.0 rd, 1.0 wr, level 1, files in(3, 1) out(1) MB in(0.2, 0.9) out(1.1), read-write-amplify(12.9) write-amplify(6.5) OK, records in: 6567, records dropped: 0\n2016/10/25-10:47:30.494834 4c78 (Original Log Time 2016/10/25-10:47:30.494834) EVENT_LOG_v1 {\"time_micros\": 1477367250494834, \"job\": 3, \"event\": \"compaction_finished\", \"compaction_time_micros\": 1154570, \"output_level\": 1, \"num_output_files\": 1, \"total_output_size\": 1176579, \"num_input_records\": 39542, \"num_output_records\": 39542, \"num_subcompactions\": 1, \"lsm_state\": [1, 1, 0, 0, 0, 0, 0]}\n2016/10/25-10:47:30.496847 4c78 [DEBUG] [JOB 3] Delete var/db/r_data/1/000173.sst type=2 #173 -- OK\n\n```\n\nMy question is how to improve db open performance, and maybe I'm need to reduce some options (see attach) to prevent long db open?\n",
	"number": 1421,
	"title": "long time to DBWithTTL::Open after app restart"
}, {
	"body": "In one deployment we saw high latencies (presumably from slow iterator operations) and a lot of CPU time reported by perf with this stack:\n\n```\n  rocksdb::MergingIterator::Next\n  rocksdb::DBIter::FindNextUserEntryInternal\n  rocksdb::DBIter::Seek\n```\n\nI think what's happening is:\n1. we create a snapshot iterator,\n2. we do lots of Put()s for the same key x; this creates lots of entries in memtable,\n3. we seek the iterator to a key slightly smaller than x,\n4. the seek walks over lots of entries in memtable for key x, skipping them because of high sequence numbers.\n\nCC @IslamAbdelRahman\n",
	"number": 1413,
	"title": "Less linear search in DBIter::Seek() when keys are overwritten a lot"
}, {
	"body": "Hi team. We are using rocksdb 4.1.0 and we hit a segfault inside VersionSet::LogAndApply when we CreateColumnFamily, here is the function call stack and logs:\n\n**call stack:**\n#0  0x00007ff4fc0e4444 in rocksdb::VersionSet::LogAndApply(rocksdb::ColumnFamilyData*, rocksdb::MutableCFOptions const&, rocksdb::VersionEdit*, rocksdb::InstrumentedMutex*, rock\n\nsdb::Directory*, bool, rocksdb::ColumnFamilyOptions const*) () from /usr/lib/librocksdb.so.4.1\n#1  0x00007ff4fc07a81e in rocksdb::DBImpl::CreateColumnFamily(rocksdb::ColumnFamilyOptions const&, std::string const&, rocksdb::ColumnFamilyHandle**) ()\n   from /usr/lib/librocksdb.so.4.1\n\n**logs:**\n\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) Created column family [ConsistentSnapshot] (ID 2)\nOct 20 04:31:29 localhost lrse[2836]: [kvs.ERR] (2836) MANIFEST write: IO error: /rfs/lrse/meta/info/meta/MANIFEST-000004: Device or resource busy\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) ManifestContains: checking /rfs/lrse/meta/info/meta/MANIFEST-000004\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) ManifestContains: IO error: /rfs/lrse/meta/info/meta/MANIFEST-000004: Software caused connection abort\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) ManifestContains: is unable to reopen the manifest file  /rfs/lrse/meta/info/meta/MANIFEST-000004\nOct 20 04:31:29 localhost pm[1695]: [pm.ERR]: Output from lrse: /opt/rbt/bin/lrse (pid 2836) received signal 11 (SIGSEGV) dumping core\n\nWe would like to exit gracefully in this case. Thanks!\n",
	"number": 1412,
	"title": "receive SIGSEGV inside VersionSet::LogAndApply call"
}, {
	"body": "This sentence from [SingleDelete](https://github.com/facebook/rocksdb/wiki/Single-Delete) documentation is not clear:\n\n> In contrast to the conventional Delete() operation, the deletion entry is removed along with the value when the two are lined up in a compaction. \n\nFor me, as a newcomer to RocksDB, it doesn't describe the difference from `Delete`, because it matches my initial expectation of what a `Delete` would do.\n\nOnly after reading the highlighted sentence ([from here](https://github.com/facebook/rocksdb/wiki/Delete-A-Range-Of-Keys)) I could understand the difference:\n\n> if you never overwrite existing keys, you can try to use DB::SingleDelete() instead of Delete() to kill tombstones sooner. _Tombstones will be dropped after it meets the original keys, rather than compacted to the last level._\n\nPlease, make the difference clear in `SingleDelete` documentation by specifying the behaviour of `Delete` too, so that new users can get correct understanding.\n",
	"number": 1406,
	"title": "Wiki: difference between SingleDelete and Delete is not clear"
}, {
	"body": "We have an app that uses RocksDB (rocksdbjni-3.13.1.jar). We observe a Compaction IO error. The error seems quite general (I ensured that there was free space in the host at the time of the error.) and I'd really appreciate some insight on what causes this.\n\n```\n2016/10/14-13:13:35.160260 7f25c931b700 [WARN] Compaction error: IO error: /export/content/data/../06.sst: Input/output error\n2016/10/14-13:13:35.160268 7f25c931b700 (Original Log Time 2016/10/14-13:13:35.159784) [default] compacted to: files[1 0 17 20 26 33 53] max score 1.50, MB/sec: 0.5 rd, 0.5 wr, level 2, files in(1, 17) out(16) MB in(0.0, 33.5) out(32.7), read-write-amplify(17269.6) write-amplify(8535.0) IO error: /export/../274206.sst: Input/output error, records in: 583474, records dropped: 0\n2016/10/14-13:13:35.160272 7f25c931b700 (Original Log Time 2016/10/14-13:13:35.160238) EVENT_LOG_v1 {\"time_micros\": 1476450815160021, \"job\": 5696, \"event\": \"compaction_finished\", \"output_level\": 2, \"num_output_files\": 16, \"total_output_size\": 34319108, \"num_input_records\": 570442, \"num_output_records\": 570442, \"lsm_state\": [1, 0, 17, 20, 26, 33, 53]}\n2016/10/14-13:13:35.160277 7f25c931b700 [ERROR] Waiting after background compaction error: IO error: /export/../274206.sst: Input/output error, Accumulated background error counts: 1\n2016/10/14-13:13:36.176650 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816176637, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274206}\n2016/10/14-13:13:36.177563 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816177557, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274205}\n2016/10/14-13:13:36.178574 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816178567, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274204}\n2016/10/14-13:13:36.179745 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816179738, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274203}\n2016/10/14-13:13:36.180790 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816180785, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274202}\n2016/10/14-13:13:36.181765 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816181759, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274201}\n2016/10/14-13:13:36.182739 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816182734, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274200}\n2016/10/14-13:13:36.184003 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816183996, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274199}\n2016/10/14-13:13:36.185124 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816185117, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274198}\n2016/10/14-13:13:36.186225 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816186217, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274197}\n2016/10/14-13:13:36.187240 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816187235, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274196}\n2016/10/14-13:13:36.188205 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816188200, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274195}\n2016/10/14-13:13:36.189640 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816189627, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274194}\n2016/10/14-13:13:36.191339 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816191330, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274193}\n2016/10/14-13:13:36.192962 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816192954, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274192}\n2016/10/14-13:13:36.194374 7f25c931b700 EVENT_LOG_v1 {\"time_micros\": 1476450816194366, \"job\": 5696, \"event\": \"table_file_deletion\", \"file_number\": 274191}\n2016/10/14-13:13:36.818933 7f25c931b700 [ERROR] Waiting after background compaction error: IO error: /export/...Partition_165/274206.sst: Input/output error, Accumulated background error counts: 2\n\n```\n",
	"number": 1403,
	"title": "Compaction error : IO error"
}, {
	"body": "The [wiki page](https://github.com/facebook/rocksdb/wiki/Basic-Operations) mistakenly states that default target_file_size_base is 2MB.\n\nIt seems this should be 64MB instead (according to include/rocksdb/options.h).\n",
	"number": 1400,
	"title": "Wiki: default value of target_file_size_base"
}, {
	"body": "OS: Mac OS X\n\nI use rocksdb for binding PHP, so I write php extension for rocksdb.\n\nBut I run unit test, it will have the follow informations:\n\n```\n\u279c  unit git:(master) \u2717 phpunit RocksDBTest.php\nPHPUnit 5.4.6 by Sebastian Bergmann and contributors.\n\ndyld: lazy symbol binding failed: Symbol not found: _rocksdb_open\n  Referenced from: /usr/local/Cellar/php56/5.6.25_1/lib/php/extensions/no-debug-non-zts-20131226/rocksdb.so\n  Expected in: flat namespace\n\ndyld: Symbol not found: _rocksdb_open\n  Referenced from: /usr/local/Cellar/php56/5.6.25_1/lib/php/extensions/no-debug-non-zts-20131226/rocksdb.so\n  Expected in: flat namespace\n\n/usr/local/bin/phpunit: line 2: 15319 Trace/BPT trap: 5       /usr/bin/env php -d allow_url_fopen=On -d detect_unicode=Off /usr/local/Cellar/phpunit/5.4.6/libexec/phpunit-5.4.6.phar \"$@\"\n```\n",
	"number": 1397,
	"title": "Expected in: flat namespace"
}, {
	"body": "Hey fine folks,\n\nThanks for RockDB. I just checked out the master and ran db_bench with cuckoo table format. It soon crashed.\n\n```\ngit hash 21e8daced5a2db9ba55138c60a320676306f2088\nOS: MacOS 10.11.6\nXCode 8\n```\n\n```\n$ ~/repos/rocksdb % ./db_bench -db tmp-rocks-db -benchmarks=fillsync -use_cuckoo_table\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nRocksDB:    version 4.12\nKeys:       16 bytes each\nValues:     100 bytes each (50 bytes after compression)\nEntries:    1000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    110.6 MB (estimated)\nFileSize:   62.9 MB (estimated)\nWrite rate: 0 bytes/second\nCompression: Snappy\nMemtablerep: skip_list\nPerf Level: 1\nWARNING: Assertions are enabled; benchmarks unnecessarily slow\n------------------------------------------------\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nDB path: [tmp-rocks-db]\nput error: IO error: tmp-rocks-db/000006.sst: Bad address\nlibc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: mutex lock failed: Invalid argument\nReceived signal 6 (Abort trap: 6)\n#0   Invalid connection: com.apple.coresymbolicationd   0x00102440 (in db_bench)\n#1   Invalid connection: com.apple.coresymbolicationd   abort (in libsystem_c.dylib) + 129\n#2   Invalid connection: com.apple.coresymbolicationd   __cxa_bad_cast (in libc++abi.dylib) + 0\n#3   Invalid connection: com.apple.coresymbolicationd   default_terminate_handler() (in libc++abi.dylib) + 243\n#4   Invalid connection: com.apple.coresymbolicationd   _objc_terminate() (in libobjc.A.dylib) + 124\n#5   Invalid connection: com.apple.coresymbolicationd   std::__terminate(void (*)()) (in libc++abi.dylib) + 8\n#6   Invalid connection: com.apple.coresymbolicationd   __cxxabiv1::exception_cleanup_func(_Unwind_Reason_Code, _Unwind_Exception*) (in libc++abi.dylib) + 0\n#7   Invalid connection: com.apple.coresymbolicationd   std::__1::__throw_system_error(int, char const*) (in libc++.1.dylib) + 77\n#8   Invalid connection: com.apple.coresymbolicationd   rocksdb::SyncPoint::Process(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, void*) (in db_bench) (sync_point.cc:123)\n#9   Invalid connection: com.apple.coresymbolicationd   rocksdb::PosixLogger::Flush() (in db_bench) (posix_logger.h:58)\n#10  Invalid connection: com.apple.coresymbolicationd   rocksdb::DBImpl::PurgeObsoleteFiles(rocksdb::JobContext const&, bool) (in db_bench) (vector:449)\n#11  Invalid connection: com.apple.coresymbolicationd   rocksdb::DBImpl::BackgroundCallFlush() (in db_bench) (db_impl.cc:3195)\n#12  Invalid connection: com.apple.coresymbolicationd   rocksdb::DBImpl::BGWorkFlush(void*) (in db_bench) (db_impl.cc:3025)\n#13  Invalid connection: com.apple.coresymbolicationd   rocksdb::ThreadPoolImpl::BGThread(unsigned long) (in db_bench) (threadpool_imp.cc:92)\n#14  Invalid connection: com.apple.coresymbolicationd   rocksdb::BGThreadWrapper(void*) (in db_bench) (threadpool_imp.cc:257)\n#15  Invalid connection: com.apple.coresymbolicationd   _pthread_body (in libsystem_pthread.dylib) + 131\n#16  Invalid connection: com.apple.coresymbolicationd   _pthread_body (in libsystem_pthread.dylib) + 0\n#17  Invalid connection: com.apple.coresymbolicationd   thread_start (in libsystem_pthread.dylib) + 13\n[1]    17576 abort      ./db_bench -db tmp-rocks-db -benchmarks=fillsync -use_cuckoo_table\n```\n",
	"number": 1395,
	"title": "db_bench crash with cuckoo table"
}, {
	"body": "We are using RocksDB 4.2 as a FIFO queue (or a time window). Each key is associated with the instant the key is produced. Then the queue will be scanned to find out all the keys produced at prior to a given instant.\n\nIn most cases, the sst files do not overlap with each other, and they will be put in Level-1 with the trivial move. As these files does not overlap with the files produced later, they will not be selected in the background compaction performed later and cannot be deleted.  Finally, the performance of scanning degrades significantly.\n\nI don't know whether the problem is solved in the new version. Currently, the only method to work around this problem is to perform manual compactions where the trivial move is disallowed. Is there any method to disallow trivial move in background compaction?\n",
	"number": 1394,
	"title": "Dead files due to trivial move"
}, {
	"body": "OptionChangeMigration() to avoid full compaction if the old and new options indicate the same LSM setting.\n",
	"number": 1391,
	"title": "OptionChangeMigration() to avoid full compaction for the same setting"
}, {
	"body": "hi,\nI use rocksdb as my KV store. My options are:\n        cache_size: 20000\n        # in KB\n        block_size: 16\n        # in MB\n        write_buffer_size: 64\n        # yes|no\n        compression: yes\n        bloom_filter_num: 10\n        max_write_buffer_number: 10\n        compressiontype: zlib\n        prefix_hash_len: 40\n        ttl_batch_size: 1000\n        ttl_sleep_sec: 3600\n        wal_bytes_per_sync: 1\n        bytes_per_sync: 1\n        rate_limiter: 10\n        sst_del_speed: 5\n\ncache_index_and_filter_blocks = false\nindex_type = rocksdb::BlockBasedTableOptions::kHashSearch\n\nI close automatic compaction due to the high write QPS. So I run compacting manually. The situation is that the VIRT is increasing when compacting and never released. How can I get out of it?\n\nPS:If I change the compression function, will I read from old compression format database correctly?\n",
	"number": 1381,
	"title": "VIRT very high when using manual compaction"
}, {
	"body": "platform: `Mac OS X 10.11`\nbranch: `master`\nstep: `make static_lib`\nthen it will have the follow info:\n\n```\n  CC       utilities/write_batch_with_index/write_batch_with_index_internal.o\n  CC       tools/ldb_cmd.o\n  CC       tools/ldb_tool.o\n  CC       tools/sst_dump_tool.o\n  AR       librocksdb.a\nar: creating archive librocksdb.a\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_impl_debug.o) has no symbols\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(thread_status_util_debug.o) has no symbols\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(xfunc.o) has no symbols\n```\n",
	"number": 1380,
	"title": "Have no symbols when compile on Mac OS"
}, {
	"body": "Level0 -1: use kNoCompression , level2-6: use kSnappyCompression , these options should improve the flush speed and decrease disk space amplification. Will it decrease the query speed?\n",
	"number": 1376,
	"title": "Will different compression strategy affect query performance?"
}, {
	"body": "Hi,\n\nI wonder if it would be possible to add SPDK(https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk) support when running RocksDB instances inside VMs. As far as I understand this can improve Disk Access performance access a lot, when running from inside VMs.\n\nBased on this comment[https://news.ycombinator.com/item?id=12023092](not entirely scientific) seems like there could be some big benefits for using SPDK.\n\nDoes RocksDB depend on the filesystem? How hard would it be to support this?\n\nOverall it looks like this could be a low hanging performance fruit. I'm completely new to RocksDB and don't know the internal structure, I may be talking crazy.\n\nThe referenced comment:\n\n> Submitting and completing a 4k I/O using SPDK is about 7 times more CPU efficient than the equivalent operation with libaio, which is opening a raw block device with O_DIRECT.\n> Said another way, on a recent Xeon CPU you can expect to drive somewhere around 3 million 4k I/O per second with SPDK per core. With libaio, you can do about 450,000 per core off the top of my head.\n> SPDK has no locks or cross core communication required, so it scales linearly with additional CPU cores. Blk-mq in the kernel also helped the kernel scaling problem significantly, but I'm not sure if it is perfectly linear yet.\n> Most applications need something like a filesystem to function - there is no denying that. Using SPDK requires applications to implement at least the minimal set of features in the filesystem that their application needs. Many databases and storage services already bypass the filesystem or use the filesystem as a block allocator only, so it is not a big leap from there to SPDK.\n",
	"number": 1374,
	"title": "Considering SPDK support for bypassing the CPU for disk access"
}, {
	"body": "RocksDB 4.9 introduced a bug which caused CockroachDB state invariant assertions to fire. Bisecting the changes from v4.8 to v4.9 led to https://github.com/facebook/rocksdb/commit/c27061dae763621bdcfb5cc0d0118f57269825b8 and, in particular, this diff:\n\n```\ndiff --git a/db/db_impl.cc b/db/db_impl.cc\nindex e8cb954..ce2b61e 100644\n--- a/db/db_impl.cc\n+++ b/db/db_impl.cc\n@@ -1195,8 +1195,6 @@ Status DBImpl::Recover(\n     // Note that prev_log_number() is no longer used, but we pay\n     // attention to it in case we are recovering a database\n     // produced by an older version of rocksdb.\n-    const uint64_t min_log = versions_->MinLogNumber();\n-    const uint64_t prev_log = versions_->prev_log_number();\n     std::vector<std::string> filenames;\n     s = env_->GetChildren(db_options_.wal_dir, &filenames);\n     if (!s.ok()) {\n@@ -1213,7 +1211,7 @@ Status DBImpl::Recover(\n               \"While creating a new Db, wal_dir contains \"\n               \"existing log file: \",\n               filenames[i]);\n-        } else if ((number >= min_log) || (number == prev_log)) {\n+        } else {\n           logs.push_back(number);\n         }\n       }\n```\n\nFrom what I understand of this code, the above change expanded the set of log files considered during recovery. It appears that RocksDB 4.11.2 has fixed the issue, notably in https://github.com/facebook/rocksdb/commit/2a6d0cde722a92ac6e5229c97ebbd3009921c9d9 which changed recovery to ignore stale log files.\n\nI'm primarily looking for confirmation that https://github.com/facebook/rocksdb/commit/c27061dae763621bdcfb5cc0d0118f57269825b8  did introduce the bug fixed by https://github.com/facebook/rocksdb/commit/2a6d0cde722a92ac6e5229c97ebbd3009921c9d9 so that we can feel comfortable in upgrading to v4.11.2. I haven't yet narrowed down the precise sequence of events that caused our state invariant assertions to fire, but can do so if it would be helpful.\n",
	"number": 1370,
	"title": "data corruption issue with RocksDB 4.9, maybe fixed by 4.11.2"
}, {
	"body": "Write large data into DB with auto compaction , level0_slowdown_writes_trigger=1<<30, level0_stop_writes_trigger<<30, the error occurred in the LOG. \nI can not check out the reason, Could you tell the possible reason?\n",
	"number": 1365,
	"title": "Tried to delete a non-existing file /hdfsdata/2/dinnne/VTVTGrapth//001226.sst, type=2 --IO error: no such file or diectory."
}, {
	"body": "The libraries produced on linux are now named\nlibrocksdb.a\nlibrocksdb.so\n\nAlso link with -lrt to avoid linker errors.\n\nGeneralize comments at the top to include Linux\n",
	"number": 1364,
	"title": "[cmake] Fixup a couple of builds errors on Linux."
}, {
	"body": "I notice that the performence bench haven't be edited since Dec 2014. It should been changed after 2 years development. Can you provide latest official performence bench result?  \n",
	"number": 1361,
	"title": "about performence data"
}, {
	"body": "random write 100000000  kv into DB , the length of key is 8bytes, value is 80 bytes. Traver the DB ,take one  key per  5000,  and push it into a vector, so the size of vector is 2000\uff0cclear the system cache,  then read the values from the DB of the all keys in vector to test the performance. \n\nfirst way:\n        Bulk load with disable_auto_compaction=false, all data are stored to level-1 and level-2, the time is 38s.\nsecond way:\n      Bulk load with disable_auto_compaction=true, and then call CompactRange(). After compaction ,all data are stored to level-1, the time is 100ms.\n\nI find the performance between the two ways have a big difference , the second way has a much better performance the the first one, is it right?\n",
	"number": 1359,
	"title": "The performance have a big difference when data are writted into DB in two different ways."
}, {
	"body": "As the tittle\n",
	"number": 1357,
	"title": "[Win32 Port] Why not use memory mapping for RandomAccessFile?"
}, {
	"body": "We want a Put/Write returns fail  on writing rate limit is triggered, instead of sleep and block the calling thread. This may be implemented by adding an option in `WriteOption`.\n\nThanks!\n",
	"number": 1356,
	"title": "Feature Request: Allowing Put/Write do not wait but fail on write rate limit is triggered"
}, {
	"body": "Fristly write all data(8.7G) into level-0 with options max_bytes_for_level_base=1*SizeUnit.GB, than call CompactRange() to compact , after compaction, I find that all data are compacted into level-1 and the size is 8.4G far greater than the option  max_bytes_for_level_base. Why is this?\n",
	"number": 1355,
	"title": "A problem of manual compact\u3002"
}, {
	"body": "Firstly flush all data(8.7G) into level-0, then call CompactRange() to compact data into level-1, I found that the data will increase to more than 16G gradually and the SST file will be more and more during compaction\u3002What can I do to control the expand factor of data and to delete source SST file immediately after compaction?\n",
	"number": 1354,
	"title": "How to avoid the expand of data during compaction?"
}, {
	"body": "I have a use case where RocksDB performs well once the cache is warmed up, but until then the latency suffers. I am planning on adding a hack for this, which is to have cache just the keys at application layer, persist it before shutdown, and on startup read all the keys again to force RocksDB cache to be warmed up. It would be nice to have RocksDB do this instead, so that we can get reliable, stable performance even immediately after startup.\n",
	"number": 1352,
	"title": "Cache persistence and warmup"
}, {
	"body": "@siying @yiwu-arbug \n\nI tried to randomly kill server in every 5min, and after two days, the above error happened, it seems that WAL is corrupted. \n\nWe use 4.12 version, have 4 column families, and use WAL recover mode 1. \n",
	"number": 1351,
	"title": "WAL recover failed: dropping 42 bytes; Corruption: truncated header"
}, {
	"body": "",
	"number": 1347,
	"title": "Can I change the ColumnFamily (e.g. CreateColumnFamily or DropColumnFamily or DestroyColumnFamilyHandle) without reopening the DB?"
}, {
	"body": "Hi, \n    I have a problem that when I open two db(A and B) in one progress, then called addfile interface to import sst file to`db A`, there was much(compare with don't called addfile interface) timeout when I called multi-get on `db B`. When timeout happened, the load of machine is low and there is no share block-cache.\n",
	"number": 1345,
	"title": "About multi-get timeout when add sst to another db."
}, {
	"body": "We implemented a custom sstable, and our sstable need scan input two passes to achieve better compression, but through TableBuilder we can not scan input two passes, so we store the input data to a temporary file for second pass scan, this is very inefficient.\n\nWould you provide a way to allowing two pass scan, by any way, such as provide a rewind-able iterator, or provide an option to push all data by TableBuilder::Add(key, value) two passes.\n",
	"number": 1334,
	"title": "Feature request: need two-pass scan for custom sst TableBuilder"
}, {
	"body": "When use java api get() function to make random access to DB, the time spent firstly is much longer than follow? \n",
	"number": 1332,
	"title": "A problem about the first get() call?"
}, {
	"body": "that i try to write a MemTableRep implement used fixed memory block ...\nso how do i notify rocksdb this MemTableRep is full ?\n",
	"number": 1331,
	"title": "problem about MemTableRep"
}, {
	"body": "Hi,\nI found that plain table format can not support iterator->prev() and seekToLast(), but block based and cuckoo can support both of them\u3002And I use db_bench to validate them, and there is some wrong with cuckoo's prev and seekToLast as follows:\n\n\"db/db_iter.cc:605: bool rocksdb::DBIter::FindValueForCurrentKey(): Assertion `iter_->IsValuePinned()' failed.\"\n\nI think  it is the problem about IsValuePinned() of different table formats. Do you have any ideas about them? Thank you!\n\nFengfeng Pan\n",
	"number": 1330,
	"title": "There exists an error about cuckoo table format's iterator->prev() and seekToLast() operations"
}, {
	"body": "I installed Visual C++ runtime for Visual Studio 2015 from https://www.microsoft.com/en-us/download/details.aspx?id=48145 but still getting **Can't find dependent libraries**\n\nDependency Walker show me these missing dependencies:\nAPI-MS-WIN-CRT-CONVERT-L1-1-0.DLL\nAPI-MS-WIN-CRT-ENVIRONMENT-L1-1-0.DLL\nAPI-MS-WIN-CRT-FILESYSTEM-L1-1-0.DLL\nAPI-MS-WIN-CRT-HEAP-L1-1-0.DLL\nAPI-MS-WIN-CRT-LOCALE-L1-1-0.DLL\nAPI-MS-WIN-CRT-MATH-L1-1-0.DLL\nAPI-MS-WIN-CRT-MULTIBYTE-L1-1-0.DLL\nAPI-MS-WIN-CRT-RUNTIME-L1-1-0.DLL\nAPI-MS-WIN-CRT-STDIO-L1-1-0.DLL\nAPI-MS-WIN-CRT-STRING-L1-1-0.DLL\nAPI-MS-WIN-CRT-TIME-L1-1-0.DLL\nAPI-MS-WIN-CRT-UTILITY-L1-1-0.DLL\nAPI-MS-WIN-SERVICE-PRIVATE-L1-1-1.DLL\nAPI-MS-WIN-CORE-KERNEL32-PRIVATE-L1-1-1.DLL\nAPI-MS-WIN-CORE-PRIVATEPROFILE-L1-1-1.DLL\n\nRocksDB is running well on Win7\n",
	"number": 1329,
	"title": "RocksDB on Windows Server 2012 R2 -  Can't find dependent libraries"
}, {
	"body": "Program terminated with signal SIGSEGV, Segmentation fault.\nin tcmalloc::SLL_Next(void_) () from /usr/lib/libtcmalloc.so.4\n in tcmalloc::SLL_PopRange(void__, int, void__, void__) () from /usr/lib/libtcmalloc.so.4\n in tcmalloc::ThreadCache::FreeList::PopRange(int, void__, void__) () from /usr/lib/libtcmalloc.so.4\n in tcmalloc::ThreadCache::ReleaseToCentralCache(tcmalloc::ThreadCache::FreeList_, unsigned long, int) () from /usr/lib/libtcmalloc.so.4\n in tcmalloc::ThreadCache::ListTooLong(tcmalloc::ThreadCache::FreeList_, unsigned long) () from /usr/lib/libtcmalloc.so.4\n in tcmalloc::ThreadCache::Deallocate(void_, unsigned long) () from /usr/lib/libtcmalloc.so.4\n in ?? () from /usr/lib/libtcmalloc.so.4\n in ?? () from /usr/lib/libtcmalloc.so.4\n in ?? () from /usr/lib/libtcmalloc.so.4\n in tc_delete () from /usr/lib/libtcmalloc.so.4\n in rocksdb::VersionSet::Recover(std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, bool) () from /usr/lib/librocksdb.so.4.1\n in rocksdb::DBImpl::Recover(std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, bool, bool) () from /usr/lib/librocksdb.so.4.1\n in rocksdb::DB::Open(rocksdb::DBOptions const&, std::string const&, std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> >_, rocksdb::DB_*) () from /usr/lib/librocksdb.so.4.1\n in KeyValueStore::StoreImpl::StoreImpl (this=0x31a1f00, store_root=\"path\", options=...)\n    at store.cc:100\n in std::make_unique<KeyValueStore::StoreImpl, std::string const&, rocksdb::Options const&> () at make_unique.h:12\n in KeyValueStore::ModuleImpl::Store (this=0x317e210, kvs_root=\"path\", options=...) at module.cc:46\n\nThe rocksdb version is 4.1\n",
	"number": 1322,
	"title": "tcmalloc coredump while opening the db."
}, {
	"body": "The break trace as below:\n\n```\n(gdb) bt\n#0  0x00000000004937db in rocksdb::DBImpl::WriteImpl(rocksdb::WriteOptions const&, rocksdb::WriteBatch*, rocksdb::WriteCallback*) () at /usr/local/gcc/include/c++/5.1.0/bits/atomic_base.h:396\n#1  0x00000000004952b4 in rocksdb::DBImpl::Write(rocksdb::WriteOptions const&, rocksdb::WriteBatch*) () at db/db_impl.cc:4260\n#2  0x000000000043983b in writeDataEntryForFamily(rocksdb::DB*, HmsetEntry const&, unsigned long long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&) ()\n```\n\nthe code of atomic_base.h:396 as blow:\n\n```\n      _GLIBCXX_ALWAYS_INLINE __int_type\n      load(memory_order __m = memory_order_seq_cst) const noexcept\n      {\n       memory_order __b = __m & __memory_order_mask;\n  __glibcxx_assert(__b != memory_order_release);\n  __glibcxx_assert(__b != memory_order_acq_rel);\n\n  return __atomic_load_n(&_M_i, __m);\n      }\n```\n\nWho can help me? Thank you.\n",
	"number": 1319,
	"title": "Segmentation fault in WriteImpl with rocksDB-4.8"
}, {
	"body": "When running on a ppc64 architecture, the JNI wrapper should not try to use the pre-built librocksdbjni-linux64.so, which only runs on x64 architectures. Consequently, the shared library naming convention should consider the whole architecture name (e.g. X64/PPC64 instead of only 64).\n",
	"number": 1317,
	"title": "RocksDB JNI reads x64 build on ppc64 architecture"
}, {
	"body": "",
	"number": 1312,
	"title": "There is a problem of function UnSchedule() in env_hdfs.h. No-return value would lead to compilation failed."
}, {
	"body": "I remeber it is mentioned in the wiki  that the size of index block would too large  if the corresponding SST file is too big. Is there a good way can be used  to solve the problem?\n",
	"number": 1307,
	"title": "If a SST file is too big, the size of the index block  would be larger than memory."
}, {
	"body": "At the moment the RocksJava releases for Linux are built on CentOS 5.6 which provides `glibc 2.5` and `libstdc++ 4.1`.\n\nObviously that version of glibc and libstdc++ is quite outdated now; However I am trying to understand forwards and backwards compatibility between glibc versions.\n\nUsing a `librocksdbjni-linux64.so` file built on CentOS 5.6 (`glibc 2.5`) on CentOS 7.2 (`glibc 2.17`) seems to work fine.\n\nHowever a version of `librocksdbjni-linux64.so` built on TinyCore Linux 7.2 (`glibc 2.22`) will not work on CentOS 7.2 and instead results in the error:\n\n```\nException in thread \"main\" java.lang.UnsatisfiedLinkError: /tmp/librocksdbjni8724249777028282150.so: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /tmp/librocksdbjni8724249777028282150.so)\n    at java.lang.ClassLoader$NativeLibrary.load(Native Method)\n    at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1968)\n    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1893)\n    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1854)\n    at java.lang.Runtime.load0(Runtime.java:795)\n    at java.lang.System.load(System.java:1062)\n    at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78)\n```\n\nRunning `ldd /tmp/librocksdbjni8724249777028282150.so` shows:\n\n```\n./librocksdbjni-linux64.so: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by ./librocksdbjni-linux64.so)\n./librocksdbjni-linux64.so: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by ./librocksdbjni-linux64.so)\n./librocksdbjni-linux64.so: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by ./librocksdbjni-linux64.so)\n    linux-vdso.so.1 =>  (0x00007ffeb2cd0000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00007ff06229e000)\n    librt.so.1 => /lib64/librt.so.1 (0x00007ff062096000)\n    libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007ff061d8d000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007ff061a8b000)\n    libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007ff061875000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007ff0614b2000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007ff062b90000)\n```\n\nCan someone with a better understanding of glibc linking issues shed some light on this? I am wondering ultimately if we need different builds for different Linux distributions depending on what glibc versions they provide? \n",
	"number": 1305,
	"title": "Which glibc should we support in binary releases?"
}, {
	"body": "Hi,\nThis is Somnath and I am a Ceph developer working on integrating rocksdb as a metadata store for Ceph's upcoming new backend. In the process we found this bug within rocksdb. If I enable log recycling with the rocksdb (recycle_log_file_num=16) after ~5-6 hours of run (1M seq but can happen to any workload I guess))  I am getting an error during db->WriteBatch(). Digging down further I got the following anomaly in the rocksdb log.\n\nIt seems the bug is around this portion.\n\n2016-08-25 00:44:03.348710 7f7c117ff700  4 rocksdb: adding log 254 to recycle list\n\n2016-08-25 00:44:03.348722 7f7c117ff700  4 rocksdb: adding log 256 to recycle list\n\n2016-08-25 00:44:03.348725 7f7c117ff700  4 rocksdb: (Original Log Time 2016/08/25-00:44:03.347467) [default] Level-0 commit table #258 started\n2016-08-25 00:44:03.348727 7f7c117ff700  4 rocksdb: (Original Log Time 2016/08/25-00:44:03.348225) [default] Level-0 commit table #258: memtable #1 done\n2016-08-25 00:44:03.348729 7f7c117ff700  4 rocksdb: (Original Log Time 2016/08/25-00:44:03.348227) [default] Level-0 commit table #258: memtable #2 done\n2016-08-25 00:44:03.348730 7f7c117ff700  4 rocksdb: (Original Log Time 2016/08/25-00:44:03.348239) EVENT_LOG_v1 {\"time_micros\": 1472111043348233, \"job\": 88, \"event\": \"flush_finished\", \"lsm_state\": [3, 4, 0, 0, 0, 0, 0], \"immutable_memtables\": 0}\n2016-08-25 00:44:03.348735 7f7c117ff700  4 rocksdb: (Original Log Time 2016/08/25-00:44:03.348297) [default] Level summary: base level 1 max bytes base 5368709120 files[3 4 0 0 0 0 0] max score 0.75\n\n2016-08-25 00:44:03.348751 7f7c117ff700  4 rocksdb: [JOB 88] Try to delete WAL files size 131512372, prev total WAL file size 131834601, number of live WAL files 3.\n\n2016-08-25 00:44:03.348761 7f7c117ff700 10 bluefs unlink db.wal/000256.log\n2016-08-25 00:44:03.348766 7f7c117ff700 20 bluefs _drop_link had refs 1 on file(ino 19 size 0x3f3ddd9 mtime 2016-08-25 00:41:26.298423 bdev 0 extents [0:0xc500000+200000,0:0xcb00000+800000,0:0xd700000+700000,0:0xe200000+800000,0:0xee00000+800000,0:0xfa00000+800000,0:0x10600000+700000,0:0x11100000+700000,0:0x11c00000+800000,0:0x12800000+100000])\n2016-08-25 00:44:03.348775 7f7c117ff700 20 bluefs _drop_link destroying file(ino 19 size 0x3f3ddd9 mtime 2016-08-25 00:41:26.298423 bdev 0 extents [0:0xc500000+200000,0:0xcb00000+800000,0:0xd700000+700000,0:0xe200000+800000,0:0xee00000+800000,0:0xfa00000+800000,0:0x10600000+700000,0:0x11100000+700000,0:0x11c00000+800000,0:0x12800000+100000])\n2016-08-25 00:44:03.348794 7f7c117ff700 10 bluefs unlink db.wal/000254.log\n2016-08-25 00:44:03.348796 7f7c117ff700 20 bluefs _drop_link had refs 1 on file(ino 18 size 0x3f3d402 mtime 2016-08-25 00:41:26.299110 bdev 0 extents [0:0x6500000+400000,0:0x6d00000+700000,0:0x7800000+800000,0:0x8400000+800000,0:0x9000000+800000,0:0x9c00000+700000,0:0xa700000+900000,0:0xb400000+800000,0:0xc000000+500000])\n2016-08-25 00:44:03.348803 7f7c117ff700 20 bluefs _drop_link destroying file(ino 18 size 0x3f3d402 mtime 2016-08-25 00:41:26.299110 bdev 0 extents [0:0x6500000+400000,0:0x6d00000+700000,0:0x7800000+800000,0:0x8400000+800000,0:0x9000000+800000,0:0x9c00000+700000,0:0xa700000+900000,0:0xb400000+800000,0:0xc000000+500000])\n\nbluefs part of the log is within ceph.\nSo, log 254 is added to the recycle list and at the same time it is added for deletion. The following log entry is in response to rocksdb delete call.\n2016-08-25 00:44:03.348794 7f7c117ff700 10 bluefs unlink db.wal/000254.log\n\nIt seems there is a race condition in this portion (?).\n\nI was going through the rocksdb code and I found the following.\n1. DBImpl::FindObsoleteFiles is the one that is responsible for populating log_recycle_files and log_delete_files. It is also deleting entries from alive_log_files_. But, this is always under mutex_ lock.\n2. Log is deleted from DBImpl::DeleteObsoleteFileImpl which is _not_ under lock , but iterating over log_delete_files. This is fishy but it shouldn't be the reason for same log number end up in two list.\n3. I saw all the places but the following  place alive_log_files_ (within DBImpl::WriteImpl)  is accessed without lock.\n\n4625       alive_log_files_.back().AddSize(log_entry.size());   \n\nCan it be reintroducing the same log number (254) , I am not sure.\n\nIf I run _disabling_ rocksdb log recyling it is going through fine for me, no db error.\n\nDisabling log recycling is probably not an option for us as I am seeing it is introducing spikes and the output is not very stable.\n\nAppreciate any help resolving this issue.\n\nThanks & Regards\nSomnath\n",
	"number": 1303,
	"title": "log recycling is trying to reuse the deleted log file"
}, {
	"body": "We can add prev_ for skiplist node to run Prev in O(1) instead of O(logN) as it locks when inserting.\nAnd it does no harm to read without lock.\n",
	"number": 1300,
	"title": "add prev_ for skiplist node"
}, {
	"body": "Closes https://github.com/facebook/rocksdb/issues/697\nCloses https://github.com/facebook/rocksdb/issues/1151\n",
	"number": 1298,
	"title": "Add TransactionDB and OptimisticTransactionDB to the Java API"
}, {
	"body": "Hi,\nwhat is the fastest way to delete all keys from column family? Is sequence of `DropColumnFamily(); CreateColumnFamily();` enough fast (preferable constant time) and does it guarantee that no keys from the given column family would exist after column family with the same name is reopened?\n\nI need to do this once a day - clear about 30 column families from about 30 threads, in the same time with minimal delay.\n",
	"number": 1295,
	"title": "Best way to delete everything from column family"
}, {
	"body": "If We want to split a big sst file into two smaller sst files according to the middle-key, What can we do?\nCan we travel a specified Block directly\uff1f\n",
	"number": 1293,
	"title": "How to get the middle-key of a sst file?"
}, {
	"body": "I am running into segmentation fault with the following parameters:\n\n===== Benchmark =====\nStart filluniquerandom at Mon Aug 22 11:37:00 PDT 2016\nLoading 1048576 unique keys randomly into database...\n./db_bench --db=/rocksdb/db/ --num_levels=6 --key_size=20 --value_size=800 --block_size=4096 --cache_size=17179869184 --cache_numshardbits=6 --compression_type=snappy --compression_ratio=0.5 --hard_rate_limit=2 --rate_limit_delay_max_milliseconds=1000000 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=134217728 --max_bytes_for_level_base=1073741824 --sync=0 --disable_data_sync=1 --verify_checksum=1 --delete_obsolete_files_period_micros=62914560 --max_grandparent_overlap_factor=10 --statistics=1 --stats_per_interval=1 --stats_interval=1048576 --histogram=1 --memtablerep=skip_list --bloom_bits=10 --num_multi_db=8 --rate_limiter_bytes_per_sec=157286400 --open_files=20480 --level0_file_num_compaction_trigger=8 --level0_slowdown_writes_trigger=16 --level0_stop_writes_trigger=24 --max_background_compactions=16 --max_background_flushes=16 --benchmarks=filluniquerandom --use_existing_db=0 --num=1048576 --threads=1 2>&1 | tee /rocksdb/output/benchmark_filluniquerandom.log\nRocksDB:    version 4.9\nDate:       Mon Aug 22 11:37:12 2016\nCPU:        8 \\* Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz\nCPUCache:   8192 KB\n2016/08/22-11:38:55  ... thread 0: (1048576,1048576) ops and (323053.1,323053.1) ops/second in (3.245832,3.245832) seconds\n\nI have attached the output log for reference as well.\n\n[segmentationfault.txt](https://github.com/facebook/rocksdb/files/431016/segmentationfault.txt)\n",
	"number": 1292,
	"title": "segmentation fault in RocksDB version 4.9"
}, {
	"body": "I have a problem with RocksDB version: 4.6.1. The crash breaktrace as blow:\n\n```\n#0  rocksdb::Compaction::ShouldStopBefore(rocksdb::Slice const&) () at db/compaction.cc:300\n#1  0x00000000005b128a in rocksdb::CompactionJob::ProcessKeyValueCompaction(rocksdb::CompactionJob::SubcompactionState*) () at db/compaction_job.cc:672\n#2  0x00000000006d4d40 in execute_native_thread_routine () at ../../../../../libstdc++-v3/src/c++11/thread.cc:84\n#3  0x000000335fa07a51 in start_thread () from /lib64/libpthread.so.0\n#4  0x000000335f6e896d in clone () from /lib64/libc.so.6\n```\n\nStrace to source code. Only such useful info as blow:\n\n```\n(gdb) f 1\n#1  0x00000000005b128a in rocksdb::CompactionJob::ProcessKeyValueCompaction(rocksdb::CompactionJob::SubcompactionState*) () at db/compaction_job.cc:672\n672         } else if (sub_compact->compaction->ShouldStopBefore(key) &&\n(gdb) p key\n$1 = 0\n(gdb) f 0\n#0  rocksdb::Compaction::ShouldStopBefore(rocksdb::Slice const&) () at db/compaction.cc:300\n300           overlapped_bytes_ += grandparents_[grandparent_index_]->fd.GetFileSize();\n(gdb) p grandparent_index_\nNo symbol \"grandparent_index_\" in current context.\n(gdb) p grandparents_\n```\n\nIs anybody can help me? Thanks.\n",
	"number": 1290,
	"title": "crash in ShouldStopBefore of sub-compact"
}, {
	"body": "Good day,\n\nWe use Go and [gorocksdb](https://github.com/tecbot/gorocksdb).\n\nHowever, as far as I'm concerned, gorocksdb requires an exposed C function [here](https://github.com/facebook/rocksdb/blob/master/include/rocksdb/c.h) in order to run table_options.block_cache->GetUsage();\n\nIf that is the case, can we go ahead and make this modification and send a PR here first?\n\nThanks.\n\nJC\n",
	"number": 1289,
	"title": "Block cache get size"
}, {
	"body": "There  is a restriction on the SST files ingested into a DB, is that the key ranges in the sst files can not overlap  with each other or existing/deleted keys in the DB, why? we want to write s single sst file and than \nadd it to an existing DB, but failed due to the restriction. Could you tell us  what about  the restriction you consider? And what can I do to add sst filea to DB if the keys ranges overlap with the the files in the DB?\n",
	"number": 1288,
	"title": "We want to add a single sst file to a DB, but failed due to the too tight restriction."
}, {
	"body": "Hi.\nFreebsd build fail with `stoull` is not a member of `std` error\n\n```\n]>uname -a\nFreeBSD devel.ximad.com 10.2-RELEASE-p9 FreeBSD 10.2-RELEASE-p9 #0: Thu Jan 14 01:32:46 UTC 2016     root@amd64-builder.daemonology.net:/usr/obj/usr/src/sys/GENERIC  amd64\n```\n\n```\n]>gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/usr/local/libexec/gcc48/gcc/x86_64-portbld-freebsd10.2/4.8.5/lto-wrapper\nTarget: x86_64-portbld-freebsd10.2\nConfigured with: ./../gcc-4.8.5/configure --disable-multilib --disable-bootstrap --disable-nls --enable-gnu-indirect-function --libdir=/usr/local/lib/gcc48 --libexecdir=/usr/local/libexec/gcc48 --program-suffix=48 --with-as=/usr/local/bin/as --with-gmp=/usr/local --with-gxx-include-dir=/usr/local/lib/gcc48/include/c++/ --with-ld=/usr/local/bin/ld --with-pkgversion='FreeBSD Ports Collection' --with-system-zlib --with-ecj-jar=/usr/local/share/java/ecj-4.5.jar --enable-languages=c,c++,objc,fortran,java --prefix=/usr/local --localstatedir=/var --mandir=/usr/local/man --infodir=/usr/local/info/gcc48 --build=x86_64-portbld-freebsd10.2\nThread model: posix\ngcc version 4.8.5 (FreeBSD Ports Collection) \n```\n\n```\n]>git clone https://github.com/facebook/rocksdb\n]>cd rocksdb\n]>gmake\nMakefile:101: Warning: Compiling in debug mode. Don't use the resulting binary in production\n  GEN      util/build_version.cc\n...\n  CC       util/lru_cache.o\n  CC       util/threadpool.o\n  CC       util/transaction_test_util.o\nutil/transaction_test_util.cc: In member function 'bool rocksdb::RandomTransactionInserter::DoInsert(rocksdb::DB*, rocksdb::Transaction*, bool)':\nutil/transaction_test_util.cc:97:19: error: 'stoull' is not a member of 'std'\n       int_value = std::stoull(value);\n                   ^\nutil/transaction_test_util.cc: In static member function 'static rocksdb::Status rocksdb::RandomTransactionInserter::Verify(rocksdb::DB*, uint16_t)':\nutil/transaction_test_util.cc:208:28: error: 'stoull' is not a member of 'std'\n       uint64_t int_value = std::stoull(value.ToString());\n                            ^\ngmake: *** [Makefile:1456: util/transaction_test_util.o] Error 1\n```\n",
	"number": 1286,
	"title": "freebsd compile error"
}, {
	"body": "Looks like there's a bug in `db_ttl_impl`. `DBWithTTLImpl` overrides only the `NewIterator` method. There's no 'NewIterators' in this class, so this method is inherited, and iterators obtained as the result of this method are not `TtlIterator`. They don't know anything about TTL and won't strip the time from value.\n\nexample of patch against 4.5.1\nhttp://textuploader.com/58c63\n",
	"number": 1283,
	"title": "inherited NewIterators method in DBWithTTLImpl"
}, {
	"body": "Hi,\nWe'd like to make use of https://github.com/facebook/rocksdb/wiki/EventListener, but in the java API. Are there any plans to add this?\n\nThanks,\nDamian\n",
	"number": 1282,
	"title": "Support for EventListeners in the Java API"
}, {
	"body": "Given the use of WriteBatch I think that \"groups\" or \"commit groups\" is a better term than \"batches\" for reporting the number of commit groups.\n\nhttps://www.facebook.com/groups/rocksdb.dev/permalink/999367556828425/\n",
	"number": 1277,
	"title": "consider changing \"batches\" to \"groups\""
}, {
	"body": "I would like to use RocksDB on a windows 7 64 bits platform.\nI try to compile using the steps describe in the CMakeLists.txt using the following command:\n\n1) cmake -G \"Visual Studio 14 Win64\" -DJNI=1 .. \nlog file: [msbuild.txt](https://github.com/facebook/rocksdb/files/411586/msbuild.txt)\n\n2) msbuild rocksdb.sln /m\nThe last one generates the following errors:\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(2039): error C2131: expression did not evaluate to a constant [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(2041): error C2131: expression did not evaluate to a constant [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(2043): error C2440: 'reinterpret_cast': cannot convert from 'int' to 'jint' [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(2066): error C2440: 'reinterpret_cast': cannot convert from 'jint' to 'int32_t' [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(3346): error C2131: expression did not evaluate to a constant [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(3348): error C2131: expression did not evaluate to a constant [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(3350): error C2440: 'reinterpret_cast': cannot convert from 'int' to 'jint' [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n         c:\\workspace\\rocksdb-master\\java\\rocksjni\\options.cc(3374): error C2440: 'reinterpret_cast': cannot convert from 'jint' to 'int32_t' [C:\\workspace\\rocksdb-master\\build\\java\\rocksdbjni.vcxproj]\n\nfull log file:  [cmake.txt](https://github.com/facebook/rocksdb/files/411585/cmake.txt)\n\nIf I compile the project without JNI it works but I've got an exception java.lang.UnsatisfiedLinkError when I try to use the DLL with a recent Rocksdb Java JAR \n\nWhat did I miss or what can I do in order to build correctly ?\n",
	"number": 1270,
	"title": "windows + jni compilation errors"
}, {
	"body": "I have compiled rocksdb with Visual Studio 2015 x64 using cmake as static library with --DSNAPPY=1 option according to tutorial for Windows in CMakeLists.txt and tried to use the c_simple_example (because only C API works, c++ example gets unresolved externals). All compiles well,but when i am calling\n`db = rocksdb_open(options, &nodesFileName[0], &err);`\ni get error \nInvalid argument: Compression type Snappy is not linked with the binary.\nIt doesn\u00b4t even work if i specify exactly not to use compression.\n`rocksdb_options_set_compression(options, 0);`\n\nI linked rocksdb with this version of snappy https://bitbucket.org/robertvazan/snappy-visual-cpp. Linking rocksdblib into project results in unresolved external snappy::RawCompress, so it can\u00b4t be compiled that way.\n\nHow do you compile this on Windows without the Snappy error? I am pretty sure i have tried all combinations of\nConfiguration type -> Static lib / Dynamic library\nand\nCode generation -> Multithreaded / Multithreaded DLL.\nOnly  Static lib/Multithreaded compiles it correctly, but maybe because of bugged C API it still throws the snappy error?\n",
	"number": 1266,
	"title": "Windows error: Compression type Snappy is not linked with the binary."
}, {
	"body": "In traversing DB using Iterator API ,  it show significant difference between c++ API and Java API. Why?\n",
	"number": 1263,
	"title": "What can I do to improve the performance of rocksdbjava Iterator API?"
}, {
	"body": "I can not find API to set `delay_write_rate` in RocksJava? Is it not supported?\n",
	"number": 1260,
	"title": "delay_write_rate?"
}, {
	"body": "app is using rocksdb /tmp/rocksdb and created a backup to /tmp/rocksdb/backup\ncan we run another application to restore the db from /tmp/rocksdb/backup?\n",
	"number": 1256,
	"title": "can restore rocksdb when using?"
}, {
	"body": "Windows refuses to honor a positional read with an offset that is too big for the given file yet this apparently works for Posix. What is the rationale behind this ?\n\nc:\\dev\\rocksdb\\rocksdb\\util\\env_basic_test.cc(243): error: Value of: rand_file->Read(1000, 5, &result, scratch).ok()\nARNING:   Actual: false\n",
	"number": 1255,
	"title": "EnvDefault/EnvBasicTestWithParam.ReadWrite/0 fails on Windows due to positional read beyond the end of the file"
}, {
	"body": "The `multiGet` function in RocksJava in the `RocksDB` and and `Transaction` class make copies of the provided `key` lists, this should be avoided by either:\n1. Using Slices instead so that the `byte[]` are encapsulated and the `List`/`Map` contains just a reference to the slice.\n2. Just returning a `List` of values as opposed to a `Map`. Probably the simplest option and closest to the C++ API!\n",
	"number": 1251,
	"title": "RocksDB#multiGet copies keys"
}, {
	"body": "It is common practice to pre-allocate arrays so in high write\nsituations you don't need to keep\ncreating large array objects. As the current API requires full array,\nthis means an array needs to be\ncopied before passing to RocksDB.\n\nBy supporting the underlying C API, an additional memory copy can be\navoided leading to\na big saving in GC in high write circumstances.\n",
	"number": 1247,
	"title": "Support underlying put API"
}, {
	"body": "I'm not sure if it's a bug or intentional. On Linux you need to set `ROCKSDB_PLATFORM_POSIX` explicitly if certain header files are used.\n\nHere's a minimal example:\n\n```\n#include \"table/block_based_table_factory.h\"\n\nint main() {\n  rocksdb::Options options;\n  rocksdb::BlockBasedTableOptions table_options;\n  options.table_factory.reset(rocksdb::NewBlockBasedTableFactory(table_options));\n\n  return 0;\n}\n```\n\nPut this file into the root of a RocksDB checkout and compile it e.g. with:\n\n```\nclang++ table_options.cc -L../ -Iinclude -I. -L. --std=c++11 -lrocksdb_debug -lpthread -lbz2 -lz -lsnappy\n```\n\nYou'll get:\n\n```\nIn file included from table_options.cc:1:\nIn file included from ./table/block_based_table_factory.h:18:\nIn file included from ./db/dbformat.h:20:\n./util/coding.h:84:7: error: use of undeclared identifier 'port'\n  if (port::kLittleEndian) {\n      ^\n./util/coding.h:98:7: error: use of undeclared identifier 'port'\n  if (port::kLittleEndian) {\n      ^\n2 errors generated\n```\n\nIt compiles fine when `-DROCKSDB_PLATFORM_POSIX` is set:\n\n```\nclang++ table_options.cc -L../ -Iinclude -I. -L. --std=c++11 -lrocksdb_debug -lpthread -lbz2 -lz -lsnappy -DROCKSDB_PLATFORM_POSIX\n```\n",
	"number": 1246,
	"title": "ROCKSDB_PLATFORM_POSIX needs to be set"
}, {
	"body": "This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. \n",
	"number": 1241,
	"title": "Added CompactionFilterFactory support to RocksJava"
}, {
	"body": "The tests run by `make check` require Bash. On Debian you'd need to run\nthe test as `make SHELL=/bin/bash check`. This commit makes it work on\nall POSIX compatible shells (tested on Debian with `dash`).\n\nThis is a follow-up on #1225 which was reverted as it didn't create the\nlogs files correctly.\n",
	"number": 1238,
	"title": "Remove bashism from `make check`"
}, {
	"body": "I have a setup with ~1000 column families. I hit a deadlock where a write is stalled forever and now all the operations are waiting on condition_variable. The Logs suggest the write is stalled waiting for some background flush/compaction but they are not getting scheduled. \n\nFollowing is the stack trace for the various threads and some of the db and column family options\n\n2016/07/22-13:36:41.410949 7fc6cbf07700 [WARN] [1008_1] Stopping writes because we have 148 level-0 files\n2016/07/22-13:36:41.605307 7fc6cd70a700 [WARN] [1008_1] Stopping writes because we have 147 level-0 files\n\nStacktrace for pid: 157756\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d293da5c  std::condition_variable::wait(std::unique_lockstd::mutex&amp;)\n0x7fc6d419a58b  rocksdb::WriteThread::BlockingAwaitState(rocksdb::WriteThread::Writer_, unsigned char)\n0x7fc6d419a65d  rocksdb::WriteThread::AwaitState(rocksdb::WriteThread::Writer_, unsigned char, rocksdb::WriteThread::AdaptationContext_)\n0x7fc6d419ae23  rocksdb::WriteThread::EnterUnbatched(rocksdb::WriteThread::Writer_, rocksdb::InstrumentedMutex_)\n0x7fc6d4116599  rocksdb::DBImpl::CreateColumnFamily(rocksdb::ColumnFamilyOptions const&amp;, std::string const&amp;, rocksdb::ColumnFamilyHandle_*)\n\nStacktrace for pid: 157762\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d293da5c  std::condition_variable::wait(std::unique_lockstd::mutex&amp;)\n0x7fc6d419a58b  rocksdb::WriteThread::BlockingAwaitState(rocksdb::WriteThread::Writer_, unsigned char)\n0x7fc6d419a65d  rocksdb::WriteThread::AwaitState(rocksdb::WriteThread::Writer_, unsigned char, rocksdb::WriteThread::AdaptationContext_)\n0x7fc6d419ae23  rocksdb::WriteThread::EnterUnbatched(rocksdb::WriteThread::Writer_, rocksdb::InstrumentedMutex_)\n0x7fc6d411bd3e  rocksdb::DBImpl::FlushMemTable(rocksdb::ColumnFamilyData_, rocksdb::FlushOptions const&amp;)\n0x7fc6d411c880  rocksdb::DBImpl::Flush(rocksdb::FlushOptions const&amp;, rocksdb::ColumnFamilyHandle*)\n\nStacktrace for pid: 157763\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41a23fd  rocksdb::port::CondVar::Wait()\n0x7fc6d41f3975  rocksdb::InstrumentedCondVar::Wait()\n0x7fc6d4108cf8  rocksdb::DBImpl::DelayWrite(unsigned long)\n0x7fc6d412ae67  rocksdb::DBImpl::WriteImpl(rocksdb::WriteOptions const&amp;, rocksdb::WriteBatch_, rocksdb::WriteCallback_)\n0x7fc6d412bd04  rocksdb::DBImpl::Write(rocksdb::WriteOptions const&amp;, rocksdb::WriteBatch*)\n\nStacktrace for pid: 157777\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n\nStacktrace for pid: 157778\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n0x7fc6d2bc32df  \n\nStacktrace for pid: 157780\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n\nStacktrace for pid: 157781\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n0x7fc6d2bc32df  \n0x7fc6d20b3c1d  clone\n0   [(nil)]\n\nStacktrace for pid: 157782\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n0x7fc6d2bc32df  \n0x7fc6d20b3c1d  clone\n\nStacktrace for pid: 157783\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n0x7fc6d2bc32df  \n0x7fc6d20b3c1d  clone\n0   [(nil)]\n\nStacktrace for pid: 157784\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n0x7fc6d2bc32df  \n\nStacktrace for pid: 157785\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n0x7fc6d2bc32df  \n0x7fc6d20b3c1d  clone\n0   [(nil)]\n\nStacktrace for pid: 157786\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d41ed8c1  rocksdb::ThreadPool::BGThread(unsigned long)\n0x7fc6d41edaa3  \n0x7fc6d2bc32df  \n\nStacktrace for pid: 160939\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d293da5c  std::condition_variable::wait(std::unique_lockstd::mutex&amp;)\n0x7fc6d419a58b  rocksdb::WriteThread::BlockingAwaitState(rocksdb::WriteThread::Writer_, unsigned char)\n0x7fc6d419a65d  rocksdb::WriteThread::AwaitState(rocksdb::WriteThread::Writer_, unsigned char, rocksdb::WriteThread::AdaptationContext_)\n0x7fc6d419a956  rocksdb::WriteThread::JoinBatchGroup(rocksdb::WriteThread::Writer_)\n0x7fc6d4129fba  rocksdb::DBImpl::WriteImpl(rocksdb::WriteOptions const&amp;, rocksdb::WriteBatch_, rocksdb::WriteCallback_)\n0x7fc6d412bd04  rocksdb::DBImpl::Write(rocksdb::WriteOptions const&amp;, rocksdb::WriteBatch*)\n\nStacktrace for pid: 160940\n0x7fc6d2bc760a  pthread_cond_wait\n0x7fc6d293da5c  std::condition_variable::wait(std::unique_lockstd::mutex&amp;)\n0x7fc6d419a58b  rocksdb::WriteThread::BlockingAwaitState(rocksdb::WriteThread::Writer_, unsigned char)\n0x7fc6d419a65d  rocksdb::WriteThread::AwaitState(rocksdb::WriteThread::Writer_, unsigned char, rocksdb::WriteThread::AdaptationContext_)\n0x7fc6d419a956  rocksdb::WriteThread::JoinBatchGroup(rocksdb::WriteThread::Writer_)\n0x7fc6d4129fba  rocksdb::DBImpl::WriteImpl(rocksdb::WriteOptions const&amp;, rocksdb::WriteBatch_, rocksdb::WriteCallback_)\n0x7fc6d412bd04  rocksdb::DBImpl::Write(rocksdb::WriteOptions const&amp;, rocksdb::WriteBatch*)\n\nOptions.max_total_wal_size: 2147483648\nwrite_buffer_size: 262144\n2016/07/22-13:31:32.837502 3118700                   max_write_buffer_number: 4\n2016/07/22-13:31:32.837504 3118700                          arena_block_size: 33554432\n2016/07/22-13:31:32.837505 3118700                memtable_prefix_bloom_bits: 0\n2016/07/22-13:31:32.837507 3118700              memtable_prefix_bloom_probes: 6\n2016/07/22-13:31:32.837508 3118700  memtable_prefix_bloom_huge_page_tlb_size: 0\n2016/07/22-13:31:32.837509 3118700                     max_successive_merges: 0\n2016/07/22-13:31:32.837511 3118700                            filter_deletes: 0\n2016/07/22-13:31:32.837512 3118700                  disable_auto_compactions: 0\n2016/07/22-13:31:32.837513 3118700       soft_pending_compaction_bytes_limit: 0\n2016/07/22-13:31:32.837514 3118700       hard_pending_compaction_bytes_limit: 0\n2016/07/22-13:31:32.837516 3118700        level0_file_num_compaction_trigger: 8\n2016/07/22-13:31:32.837517 3118700            level0_slowdown_writes_trigger: 25\n2016/07/22-13:31:32.837518 3118700                level0_stop_writes_trigger: 40\n2016/07/22-13:31:32.837519 3118700            max_grandparent_overlap_factor: 10\n2016/07/22-13:31:32.837520 3118700                expanded_compaction_factor: 25\n2016/07/22-13:31:32.837521 3118700                  source_compaction_factor: 1\n2016/07/22-13:31:32.837523 3118700                     target_file_size_base: 4194304\n2016/07/22-13:31:32.837524 3118700               target_file_size_multiplier: 10\n2016/07/22-13:31:32.837525 3118700                  max_bytes_for_level_base: 33554432\n",
	"number": 1235,
	"title": "Deadlock after hitting Stall "
}, {
	"body": "ava.lang.ExceptionInInitializerError: empty message\ncom.facebook.imagepipeline.memory.NativeMemoryChunkPool.com.facebook.imagepipeline.memory.NativeMemoryChunk alloc(int)(NativeMemoryChunkPool.java:60)\ncom.facebook.imagepipeline.memory.NativeMemoryChunkPool.void free(java.lang.Object)(NativeMemoryChunkPool.java:22)\njava.lang.Object alloc(int)\ncom.facebook.imagepipeline.memory.BasePool.java.lang.Object get(int)(BasePool.java:259)\ncom.facebook.imagepipeline.memory.NativePooledByteBufferOutputStream.void <init>(com.facebook.imagepipeline.memory.NativeMemoryChunkPool,int)(NativePooledByteBufferOutputStream.java:53)\ncom.facebook.imagepipeline.memory.NativePooledByteBufferFactory.com.facebook.imagepipeline.memory.NativePooledByteBuffer newByteBuffer(java.io.InputStream,int)(NativePooledByteBufferFactory.java:98)\ncom.facebook.imagepipeline.memory.NativePooledByteBufferFactory.com.facebook.imagepipeline.memory.PooledByteBufferOutputStream newOutputStream()(NativePooledByteBufferFactory.java:26)\ncom.facebook.imagepipeline.memory.PooledByteBuffer newByteBuffer(java.io.InputStream,int)\ncom.facebook.imagepipeline.memory.PooledByteBuffer newByteBuffer(byte[])\ncom.facebook.imagepipeline.memory.PooledByteBuffer newByteBuffer(java.io.InputStream)\ncom.facebook.imagepipeline.cache.BufferedDiskCache.com.facebook.imagepipeline.memory.PooledByteBuffer readFromDiskCache(com.facebook.cache.common.CacheKey)(BufferedDiskCache.java:316)\ncom.facebook.imagepipeline.cache.BufferedDiskCache.com.facebook.imagepipeline.cache.StagingArea access$000(com.facebook.imagepipeline.cache.BufferedDiskCache)(BufferedDiskCache.java:38)\ncom.facebook.imagepipeline.memory.PooledByteBuffer access$400(com.facebook.imagepipeline.cache.BufferedDiskCache,com.facebook.cache.common.CacheKey)\nvoid access$500(com.facebook.imagepipeline.cache.BufferedDiskCache,com.facebook.cache.common.CacheKey,com.facebook.imagepipeline.image.EncodedImage)\ncom.facebook.imagepipeline.cache.BufferedDiskCache$2.com.facebook.imagepipeline.image.EncodedImage call()(BufferedDiskCache.java:162)\ncom.facebook.imagepipeline.cache.BufferedDiskCache$2.java.lang.Object call()(BufferedDiskCache.java:146)\nbolts.Task$2.void run()(Task.java:195)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1076)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:569)\njava.lang.Thread.run(Thread.java:856)\n",
	"number": 1234,
	"title": "java.lang.ExceptionInInitializerError"
}, {
	"body": "The C++ API .key() and .value() methods return Slice objects, which are just pointers to external byte arrays. This is very efficient because it avoids copying data.\n\nThe Java API, unfortunately, forces some unnecessary copying internally. Instead, it should offer the option to get a slice of a native data structure. This change will yield substantial performance benefits and greatly reduce the amount of garbage that gets generated in the JVM.\n\nSee related discussion on Facebook: https://www.facebook.com/groups/rocksdb.dev/permalink/985085361589978/ with @adamretter \n",
	"number": 1227,
	"title": "Make Java API more efficient by exposing zero-copy slices"
}, {
	"body": "LOG files should be unbuffered (writing to OS immediately), or at least should have an option to write without buffering. Otherwise full logs might not be written in case of rocksdb got crashed, which makes debugging harder.\n\n```\ndiff --git a/util/env_posix.cc b/util/env_posix.cc\nindex 0c14c03..cf9712b 100644\n--- a/util/env_posix.cc\n+++ b/util/env_posix.cc\n@@ -561,6 +561,7 @@ class PosixEnv : public Env {\n     {\n       IOSTATS_TIMER_GUARD(open_nanos);\n       f = fopen(fname.c_str(), \"w\");\n+      setvbuf( f, (char *)NULL, _IONBF, 0 );\n     }\n     if (f == nullptr) {\n       result->reset();\n```\n",
	"number": 1216,
	"title": "LOG files should be unbuffered"
}, {
	"body": "It's an xml for starters, plus Maven expects it to be named pom.xml.\nThere's no other pom.xml in this directory which makes this change trivial.\n",
	"number": 1211,
	"title": "rocksdb/java/rocksjni.pom does not follow Maven naming convention"
}, {
	"body": "following code hangs forever after 10-30minutes\n\n```\n#include <string.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <stdio.h>\n#include <pthread.h>\n#include \"rocksdb/c.h\"\n\ntypedef struct {\n        rocksdb_t *db;\n        rocksdb_writeoptions_t *woptions;\n        rocksdb_readoptions_t *roptions;\n} rocksdb_data_t;\n\nsize_t create_random_key(char *prefix, size_t prefix_size, char *key, size_t key_size)\n{\n        memcpy(key, prefix, prefix_size);\n        for (size_t i = 0; i < key_size - prefix_size; i++) {\n                key[prefix_size + i] = rand() % 255;\n        }\n\n        return key_size;\n}\n\nvoid *write_thread(void *arg) {\n        rocksdb_data_t *data = (rocksdb_data_t *)arg;\n\n        while (1) {\n                char key[16];\n                size_t key_size = create_random_key(\"wt\", 2, key, sizeof(key));\n\n                char *err_str = NULL;\n                rocksdb_put(data->db, data->woptions, key, key_size, \"test1234\", 8, &err_str);\n                if (err_str) {\n                        fprintf(stderr, \"rocksdb_put error: %s\\n\", err_str);\n                        free(err_str);\n                }\n                usleep(rand() % 1000);\n        }\n}\nint main() {\n        rocksdb_options_t *options = rocksdb_options_create();\n        rocksdb_options_set_create_if_missing(options, 1);\n        rocksdb_options_enable_statistics(options);\n\n        rocksdb_env_t *env = rocksdb_create_default_env();\n        rocksdb_env_set_high_priority_background_threads(env, 2);\n        rocksdb_env_set_background_threads(env, 2);\n\n        rocksdb_options_set_env(options, env);\n        rocksdb_options_set_max_background_flushes(options, 2);\n        //rocksdb_options_set_max_background_compactions(options, 1);\n        rocksdb_options_set_disable_auto_compactions(options, 1);\n\n        rocksdb_options_set_write_buffer_size(options, 64 * 1024 * 1024);\n        rocksdb_options_set_max_write_buffer_number(options, 8);\n        rocksdb_options_set_max_log_file_size(options, 1 * 1024 * 1024 * 1024);\n        rocksdb_options_set_keep_log_file_num(options, 5);\n        rocksdb_options_set_log_file_time_to_roll(options, 0);\n\n        char *err_str = NULL;\n        rocksdb_t *db = rocksdb_open(options, \"/tmp/test_storage\", &err_str);\n        if (err_str) {\n                fprintf(stderr, \"rocksdb_open error: %s\\n\", err_str);\n                free(err_str);\n                exit(1);\n        }\n        rocksdb_data_t data;\n        data.db = db;\n        data.woptions = rocksdb_writeoptions_create();\n        data.roptions = rocksdb_readoptions_create();\n\n        pthread_t wthreads[30];\n        for (size_t i = 0; i < sizeof(wthreads) / sizeof(wthreads[0]); i++) {\n                pthread_create(&wthreads[i], NULL, write_thread, &data);\n        }\n\n        void *result;\n        pthread_join(wthreads[0], &result);\n}\n\n```\n\ngdb output:\n\n```\n\nThread 35 (Thread 0x7ff06d230700 (LWP 22361)):\n#0  0x00007ff06d5ec0af in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x000000000052b9c7 in rocksdb::ThreadPool::BGThread (this=this@entry=0xe6ba90, thread_id=thread_id@entry=0) at util/thread_posix.cc:68\n#2  0x000000000052bbf3 in rocksdb::BGThreadWrapper (arg=0xe76440) at util/thread_posix.cc:143\n#3  0x00007ff06d5e80db in start_thread () from /lib64/libpthread.so.0\n#4  0x00007ff06d31890d in clone () from /lib64/libc.so.6\n\n.....\n\nThread 31 (Thread 0x7ff06b22c700 (LWP 22365)):\n#0  0x00007ff06d5ec0af in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x00007ff06ddcaa2c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib64/libstdc++.so.6\n#2  0x00000000004edeac in wait<rocksdb::WriteThread::BlockingAwaitState(rocksdb::WriteThread::Writer*, uint8_t)::__lambda4> (__p=..., __lock=..., this=0x7ff06b22ba80) at /usr/include/c++/4.8/condition_variable:93\n#3  rocksdb::WriteThread::BlockingAwaitState (this=<optimized out>, w=0x7ff06b22ba00, goal_mask=<optimized out>) at db/write_thread.cc:37\n#4  0x00000000004edf7d in rocksdb::WriteThread::AwaitState (this=this@entry=0xe77188, w=w@entry=0x7ff06b22ba00, goal_mask=goal_mask@entry=14 '\\016',\n    ctx=ctx@entry=0x7ff3e0 <rocksdb::WriteThread::JoinBatchGroup(rocksdb::WriteThread::Writer*)::ctx>) at db/write_thread.cc:157\n#5  0x00000000004ee274 in rocksdb::WriteThread::JoinBatchGroup (this=this@entry=0xe77188, w=w@entry=0x7ff06b22ba00) at db/write_thread.cc:227\n#6  0x00000000004844a2 in rocksdb::DBImpl::WriteImpl (this=0xe76ca0, write_options=..., my_batch=<optimized out>, callback=callback@entry=0x0) at db/db_impl.cc:4201\n#7  0x00000000004861a4 in rocksdb::DBImpl::Write (this=<optimized out>, write_options=..., my_batch=<optimized out>) at db/db_impl.cc:4155\n#8  0x0000000000486234 in rocksdb::DB::Put (this=this@entry=0xe76ca0, opt=..., column_family=0xe84550, key=..., value=...) at db/db_impl.cc:5297\n#9  0x0000000000486281 in rocksdb::DBImpl::Put (this=this@entry=0xe76ca0, o=..., column_family=<optimized out>, key=..., val=...) at db/db_impl.cc:4130\n#10 0x000000000048654a in rocksdb::DB::Put (this=0xe76ca0, options=..., key=..., value=...) at ./include/rocksdb/db.h:185\n#11 0x0000000000457ea7 in rocksdb_put (db=<optimized out>, options=<optimized out>, key=<optimized out>, keylen=<optimized out>, val=<optimized out>, vallen=<optimized out>, errptr=0x7ff06b22bd28) at db/c.cc:641\n#12 0x0000000000456fb1 in write_thread (arg=0x7fff22013fb0) at test.c:35\n#13 0x00007ff06d5e80db in start_thread () from /lib64/libpthread.so.0\n#14 0x00007ff06d31890d in clone () from /lib64/libc.so.6\n```\n",
	"number": 1208,
	"title": "RocksDB 4.6.1 hangs forever with disabled autocompactions"
}, {
	"body": "When creating a snapshot and the directory already exists for instance we get Status IOError\nbut only the string has at the end \"File exists\" we should preserve the errno code\n",
	"number": 1205,
	"title": "IOError preserve errno"
}, {
	"body": "There seems to be a typo error in the following paragraph of the [_Write-Ahead-Log-File-Format.md_](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log-File-Format) :\n\n_Write ahead log (WAL) serializes memtable operations to persistent medium as log files. In the event of a failure, WAL files can be used to recover the database to its consistent state, by reconstructing the memtable from the logs. When a memtable is flushed out to persistent medium safely, the corresponding WAL log(s) become obsolete and are **achieved.** Eventually the archived logs are purged from disk after a certain period of time._\n\nShouldn't **achieved** be corrected to **archived** ? \n\nIf this seems legit, then you can find the corrected version in my [repo](https://github.com/aayushKumarJarvis/rocks-wiki). I request people to review this change and merge the correction.\n",
	"number": 1203,
	"title": "Typo error in Wiki Documentation for Write-Ahead-Log-File-Format.md"
}, {
	"body": "In the project in which we're hoping to integrate RocksDB, we use a number of maps cut up into rectangle objects. \n\nEach rectangle object consists of the following attributes\nMinX, MaxX, MinY, MaxY, and Rectangle Name (each rectangle Name being a unique string).\n\nI've used the following methods to store the rectangles into the SpatialDB\n(please correct me if I'm wrong here)\n\nfor(MapSet->FirstRectangle(); MapSet->HasNextRectangle(); MapSet->Next()) \n{\nfeatures.Set(\"MapResolution\", MapSet->Resolution)\n\ndb->Insert(WriteOptions(), BoundingBox<double>(MinX, MinY, MaxX, MaxY), RectName, features, {\"zoom10\"})\n}\n\nit took over the course of a few days to write 3500 of these objects into the rocksdb database. \n\nWhere my problem is now, is that I want to recreate my rectangles from the Spatial Database in which I've stored these attributes. \n\nHow do I go about retrieving these Point values from RocksDB::spatialDB\n",
	"number": 1202,
	"title": "Reading Geo-Data from RocksDB SpatialDB"
}, {
	"body": "Hi.\nJust wanted to let you guys know that in line 585 of /db/internal_stats.cc,\nIn the snprintf function string constant portion saying \"Interval WAL\", it should be GB instead of MB\n\nThanks\n",
	"number": 1201,
	"title": "RocksDB typo found in /db/internal_stats.cc"
}, {
	"body": "Hi,\n\nI have a quick question. We are thinking about doing some encoding for keys in a way that results in something like,\n\nkey = value1|value2\n\nThe idea would be then to iterate over value1 as some sort of prefix (it is not exactly this, but it is a good first order approximation). \n\nWe were not thinking about inserting a value associated to each key originally, just call rocksdb_iter_key and then decode the key. It could simplify our design if we did something like,\n\nkey = value1\nvalue = value2\n\nso that we would call both rocksdb_iter_key and rocksdb_iter_value and not do the decoding of the key. The question is, is there any advantage, performance wise, of doing one thing (calling only rocksdb_iter_key and then decoding the key) vs the other (calling rocksdb_iter_key and rocksdb_iter_value). The decoding of the key is essentially free compared with the cost of I/O so we are not trying to save there, only simplifying the design. In other words, I am interested in gauging the cost of these calls from RocksDB point of view. Is rocksdb_iter_seek/next where most of the I/O cost is, or is the I/O cost distributed among rocksdb_iter_seek/next, rocksdb_iter_key and rocksdb_iter_value?\n\nThanks,\n\nEthan.\n",
	"number": 1200,
	"title": "rocksdb_iter_key vs rocksdb_iter_value performance"
}, {
	"body": "# make shared_lib && INSTALL_PATH=/usr make install-shared && ldconfig\n\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\n  GEN      util/build_version.cc\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ncc1plus: error: bad value (zEC12) for -march= switch\ng++ -Wl,--no-as-needed -shared -Wl,-soname -Wl,librocksdb.so.4.1  -g -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -I. -I./include -std=c++11  -DROCKSDB_PLATFORM_POSIX  -DOS_LINUX -fno-builtin-memcmp -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS=google -DZLIB -DBZIP2 -DROCKSDB_MALLOC_USABLE_SIZE -march=zEC12   -isystem ./third-party/gtest-1.7.0/fused-src -O2 -fno-omit-frame-pointer -DDUMBDUMMY -DNDEBUG -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -fPIC db/builder.cc db/c.cc db/column_family.cc db/compacted_db_impl.cc db/compaction.cc db/compaction_iterator.cc db/compaction_job.cc db/compaction_picker.cc db/convenience.cc db/db_filesnapshot.cc db/dbformat.cc db/db_impl.cc db/db_impl_debug.cc db/db_impl_readonly.cc db/db_impl_experimental.cc db/db_iter.cc db/experimental.cc db/event_helpers.cc db/file_indexer.cc db/filename.cc db/flush_job.cc db/flush_scheduler.cc db/forward_iterator.cc db/internal_stats.cc db/log_reader.cc db/log_writer.cc db/managed_iterator.cc db/memtable_allocator.cc db/memtable.cc db/memtable_list.cc db/merge_helper.cc db/merge_operator.cc db/repair.cc db/slice.cc db/snapshot_impl.cc db/table_cache.cc db/table_properties_collector.cc db/transaction_log_impl.cc db/version_builder.cc db/version_edit.cc db/version_set.cc db/wal_manager.cc db/write_batch.cc db/write_batch_base.cc db/write_controller.cc db/write_thread.cc port/stack_trace.cc port/port_posix.cc table/adaptive_table_factory.cc table/block_based_filter_block.cc table/block_based_table_builder.cc table/block_based_table_factory.cc table/block_based_table_reader.cc table/block_builder.cc table/block.cc table/block_hash_index.cc table/block_prefix_index.cc table/bloom_block.cc table/cuckoo_table_builder.cc table/cuckoo_table_factory.cc table/cuckoo_table_reader.cc table/flush_block_policy.cc table/format.cc table/full_filter_block.cc table/get_context.cc table/iterator.cc table/merger.cc table/meta_blocks.cc table/sst_file_writer.cc table/plain_table_builder.cc table/plain_table_factory.cc table/plain_table_index.cc table/plain_table_key_coding.cc table/plain_table_reader.cc table/table_properties.cc table/two_level_iterator.cc tools/dump/db_dump_tool.cc util/arena.cc util/auto_roll_logger.cc util/bloom.cc util/build_version.cc util/cache.cc util/coding.cc util/comparator.cc util/compaction_job_stats_impl.cc util/crc32c.cc util/db_info_dumper.cc util/delete_scheduler_impl.cc util/dynamic_bloom.cc util/env.cc util/env_hdfs.cc util/env_posix.cc util/file_util.cc util/file_reader_writer.cc util/filter_policy.cc util/hash.cc util/hash_cuckoo_rep.cc util/hash_linklist_rep.cc util/hash_skiplist_rep.cc util/histogram.cc util/instrumented_mutex.cc util/iostats_context.cc utilities/backupable/backupable_db.cc utilities/convenience/info_log_finder.cc utilities/checkpoint/checkpoint.cc utilities/compaction_filters/remove_emptyvalue_compactionfilter.cc utilities/document/document_db.cc utilities/document/json_document_builder.cc utilities/document/json_document.cc utilities/flashcache/flashcache.cc utilities/geodb/geodb_impl.cc utilities/leveldb_options/leveldb_options.cc utilities/merge_operators/put.cc utilities/merge_operators/string_append/stringappend2.cc utilities/merge_operators/string_append/stringappend.cc utilities/merge_operators/uint64add.cc utilities/redis/redis_lists.cc utilities/spatialdb/spatial_db.cc utilities/table_properties_collectors/compact_on_deletion_collector.cc utilities/transactions/optimistic_transaction_impl.cc utilities/transactions/optimistic_transaction_db_impl.cc utilities/transactions/transaction_base.cc utilities/transactions/transaction_db_impl.cc utilities/transactions/transaction_db_mutex_impl.cc utilities/transactions/transaction_lock_mgr.cc utilities/transactions/transaction_impl.cc utilities/transactions/transaction_util.cc utilities/ttl/db_ttl_impl.cc utilities/write_batch_with_index/write_batch_with_index.cc utilities/write_batch_with_index/write_batch_with_index_internal.cc util/event_logger.cc util/log_buffer.cc util/logging.cc util/memenv.cc util/murmurhash.cc util/mutable_cf_options.cc util/options_builder.cc util/options.cc util/options_helper.cc util/options_parser.cc util/perf_context.cc util/perf_level.cc util/rate_limiter.cc util/skiplistrep.cc util/slice.cc util/statistics.cc util/status.cc util/status_message.cc util/string_util.cc util/sync_point.cc util/thread_local.cc util/thread_status_impl.cc util/thread_status_updater.cc util/thread_status_updater_debug.cc util/thread_status_util.cc util/thread_status_util_debug.cc util/vectorrep.cc util/xfunc.cc util/xxhash.cc  \\\n     -lpthread -lrt -lsnappy -lgflags -lz -lbz2 -o librocksdb.so.4.1.0\ndb/builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/c.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/column_family.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/compacted_db_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ndb/compaction.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/compaction_iterator.cc:1:0: error: bad value (zEC12) for -march= switch\n // Use of this source code is governed by a BSD-style license that can be\n ^\ndb/compaction_job.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/compaction_picker.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/convenience.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/db_filesnapshot.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/dbformat.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/db_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/db_impl_debug.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/db_impl_readonly.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/db_impl_experimental.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/db_iter.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/experimental.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ndb/event_helpers.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/file_indexer.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/filename.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/flush_job.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/flush_scheduler.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/forward_iterator.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/internal_stats.cc:1:0: error: bad value (zEC12) for -march= switch\n //  This source code is licensed under the BSD-style license found in the\n ^\ndb/log_reader.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/log_writer.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/managed_iterator.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/memtable_allocator.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ndb/memtable.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/memtable_list.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/merge_helper.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/merge_operator.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/repair.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/slice.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\ndb/snapshot_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\ndb/table_cache.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/table_properties_collector.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/transaction_log_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/version_builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/version_edit.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/version_set.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/wal_manager.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/write_batch.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/write_batch_base.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\ndb/write_controller.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ndb/write_thread.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nport/stack_trace.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nport/port_posix.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/adaptive_table_factory.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n ^\ntable/block_based_filter_block.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/block_based_table_builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/block_based_table_factory.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/block_based_table_reader.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/block_builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/block.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/block_hash_index.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2013, Facebook, Inc. All rights reserved.\n ^\ntable/block_prefix_index.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2014, Facebook, Inc. All rights reserved.\n ^\ntable/bloom_block.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ntable/cuckoo_table_builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ntable/cuckoo_table_factory.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2014, Facebook, Inc. All rights reserved.\n ^\ntable/cuckoo_table_reader.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ntable/flush_block_policy.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/format.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/full_filter_block.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ntable/get_context.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ntable/iterator.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/merger.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/meta_blocks.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/sst_file_writer.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\ntable/plain_table_builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/plain_table_factory.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n ^\ntable/plain_table_index.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\ntable/plain_table_key_coding.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/plain_table_reader.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n ^\ntable/table_properties.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntable/two_level_iterator.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\ntools/dump/db_dump_tool.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/arena.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/auto_roll_logger.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/bloom.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/build_version.cc:1:0: error: bad value (zEC12) for -march= switch\n #include \"build_version.h\"\n ^\nutil/cache.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/coding.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/comparator.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/compaction_job_stats_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/crc32c.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/db_info_dumper.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/delete_scheduler_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutil/dynamic_bloom.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2013, Facebook, Inc. All rights reserved.\n ^\nutil/env.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/env_hdfs.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/env_posix.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/file_util.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/file_reader_writer.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/filter_policy.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/hash.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/hash_cuckoo_rep.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/hash_linklist_rep.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/hash_skiplist_rep.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/histogram.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/instrumented_mutex.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutil/iostats_context.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutilities/backupable/backupable_db.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/convenience/info_log_finder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/checkpoint/checkpoint.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/compaction_filters/remove_emptyvalue_compactionfilter.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/document/document_db.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/document/json_document_builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/document/json_document.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/flashcache/flashcache.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutilities/geodb/geodb_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/leveldb_options/leveldb_options.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutilities/merge_operators/put.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/merge_operators/string_append/stringappend2.cc:1:0: error: bad value (zEC12) for -march= switch\n /**\n ^\nutilities/merge_operators/string_append/stringappend.cc:1:0: error: bad value (zEC12) for -march= switch\n /**\n ^\nutilities/merge_operators/uint64add.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/redis/redis_lists.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright 2013 Facebook\n ^\nutilities/spatialdb/spatial_db.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/table_properties_collectors/compact_on_deletion_collector.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/optimistic_transaction_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/optimistic_transaction_db_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/transaction_base.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/transaction_db_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/transaction_db_mutex_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/transaction_lock_mgr.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/transaction_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/transactions/transaction_util.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutilities/ttl/db_ttl_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n ^\nutilities/write_batch_with_index/write_batch_with_index.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutilities/write_batch_with_index/write_batch_with_index_internal.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutil/event_logger.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/log_buffer.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/logging.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/memenv.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n ^\nutil/murmurhash.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/mutable_cf_options.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/options_builder.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/options.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/options_helper.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/options_parser.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/perf_context.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/perf_level.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/rate_limiter.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/skiplistrep.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/slice.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/statistics.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/status.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/status_message.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2015, Facebook, Inc.  All rights reserved.\n ^\nutil/string_util.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/sync_point.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/thread_local.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/thread_status_impl.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/thread_status_updater.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/thread_status_updater_debug.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/thread_status_util.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/thread_status_util_debug.cc:1:0: error: bad value (zEC12) for -march= switch\n // Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/vectorrep.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2013, Facebook, Inc.  All rights reserved.\n ^\nutil/xfunc.cc:1:0: error: bad value (zEC12) for -march= switch\n //  Copyright (c) 2014, Facebook, Inc.  All rights reserved.\n ^\nutil/xxhash.cc:1:0: error: bad value (zEC12) for -march= switch\n /*\n ^\nmake: **\\* [librocksdb.so.4.1.0] Error 1\n\nEnvironment \n\nRHEL - 7\n\ngcc --version\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nIs there any way to resolve this. Thanks in advance :)\n",
	"number": 1199,
	"title": "make_shared lib is not successful on RHEL 7"
}, {
	"body": "We're using the ruby API set to store a large set of k,v pairs to databases.  Inadvertently, I get into a situation where the k,v pairs can't be retrieved using the get API.  However, running an iterator on the database can see the values.  Iterating over the values, and invoking a put doesn't address this either.  Is there a flush API in ruby that can help address this?  Or is there some parameter that can be set, that can address this issue?  The default settings for SST, logs, bufs, etc. are being used.  \n",
	"number": 1198,
	"title": "rocksdb doesn't sync to disk"
}, {
	"body": "I saw that rocksdb supports Android and iOS but I didn't find any document for building/integrating it for/into those platforms. So if I'm correct (that it indeed supports those platforms), could you guys give some time to write some document for it ? Such as:\n- Common use cases for using Rocksdb on those platforms (when should use, when should not use, ...).\n- How to build it for Android/iOS.\n- Specific build options for those platforms.\n- Good practices when wrapping/using it on those platforms.\n\nThank you.\n",
	"number": 1193,
	"title": "Is there any document for building/integrating rocksdb for/into android/ios app ?"
}, {
	"body": "(gdb) bt\n#0  0x00007f58ee798425 in raise () from /lib/x86_64-linux-gnu/libc.so.6\n#1  0x00007f58ee79bb8b in abort () from /lib/x86_64-linux-gnu/libc.so.6\n#2  0x0000000001237fbd in __gnu_cxx::__verbose_terminate_handler() () at ../../../../gcc-4.9.3/libstdc++-v3/libsupc++/vterminate.cc:95\n#3  0x00000000011dff96 in __cxxabiv1::__terminate(void (*)()) () at ../../../../gcc-4.9.3/libstdc++-v3/libsupc++/eh_terminate.cc:47\n#4  0x00000000011dffe1 in std::terminate() () at ../../../../gcc-4.9.3/libstdc++-v3/libsupc++/eh_terminate.cc:57\n#5  0x00000000011ded2f in __cxa_pure_virtual () at ../../../../gcc-4.9.3/libstdc++-v3/libsupc++/pure.cc:50\n#6  0x00000000010a79bc in rocksdb::BlockPrefixIndex::GetBlocks(rocksdb::Slice const&, unsigned int**) () at table/block_prefix_index.cc:215\n#7  0x00000000010a62b3 in rocksdb::BlockIter::PrefixSeek(rocksdb::Slice const&, unsigned int*) () at table/block.cc:294\n#8  0x00000000010a6377 in rocksdb::BlockIter::Seek(rocksdb::Slice const&) () at table/block.cc:93\n#9  0x00000000010a185f in rocksdb::BlockBasedTable::Get(rocksdb::ReadOptions const&, rocksdb::Slice const&, rocksdb::GetContext*, bool) ()\n\n```\nat table/block_based_table_reader.cc:1235\n```\n#10 0x000000000115f107 in rocksdb::TableCache::Get(rocksdb::ReadOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, rocksdb::Slice const&, rocksdb::GetContext_, rocksdb::HistogramImpl_, bool) () at db/table_cache.cc:271\n#11 0x0000000001169c54 in rocksdb::Version::Get(rocksdb::ReadOptions const&, rocksdb::LookupKey const&, std::string_, rocksdb::Status_, rocksdb::MergeContext_, bool_, bool_, unsigned long_) () at db/version_set.cc:903\n#12 0x000000000111094b in rocksdb::DBImpl::GetImpl(rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle_, rocksdb::Slice const&, std::string_, bool*) ()\n\n```\nat db/db_impl.cc:3289\n```\n#13 0x0000000001110af9 in rocksdb::DBImpl::Get(rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle_, rocksdb::Slice const&, std::string_) () at db/db_impl.cc:3197\n#14 0x00000000010d2fc5 in rocksdb::DBWithTTLImpl::Get(rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle_, rocksdb::Slice const&, std::string_) ()\n\n```\nat utilities/ttl/db_ttl_impl.cc:195\n```\n\nany body met this ?\n",
	"number": 1186,
	"title": "rocksdb::BlockPrefixIndex::GetBlocks error"
}, {
	"body": "Checked exceptions on every database operation are unwieldy (especially in Java 8 lambdas) and are usually handled in a higher layer so you tend to propagate them instead of handling locally.\n\nHence the request is to make RocksDBException an un-checked exception to make the API usage more convenient.\n",
	"number": 1185,
	"title": "RocksJava: make RocksDBException a RuntimeException"
}, {
	"body": "Hello,\n\nI'm using rocksdb as a dll library on Windows. One of the users receive the following stable error:\n\n> 2016/06/16-16:18:08.905453 2a00 [WARN] Compaction error: IO error: Failed to FlushViewOfFile: D:\\Program Files\\Aguarium 2016\\Data\\ATrees\\IDeA\\tree__$mm.ldb_data/000057.sst: The process cannot access the file because another process has locked a portion of the file.\n> \n> 2016/06/16-16:18:08.905453 2a00 [ERROR] Waiting after background compaction error: IO error: Failed to FlushViewOfFile: D:\\Program Files\\Aguarium 2016\\Data\\ATrees\\IDeA\\tree__$mm.ldb_data/000057.sst: The process cannot access the file because another process has locked a portion of the file.\n> , Accumulated background error counts: 1\n> \n> 2016/06/16-16:18:09.908511 2a00 [ERROR] Waiting after background compaction error: IO error: Failed to FlushViewOfFile: D:\\Program Files\\Aguarium 2016\\Data\\ATrees\\IDeA\\tree__$mm.ldb_data/000057.sst: The process cannot access the file because another process has locked a portion of the file.\n> , Accumulated background error counts: 2\n\nI've set `allow_mmap_writes` to true, because it gives 150% boost for me. So this error happens in `WinMmapFile` class.\n\nI've solved this error by re-trying `FlushViewOfFile` after some time. So I changed \n\n```\n      // Flush only the amount of that is a multiple of pages\n      if (!::FlushViewOfFile(mapped_begin_ + page_begin,\n                             (page_end - page_begin) + page_size_)) {\n        s = IOErrorFromWindowsError(\"Failed to FlushViewOfFile: \" + filename_,\n                                  GetLastError());\n      }\n```\n\nto\n\n```\n      bool abort = false;\n      int attempts = 0;\n      while (!abort) {\n          // Flush only the amount of that is a multiple of pages\n          bool res = ::FlushViewOfFile(mapped_begin_ + page_begin, (page_end - page_begin) + page_size_);\n          if (!res) {\n              Sleep(file_errors_delay_milliseconds);\n          }\n          abort = ++attempts > file_errors_retry_count || res; // first one is not counted as attempt\n      }\n      if (attempts > file_errors_retry_count) {\n          s = IOErrorFromWindowsError(\"Failed to FlushViewOfFile: \" + filename_, GetLastError());\n      }\n```\n\nwith `file_errors_retry_count` as 10 and `file_errors_delay_milliseconds` as 500.\n\nI understand that the solution is a bit dirty, but hope it will help you to catch the origins of this problem. Maybe it was caused by some synchronization problems.\n\nAlso I've thought about anti-virus software, but user close the anti-virus and the problem wasn't gone.\n",
	"number": 1173,
	"title": "IO error in FlushViewOfFile on Windows"
}, {
	"body": "`RocksDB#keyMayExist` in the Java API does not allow control over whether the value should be returned or not, i.e. there is no control over the initial value of `value_found`.\n",
	"number": 1171,
	"title": "`keyMayExist` in the Java API incomplete"
}, {
	"body": "Hi,\n\nHow to delete the complete data from Rocksdb without flushing out onto the disk.\nI was trying to close the db connection but that is flushing out the data on disk.\n\nThanks\n",
	"number": 1170,
	"title": "How to delete in memory data without flushing out to disk"
}, {
	"body": "If we look for example at `DBOptions` then we see that there are several options in the C++ API of RocksDB that use `uint64_t` as their type. Unfortunately in RocksJava the `DBOptionsInterface` presents these to the user as a Java `long` which is a two's complement signed 64 bit integer.\n\nSadly Java has no native support for unsigned integers. However as Java's `long` is 64 bits we can go back and forwards between Java `long` and C++ `uint64_t` without any loss of precision; the issue is that this looks rather ugly to RocksJava users as any value greater than `INT64_MAX` will be wrapped and become a negative value `long` in Java.\n\nFor example, if we examine `DBOptions::bytes_per_sync`, then when reading this value from RocksJava via `DBOptions#bytesPerSync()` we have the following underlying code performing the conversion from `uint64_t` to `jlong`:\n\n``` C++\njlong Java_org_rocksdb_Options_bytesPerSync(\n    JNIEnv* env, jobject jobj, jlong jhandle) {\n  return reinterpret_cast<rocksdb::Options*>(jhandle)->bytes_per_sync;\n}\n```\n1. The problem here is that any value between `INT64_MAX` but within the bound of `UINT64_MAX` will appear to the Java user as a negative value, whereas most likely they would have expected a positive value.\n\nWhen writing this value from RocksJava via `DBOptions#setBytesPerSync(long)` we have the following underlying code performing the conversion from `jlong` to `uint64_t`:\n\n``` C++\nvoid Java_org_rocksdb_Options_setBytesPerSync(\n    JNIEnv* env, jobject jobj, jlong jhandle, jlong bytes_per_sync) {\n  reinterpret_cast<rocksdb::Options*>(jhandle)->bytes_per_sync =\n      static_cast<int64_t>(bytes_per_sync);\n}\n```\n1. The problem here is that the Java user cannot set a positive value for `bytesPerSync` greater than `INT64_MAX` without understanding that they actually need to set the Java `long` to the equivalent negative value.\n\nIn reality there is not a clean solution to this, perhaps the best option we could hope for would be to add our own `UInt64` Java class that wraps a Java `long` or `byte[]`, and allows construction from a Java `String` or `long`, and serialization. This would at least make the Java interface explicitly express that this is a `uint64_t` and show a positive value in Java on the `toString()`.\n\n@siying @yhchiang Thoughts?\n",
	"number": 1166,
	"title": "Problems with uint64_t in RocksJava"
}, {
	"body": "when the file deletions are disabled, this interface returns value that > 0, and if the file deletions are enabled, it returns value that = 0.\nI think it should be reversed.\n",
	"number": 1165,
	"title": "the interface DBImpl::IsFileDeletionsEnabled() has opposite meaning"
}, {
	"body": "We have a workload(Ceph bluestore on SSDs), whole db size is about 0.5GB to 4GB per 400GB drive, uncompressed and constantly being updated(at a rate about 5K per second, but we want more). And we don't want rocksdb to share page cache with others, and want to configure rocksdb to use fixed amount of memory, so tried to use block cache to replace page cache. db read is good, slightly better than page cache; but db updates suffers, tps with block cache dropped by half compared to page cache, because block cache doesn't cache sst file writes then compaction will have read blocks from drive again. Can rocksdb have an option to enable block cache to cache all or partial writes? \n",
	"number": 1164,
	"title": "Add an option for block cache to cache writes"
}, {
	"body": "Hi,\n\nI am not able to find the api in java for prefix seek functionality in 4.5.1 version of RocksDB.\nCould anyone help me in confirming about this, would really appreciate that.\n\nRegards,\nShashank Jain\n",
	"number": 1159,
	"title": "Does prefix seek api avaliable for java in 4.5.1 version"
}, {
	"body": "There are three sets of mechanisms for gathering statistics over Rocksdb\n\n1) Per DB Instance counters: Tickers and Histograms exposed through statistics.h interface \n    [https://github.com/facebook/rocksdb/blob/master/include/rocksdb/statistics.h#L23](url)\n    [https://github.com/facebook/rocksdb/blob/master/include/rocksdb/statistics.h#L286](url)\n2) Per Column family counters: Internal statistics exposed through db->GetProperty() interface\n    [https://github.com/facebook/rocksdb/blob/master/include/rocksdb/db.h#L335](url)\n3) Per Thread counters: Perf and IO related numbers are exposed in perf_context.h and iostat_context.h\n    [https://github.com/facebook/rocksdb/blob/master/include/rocksdb/perf_context.h#L20](url)\n    [https://github.com/facebook/rocksdb/blob/master/include/rocksdb/iostats_context.h](url)\n\nThere are overlaps of and duplicated of counters in these three interfaces and the definitions of some counters are obscure.\n\nI am proposing to unify them and expose them in a single interface. The best candidate for unification would be Statistics class. \n1) Statistics class already has the extent-ability for adding more counters\n`enum TickersInternal : uint32_t {`\n`INTERNAL_TICKER_ENUM_START = TICKER_ENUM_MAX,`\n`INTERNAL_TICKER_ENUM_MAX`\n`};`\n\n`enum HistogramsInternal : uint32_t {`\n`INTERNAL_HISTOGRAM_START = HISTOGRAM_ENUM_MAX,`\n`INTERNAL_HISTOGRAM_ENUM_MAX`\n`};`\n\n2) To expose per-column family counters: two overriding interface function can be added\n`uint64_t getTickerCount(ColumnFamilyHandle* column_family, uint32_t tickerType) const;` \n`uint64_t histogramData(ColumnFamilyHandle* column_family, uint32_t type, HistogramData* const data) const;`\n\n3) From the user monitoring perspective, there is actually little reason for having per-thread counters. They should be presented in aggregated manor. \n\nThoughts? @siying @yhchiang @igorcanadi @IslamAbdelRahman\n",
	"number": 1158,
	"title": "Unifying PerfContext, IOStatContext, DB::Properties and Statistics"
}, {
	"body": "Hi, \n\nI am assuming that these two classes are missing in rocksdbjni.\nIt would be great if I can achieve atomicity of my transaction using either TransactionDB or OptimisticTransactionDB.\n\nI request to add these two classes to rocksdbjni.\nIt would be great if you have any planned release day for the new rocksdbjni with these (or one of these) classes.\n\nRocksDB rocks!!\n",
	"number": 1151,
	"title": "Request to add TransactionDB and OptimisticTransactionDB to rockdbjni"
}, {
	"body": "Is there any APIs available s.t. we can tell what's the current expiration time associated w/ a record in RocksDB database w/ TTL?\n\nThe use case is: since the TTL RocksDB expires records w/ background compaction threads, there may be time that get() or iterator.next() will get records that should be expired. For application that can not run correct logic w/ stale records, the application will need to tell whether the record it gets from RocksDB has actually expired at the moment of access. Is there anyway to expose this internally kept expiration timestamp via the get or iterator APIs?\n\nThanks!\n",
	"number": 1143,
	"title": "Any TTL timestamp per record available via get() API?"
}, {
	"body": "Hi,\n\nWe are using the C-API in our current work with RocksDB. I have a question about the following limitation advertised here,\n\nhttps://github.com/facebook/rocksdb/blob/master/include/rocksdb/c.h\n\nDoes not support:\n...\n  . capturing post-write-snapshot\n\nCan you guys be more explicit about what this means? What we would like to do is the following:\n- Thread 1 creates snapshot and iterates over snapshot\n- Thread 2 does writes after Thread 1 has created the snapshot but before Thread 1 releases the snapshot.\n- Thread 3 (different from Thread 1 and Thread 2) does reads after Thread 1 has created the snapshot but before Thread 1 releases the snapshot.\n\nThe behavior we would like to have is that both Thread 2 and Thread 2 are able to read the updates created by Thread 2 after Thread 1 creates the snapshot. We are fine with (in fact we expect that) thread 1 doesn't see the updates created by Thread 2.\n\nIs this is allowed with the current C-API, then I would like to understand what the limitation explained in the C API involves.\n\nThanks,\n\nEthan.\n",
	"number": 1141,
	"title": "Question about C-API limits"
}, {
	"body": "Hello and thank you for RocksDB,\n\nWhen using `ldb load`, `getline` on `cin` can use a lot of user CPU when lines are long.\n\nI noticed `ldb` using a lot of user CPU when loading large values:\n\n```\nperl -e 'while($i<1000){++$i;printf(qq{$i ==> }.(q{x} x 1_000_000).{\\n},$i)}' | \n    time ldb --create_if_missing --db=/dev/shm/test.rdb load\n  61.85 user\n   1.61 system\n1:03.42 elapsed\n        100%CPU\n```\n\n`perf record -g` showed the CPU time was spent in `_IO_getc`\n\n```\n Children      Self  Command  Shared Object        Symbol\n   54.86%    54.77%  ldb      libc-2.17.so         [.] _IO_getc\n   28.57%    28.53%  ldb      libc-2.17.so         [.] _IO_ungetc\n    5.78%     5.46%  ldb      libstdc++.so.6.0.19  [.] std::getline<char, std::char_traits<char>, std::allocator<char> >\n    2.62%     0.00%  ldb      ldb                  [.] rocksdb::LDBTool::Run\n    2.62%     0.00%  ldb      ldb                  [.] rocksdb::LDBCommandRunner::RunCommand\n    2.62%     0.00%  ldb      ldb                  [.] rocksdb::LDBCommand::Run\n```\n\nThe `getline` on `cin` code in `tools/ldb_cmd.cc::DBLoaderCommand::DoCommand` can be inefficient, to demonstrate:\n\n`getline.cc`\n\n```\n#include <iostream>\n#include <string>\nusing namespace std;\nint main(){\n  string line;\n  int num_lines;\n  while (getline(cin, line, '\\n')) {\n    ++num_lines;\n  }\n  cout << \"getline num_lines = \" << num_lines << endl;\n}\n```\n\n`getline_posix.cc`\n\n```\n#include <iostream>\n#include <stdlib.h>\n#include <cstdio>\nusing namespace std;\nint main(){\n  char *line = NULL;\n  int num_lines;\n  size_t len = 0;\n  ssize_t read;\n  while ((read = getline(&line, &len, stdin)) != -1) {\n    ++num_lines;\n  }\n  free(line);\n  cout << \"getline_posix num_lines = \" << num_lines << endl;\n}\n```\n\nCompile and run:\n\n```\n> g++ getline.cc -o getline \n> perl -e '$line=q{x} x 2048 . qq{\\n};while($i<1_000_000){print $line;++$i}' | \n    time ./getline\ngetline num_lines = 1000000\n  58.43 user \n   0.43 system \n0:58.81 elapsed\n\n> g++ getline_posix.cc -o getline_posix  \n> perl -e '$line=q{x} x 2048 . qq{\\n};while($i<1_000_000){print $line;++$i}' | \n    time ./getline_posix\ngetline_posix num_lines = 1000000\n   0.18 user\n   0.45 system\n0:00.63 elapsed\n```\n\nThis is with:\n\n```\n> g++ --version\ng++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\n> rpm -q glibc\nglibc-2.17-106.el7_2.4.x86_64\n> uname -a\nLinux HOSTNAME 3.10.0-229.el7.x86_64 #1 SMP Thu Jan 29 18:37:38 EST 2015 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nA quick test using the head of the master RocksDB branch, building with `make CFLAGS=-DSNAPPY=1`\n\n```\n> diff -du6 tools/ldb_cmd.cc.orig tools/ldb_cmd.cc\n--- tools/ldb_cmd.cc.orig       2016-05-23 11:26:39.203254542 -0400\n+++ tools/ldb_cmd.cc    2016-05-23 13:14:16.560139818 -0400\n@@ -801,26 +801,31 @@\n   if (disable_wal_) {\n     write_options.disableWAL = true;\n   }\n\n   int bad_lines = 0;\n   std::string line;\n-  while (getline(std::cin, line, '\\n')) {\n+  char *c_line = NULL;\n+  size_t len = 0;\n+  ssize_t read;\n+  while ((read = getline(&c_line, &len, stdin)) != -1) {\n+    line = std::string(c_line);\n+    line.pop_back();\n     std::string key;\n     std::string value;\n     if (ParseKeyValue(line, &key, &value, is_key_hex_, is_value_hex_)) {\n       db_->Put(write_options, GetCfHandle(), Slice(key), Slice(value));\n     } else if (0 == line.find(\"Keys in range:\")) {\n       // ignore this line\n     } else if (0 == line.find(\"Created bg thread 0x\")) {\n       // ignore this line\n     } else {\n       bad_lines ++;\n     }\n   }\n-\n+  free(c_line);\n   if (bad_lines > 0) {\n     std::cout << \"Warning: \" << bad_lines << \" bad lines ignored.\" << std::endl;\n   }\n   if (compact_) {\n     db_->CompactRange(CompactRangeOptions(), GetCfHandle(), nullptr, nullptr);\n   }\n```\n\nShows the reduction in user CPU (in debug mode), for this example down from just over 1 minute to 1 second.\n\n```\n> perl -e 'while($i<1000){++$i;printf(qq{$i ==> }.(q{x} x 1_000_000).{\\n},$i)}' | \n    time ldb --create_if_missing --db=/dev/shm/test.rdb load\n   1.17 user\n   1.70 system\n0:03.79 elapsed\n        76%CPU\n```\n\nAt the moment I would just like to report this as an issue, and not work on a portable efficient replacement.  I do not have a CLA in place.\n\nThanks.\n",
	"number": 1133,
	"title": "When using ldb load, getline on cin can use a lot of user CPU when lines are long."
}, {
	"body": "I do not want to use:  count++ \u3002Is there a function?  Sorry, my English is not well, thank you for your help.\n",
	"number": 1129,
	"title": "How to calculate the number of keys which is valid   in one column family\uff1f"
}, {
	"body": "I am using rocksdb on embedded system \nLinux tegra-ubuntu 3.10.67-g458d45c #1 SMP PREEMPT Mon Feb 8 17:44:18 PST 2016 aarch64 aarch64 aarch64 GNU/Linux\n\nI have received following error, is this compiler related ?\n\n```\n nvcc --std=c++11 TeraCore.cpp \nIn file included from /usr/include/rocksdb/immutable_options.h:10:0,\n                 from /usr/include/rocksdb/db.h:18,\n                 from TeraCore.cpp:3:\n/usr/include/rocksdb/options.h:69:32: error: enumerator value -1 is too large for underlying type \u2018char\u2019\n   kDisableCompressionOption = -1,\n```\n",
	"number": 1122,
	"title": "Nvcc complier "
}, {
	"body": "## Reproduction Environment\n\nThe issue was reproduced with the version : 4.1.0, 4.5.1\nMacBook Pro(Retina, 15-inch, Mid 2014)\nOS X El Capitan\nVersion 10.11.1\n## Reproduction Steps\n### Step 1 : Install sbt and scala.\n\n```\nbrew install sbt\nbrew install scala\n```\n### Step 2 : Clone scalechain.\n\n```\ngit clone https://github.com/ScaleChain/scalechain\n```\n### Step 3 : Switch branch to proto-stream\n\n```\ncd scalechain\ngit checkout proto-stream\n```\n### Step 4 : run unit tests.\n\n```\nsbt clean test\n```\n## Crash dump1\n\n```\n#\n# A fatal error has been detected by the Java Runtime Environment:\n#\n#  SIGSEGV (0xb) at pc=0x000000012711c569, pid=4655, tid=26635\n#\n# JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14)\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops)\n# Problematic frame:\n# C  [librocksdbjni2800608398578694726.jnilib+0xef569]  _ZN7rocksdb11MemTableRep3GetERKNS_9LookupKeyEPvPFbS4_PKcE+0xc99\n#\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n#\n# An error report file with more information is saved as:\n# /Users/kangmo/crypto/scalechain/hs_err_pid4655.log\n#\n# If you would like to submit a bug report, please visit:\n#   http://bugreport.java.com/bugreport/crash.jsp\n# The crash happened outside the Java Virtual Machine in native code.\n# See problematic frame for where to report the bug.\n#\n```\n## crash dump2\n\n```\n#\n# A fatal error has been detected by the Java Runtime Environment:\n#\n#  SIGSEGV (0xb) at pc=0x0000000127a10569, pid=4470, tid=5891\n#\n# JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14)\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops)\n# Problematic frame:\n# C  [librocksdbjni4279469314607088486.jnilib+0xef569]  rocksdb::MemTableRep::Get(rocksdb::LookupKey const&, void*, bool (*)(void*, char const*))+0xc99\n#\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n#\n# An error report file with more information is saved as:\n# /Users/kangmo/crypto/scalechain/hs_err_pid4470.log\n#\n# If you would like to submit a bug report, please visit:\n#   http://bugreport.java.com/bugreport/crash.jsp\n# The crash happened outside the Java Virtual Machine in native code.\n# See problematic frame for where to report the bug.\n#\n\n```\n",
	"number": 1121,
	"title": "rocksdbjni crashes with RocksIterator.seek, seekToFirst"
}, {
	"body": "Hi guys,\n\nWe am very interested in this feature,\n\nhttps://github.com/facebook/rocksdb/blob/master/include/rocksdb/sst_file_manager.h\n\nI see the C-API hasn't been updated to reflect it however. Basically we want to be able to limit the resources used by RocksDB. In our current thinking that comes down to:\n- Limiting memory. This can be done with the read cache size for workloads with data in the Gb range and bigger.\n- Limiting disk space. This is where SstFileTracker would be great.\n\nThanks,\n\nEthan.\n",
	"number": 1115,
	"title": "I didn't see SstFileTracker exposed through the C API"
}, {
	"body": "Hi all --\n\nSome users are requesting rocksdb packages for RHEL-based operating systems.  I found build_tools/make_packages.sh script but was not able to find any of the build artifacts in any external repos.  So I wonder if you are interested in getting rocksdb packaged for Fedora, CentOS (EPEL), etc?\n",
	"number": 1114,
	"title": "rocksdb packages for RHEL-based operating systems"
}, {
	"body": "I am wondering what the most performant approach to achieving read-your-own-writes semantics via a WBWI (`WriteBatchWithIndex`) is?\n\nI can see 4 possible approaches:\n1. Take a `WBWIIterator` on the WBWI by calling `NewIterator`, seek, and if no item is found then fallback to `db->Get`. This was described by @siying in http://rocksdb.org/blog/1901/write-batch-with-index/ and is used in MongoRocks - https://github.com/mongodb-partners/mongo-rocks/blob/master/src/rocks_recovery_unit.cpp#L387\n2. Take a database `Iterator`, and then take an `Iterator` on the WBWI by calling `NewIteratorWithBase` and passing in the database `Iterator`, and then seek.\n3. Call `GetFromBatch` on the WBWI, and if no item is found then fallback to `db->Get`.\n4. Call `GetFromBatchAndDB` on the WBWI.\n\nPersonally (4) looks to be the easiest option, and I would guess has the most potential for future internal optimization in Rocks?\n",
	"number": 1111,
	"title": "Fastest approach to Read-your-own-writes"
}, {
	"body": "For the background story - I am trying to work with `WBWI` (`WriteBatchWithIndex`) and merge operations. I have a need to be able to see the `merged` value of a key in the `WBWI` after I have performed the merge operation, e.g.\n\n``` C++\ndb->Put(key, val)\n\n...\n\nwbwi->Merge(key, merge_val);\n\nauto dbIt = db->NewIterator();\nauto it = wbwi->NewIteratorWithBase(dbIt);\n\nfor (it.Seek(keyPrefix); it.isValid() && memcmp(iterator.key(), keyPrefix) > 0; it.Next()) {\n  // unable to read the merged value here\n  it.Value();\n}\n```\n\nIt seems that `WBWIterator` does not respect the merge semantics, which I really need and would have expected it to do intrinsically.\n\nSo... I thought I would try and understand `WBWI::GetFromBatch` and `WBWI::GetFromBatchAndDB` to see if I could use those as an alternative.\n\nHowever, I am confused around the `rocksdb::Status` meaning of `kMergeInProgress` for those functions.\n\nFor example,in  `utilities/write_batch_with_index/write_batch_with_index_test.cc` when looking in  at the test `TestGetFromBatchMerge`, I don't understand why the status for `x` after calling `GetFromBatch` is `OK`:\n\n``` C++\nbatch.Put(\"x\", \"X\");\n...\nbatch.Merge(\"x\", ToString(i));\n...\ns = batch.GetFromBatch(column_family, options, \"x\", &value);\nASSERT_OK(s);\n```\n\nYet, when compared to `TestGetFromBatchMerge2`, after calling `Merge` the status is `kMergeInProgress`, e.g.\n\n``` C++\nbatch.Put(column_family, \"X\", \"x\");\ns = batch.GetFromBatch(column_family, options, \"X\", &value);\nASSERT_OK(s);\nASSERT_EQ(\"x\", value);\n\nbatch.Put(column_family, \"X\", \"x2\");\ns = batch.GetFromBatch(column_family, options, \"X\", &value);\nASSERT_OK(s);\nASSERT_EQ(\"x2\", value);\n\nbatch.Merge(column_family, \"X\", \"aaa\");\ns = batch.GetFromBatch(column_family, options, \"X\", &value);\nASSERT_TRUE(s.IsMergeInProgress());\n```\n\nThe code comments in `write_batch_with_index.h` are a bit vague. I am wondering under what circumstances when calling `GetFromBatch` (and `GetFromBatchAndDB`) will the status be `kMergeInProgress`?\n",
	"number": 1100,
	"title": "Working with WriteBatchWithIndex and merge"
}, {
	"body": "I describe my requirement as follows:\nI have two client of database, they are in two different processes,one has write and read request of database called A,another has only read request of database called B. Now A writes into database without a stop. I want that B can read the real time updates of database written by A.\nI see that rocksdb has an api  OpenForReadOnly. But it seems that B can only read the snapshot when B uses OpenForReadOnly. I don't know how can B reads the updates written by A after OpenForReadOnly. \n\nThanks\n",
	"number": 1097,
	"title": "Hi,I have some questions about a db api of OpenForReadOnly"
}, {
	"body": "The code below will segfault.\n\nThe expectation is that a call to rocksdb_slicetransform_create_fixed_prefix() should be paired with a call to rocksdb_slicetransform_destroy().\n\nHowever, once the rocksdb_slicetransform_t is associated with an rocksdb_options_t, it gets destroyed when the rocksdb_options_t is destroyed, and so if you call rocksdb_slicetransform_destroy() it segfaults.\n\nThis makes language bindings exceedingly clumsy as you have to keep track of whether the rocksdb_slicetransform_t has been associated with a rocksdb_options_t somewhere.\n\nIs this desired behavior?\n\n``` c\n/*\n The code below segfaults.\n Lines marked (B) and (C) appear to be mutually exclusive.  Is this the desired behavior?\n*/\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#include <assert.h>\n#include \"rocksdb/c.h\"\nint main(int argc, char **argv) {\n  rocksdb_slicetransform_t *prefix = rocksdb_slicetransform_create_fixed_prefix(8);  // <-- (A) Create Prefix Extractor\n  rocksdb_options_t *options = rocksdb_options_create();\n  rocksdb_options_set_create_if_missing(options, 1);\n  rocksdb_options_set_prefix_extractor(options, prefix); // <-- (B) Set Prefix Extractor\n  /* ... */\n  rocksdb_options_destroy(options);\n  rocksdb_slicetransform_destroy(prefix); // <-- (C) Destroy Prefix Extractor\n  return 0;\n}\n```\n",
	"number": 1095,
	"title": "(C API) Segfault when calling rocksdb_slicetransform_destroy() after rocksdb_options_set_prefix_extractor()"
}, {
	"body": "Hi,\n\nI am thinking to write an S3 environment for rocksdb, turned out we can leverage the AWS cpp sdk (https://github.com/aws/aws-sdk-cpp), but I wonder what's the best practice here to wind a 3rd party lib into rocks db?\n",
	"number": 1094,
	"title": "Dependency on AWS SDK"
}, {
	"body": "make -j4 shared_lib \n\nruns out of memory on my laptop\n",
	"number": 1091,
	"title": "[make] Compile cc files one at a time"
}, {
	"body": "I ran db_bench --benchmarks=fillseq and WAL sync disabled. iostat was running during the test and bytes written by iostat are about half the rate of bytes written by rocksdb.flush.write.bytes. This is true whether I use snappy or no compression.\n\nFor this example, iostat reports ~1gb of writes when compression is disabled because there are 1M keys and each value is 1024 bytes. But rocksdb.flush.write.bytes reports 2gb.\n\nThis is incremented in two places:\ndb/db_impl.cc:  RecordTick(stats_, FLUSH_WRITE_BYTES, IOSTATS(bytes_written));  \ndb/flush_job.cc:  RecordTick(stats_, FLUSH_WRITE_BYTES, meta->fd.GetFileSize());\n\nRepro script\n\n```\niostat -kx 5 >& o.io1 &\nipid=$!\n./db_bench --benchmarks=fillseq --compression_type=none --db=/s/bld/ldb --value_size=1024 --num=1000000 --write_buffer_size=$(( 4 * 1024 * 1024 )) --max_bytes_for_level_base=$(( 32 * 1024 * 1024 )) --statistics=1 --stats_per_interval=1 --stats_interval_seconds=10 >& o.db1\nkill $ipid\n```\n\nThen summarize iostat where $dname is storage device name from iostat\n\n```\ngrep $dname o.io1| awk '{ if (NR>1) { wkb += $7 } } END { printf \"%.2f\\n\", (wkb*5)/(1024*1024) }'\n```\n\nI then repeated the test with printfs in the two callers to RecordTick(..., FLUSH_WRITE_BYTES, ...) and it was called from flush_job.cc 262 times during the test with each reporting ~4M bytes for a total of ~1G. It was called from db_impl.cc 524 times and half of those calls are for ~4M bytes, while the other half are for 0 bytes. \n\nSo each of the callers reports 1gb of writes. There is double counting. My build has the fix for https://github.com/facebook/rocksdb/commit/ae21d71e94ebe82689aea39359c1661776871d52 and while the bytes reported by that fix are for flush (so it is correct to not report them as compaction bytes) that fix might not have considered the other caller.\n\nI also don't know why the other caller from db_impl.cc is called twice as frequently and half the time reports 0 bytes\n",
	"number": 1086,
	"title": "FLUSH_WRITE_BYTES is incorrect"
}, {
	"body": "When there are too many deleted keys, we are seeing that Seek/Next are taking too long to return. That is resulting in our threads to lock up for a potentially unbounded amount of time. There are techniques to mitigate this problem as documented in https://github.com/facebook/rocksdb/wiki/Delete-A-Range-Of-Keys, but we wanted a predictable upper bound on the time an iteration can take before returning.\n\nI am proposing a change where rocksdb will expose deleted keys through iteration. In this case, it is up to the user code to continuously call Next() until they find a non-deleted valid key. This will give our code the opportunity to periodically check the time and yield the thread if needed.\n\nBefore implementing, I also considered an alternative approach of passing a timeout to Seek/Next calls but that would have made the interface/implementation much more complicated.\n\nPTAL. If this change looks acceptable, I will look into implementing some tests in the next round.\n",
	"number": 1084,
	"title": "Expose deleted keys through iteration."
}, {
	"body": "I have a database with Rocks that I have been using. However when I start up now, it appears that my database is empty somehow.\n\nAt startup it now looks like the recovery process runs:\n\n``` bash\n2016-04-16 16:17:39,991 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Compression algorithms supported: \n2016-04-16 16:17:39,991 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   Snappy supported: 1 \n2016-04-16 16:17:39,991 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   Zlib supported: 1 \n2016-04-16 16:17:39,991 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   Bzip supported: 1 \n2016-04-16 16:17:39,992 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   LZ4 supported: 0 \n2016-04-16 16:17:39,992 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Fast CRC32 supported: 1 \n2016-04-16 16:17:39,992 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Recovering from manifest file: MANIFEST-000188\n\n2016-04-16 16:17:39,992 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [default]:\n\n2016-04-16 16:17:39,993 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [XML_DOM_STORE]:\n\n2016-04-16 16:17:39,993 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [SYMBOL_STORE]:\n\n2016-04-16 16:17:39,994 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [COLLECTION_STORE]:\n\n2016-04-16 16:17:39,994 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [NAME_ID_INDEX]:\n\n2016-04-16 16:17:39,994 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [NAME_INDEX]:\n\n2016-04-16 16:17:39,994 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [SORT_NAME_INDEX]:\n\n2016-04-16 16:17:39,995 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [SORT_INDEX]:\n\n2016-04-16 16:17:39,995 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [NGRAM_INDEX]:\n\n2016-04-16 16:17:39,995 [main] INFO  (RocksLoggerAdapter.java [log]:53) - --------------- Options for column family [METADATA_STORE]:\n\n2016-04-16 16:17:39,995 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   (skipping printing options)\n\n2016-04-16 16:17:39,996 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   (skipping printing options)\n\n2016-04-16 16:17:39,996 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   (skipping printing options)\n\n2016-04-16 16:17:39,996 [main] INFO  (RocksLoggerAdapter.java [log]:53) -   (skipping printing options)\n\n2016-04-16 16:17:39,997 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Recovered from manifest file:/Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/MANIFEST-000188 succeeded,manifest_file_number is 188, next_file_number is 210, last_sequence is 40955535, log_number is 0,prev_log_number is 0,max_column_family is 13\n\n2016-04-16 16:17:39,998 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [default] (ID 0), log number is 187\n\n2016-04-16 16:17:39,998 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [XML_DOM_STORE] (ID 1), log number is 187\n\n2016-04-16 16:17:39,998 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [SYMBOL_STORE] (ID 2), log number is 187\n\n2016-04-16 16:17:39,998 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [COLLECTION_STORE] (ID 3), log number is 187\n\n2016-04-16 16:17:39,998 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [NAME_ID_INDEX] (ID 4), log number is 187\n\n2016-04-16 16:17:39,999 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [NAME_INDEX] (ID 5), log number is 187\n\n2016-04-16 16:17:39,999 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [SORT_NAME_INDEX] (ID 6), log number is 187\n\n2016-04-16 16:17:39,999 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [SORT_INDEX] (ID 7), log number is 187\n\n2016-04-16 16:17:39,999 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [NGRAM_INDEX] (ID 8), log number is 187\n\n2016-04-16 16:17:39,999 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [METADATA_STORE] (ID 9), log number is 187\n\n2016-04-16 16:17:39,999 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [METADATA_INDEX] (ID 10), log number is 187\n\n2016-04-16 16:17:40,000 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [UUID_STORE] (ID 11), log number is 187\n\n2016-04-16 16:17:40,000 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [UUID_URI_INDEX] (ID 12), log number is 187\n\n2016-04-16 16:17:40,000 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Column family [URI_UUID_INDEX] (ID 13), log number is 187\n\n2016-04-16 16:17:40,001 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837860001011, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [199, 209]} \n2016-04-16 16:17:40,001 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Recovering log #199 mode 0 skip-recovery 0 \n2016-04-16 16:18:00,238 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [NAME_INDEX] [WriteLevel0TableForRecovery] Level-0 table #210: started \n2016-04-16 16:18:02,317 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [NAME_INDEX] [WriteLevel0TableForRecovery] Level-0 table #210: 66777884 bytes OK \n2016-04-16 16:18:02,317 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837882317688, \"cf_name\": \"NAME_INDEX\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 210, \"file_size\": 66777884, \"table_properties\": {\"data_size\": 66213551, \"index_size\": 628663, \"filter_size\": 0, \"raw_key_size\": 82480425, \"raw_average_key_size\": 22, \"raw_value_size\": 14732488, \"raw_average_value_size\": 4, \"num_data_blocks\": 16238, \"num_entries\": 3683122, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:02,413 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Recovering log #209 mode 0 skip-recovery 0 \n2016-04-16 16:18:22,234 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [NAME_INDEX] [WriteLevel0TableForRecovery] Level-0 table #211: started \n2016-04-16 16:18:24,303 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [NAME_INDEX] [WriteLevel0TableForRecovery] Level-0 table #211: 66777884 bytes OK \n2016-04-16 16:18:24,303 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904303854, \"cf_name\": \"NAME_INDEX\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 211, \"file_size\": 66777884, \"table_properties\": {\"data_size\": 66213551, \"index_size\": 628663, \"filter_size\": 0, \"raw_key_size\": 82480425, \"raw_average_key_size\": 22, \"raw_value_size\": 14732488, \"raw_average_value_size\": 4, \"num_data_blocks\": 16238, \"num_entries\": 3683122, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,405 [main] INFO  (RocksLoggerAdapter.java [log]:53) - Creating manifest 212\n\n2016-04-16 16:18:24,409 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [NAME_ID_INDEX] [WriteLevel0TableForRecovery] Level-0 table #213: started \n2016-04-16 16:18:24,410 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [NAME_ID_INDEX] [WriteLevel0TableForRecovery] Level-0 table #213: 63250 bytes OK \n2016-04-16 16:18:24,411 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904410993, \"cf_name\": \"NAME_ID_INDEX\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 213, \"file_size\": 63250, \"table_properties\": {\"data_size\": 62074, \"index_size\": 623, \"filter_size\": 0, \"raw_key_size\": 19457, \"raw_average_key_size\": 27, \"raw_value_size\": 50135, \"raw_average_value_size\": 69, \"num_data_blocks\": 16, \"num_entries\": 718, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,413 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [SORT_NAME_INDEX] [WriteLevel0TableForRecovery] Level-0 table #214: started \n2016-04-16 16:18:24,436 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [SORT_NAME_INDEX] [WriteLevel0TableForRecovery] Level-0 table #214: 484107 bytes OK \n2016-04-16 16:18:24,436 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904436652, \"cf_name\": \"SORT_NAME_INDEX\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 214, \"file_size\": 484107, \"table_properties\": {\"data_size\": 480188, \"index_size\": 3773, \"filter_size\": 0, \"raw_key_size\": 540855, \"raw_average_key_size\": 17, \"raw_value_size\": 0, \"raw_average_value_size\": 0, \"num_data_blocks\": 118, \"num_entries\": 31815, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,440 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [UUID_STORE] [WriteLevel0TableForRecovery] Level-0 table #215: started \n2016-04-16 16:18:24,441 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [UUID_STORE] [WriteLevel0TableForRecovery] Level-0 table #215: 668 bytes OK \n2016-04-16 16:18:24,442 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904441964, \"cf_name\": \"UUID_STORE\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 215, \"file_size\": 668, \"table_properties\": {\"data_size\": 33, \"index_size\": 35, \"filter_size\": 0, \"raw_key_size\": 9, \"raw_average_key_size\": 9, \"raw_value_size\": 8, \"raw_average_value_size\": 8, \"num_data_blocks\": 1, \"num_entries\": 1, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,444 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [UUID_URI_INDEX] [WriteLevel0TableForRecovery] Level-0 table #216: started \n2016-04-16 16:18:24,446 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [UUID_URI_INDEX] [WriteLevel0TableForRecovery] Level-0 table #216: 66443 bytes OK \n2016-04-16 16:18:24,446 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904446594, \"cf_name\": \"UUID_URI_INDEX\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 216, \"file_size\": 66443, \"table_properties\": {\"data_size\": 65318, \"index_size\": 575, \"filter_size\": 0, \"raw_key_size\": 17232, \"raw_average_key_size\": 24, \"raw_value_size\": 46272, \"raw_average_value_size\": 64, \"num_data_blocks\": 17, \"num_entries\": 718, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,449 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [URI_UUID_INDEX] [WriteLevel0TableForRecovery] Level-0 table #217: started \n2016-04-16 16:18:24,451 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [URI_UUID_INDEX] [WriteLevel0TableForRecovery] Level-0 table #217: 31558 bytes OK \n2016-04-16 16:18:24,451 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904451753, \"cf_name\": \"URI_UUID_INDEX\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 217, \"file_size\": 31558, \"table_properties\": {\"data_size\": 30297, \"index_size\": 676, \"filter_size\": 0, \"raw_key_size\": 52016, \"raw_average_key_size\": 72, \"raw_value_size\": 11488, \"raw_average_value_size\": 16, \"num_data_blocks\": 8, \"num_entries\": 718, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,456 [main] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904456005, \"job\": 1, \"event\": \"recovery_finished\"} \n2016-04-16 16:18:24,456 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 2] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks//MANIFEST-000188 type=3 #188 -- OK\n\n2016-04-16 16:18:24,471 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 2] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks//000209.log type=0 #209 -- OK\n\n2016-04-16 16:18:24,485 [main] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 2] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks//000199.log type=0 #199 -- OK\n\n2016-04-16 16:18:24,493 [main] INFO  (RocksLoggerAdapter.java [log]:53) - DB pointer 0x7ff422afc800 \n2016-04-16 16:18:24,494 [Thread-6] INFO  (RocksLoggerAdapter.java [log]:53) - [UUID_URI_INDEX] [JOB 6] Compacting 2@0 + 1@1 files to L1, score 1.00 \n2016-04-16 16:18:24,494 [Thread-7] INFO  (RocksLoggerAdapter.java [log]:53) - [NAME_INDEX] [JOB 5] Compacting 2@0 + 1@1 files to L1, score 1.00 \n2016-04-16 16:18:24,495 [Thread-10] INFO  (RocksLoggerAdapter.java [log]:53) - [NAME_INDEX] Compaction start summary: Base version 33 Base level 0, inputs: [211(63MB) 210(63MB)], [205(63MB)]\n\n2016-04-16 16:18:24,495 [Thread-11] INFO  (RocksLoggerAdapter.java [log]:53) - [NAME_ID_INDEX] Compaction start summary: Base version 32 Base level 0, inputs: [213(61KB) 191(62KB)], [179(462KB)]\n\n2016-04-16 16:18:24,495 [Thread-5] FATAL (RocksLoggerAdapter.java [log]:53) - ------- DUMPING STATS ------- \n2016-04-16 16:18:24,495 [Thread-12] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904495643, \"job\": 5, \"event\": \"compaction_started\", \"files_L0\": [211, 210], \"files_L1\": [205], \"score\": 1, \"input_data_size\": 200333577} \n2016-04-16 16:18:24,496 [Thread-13] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904495738, \"job\": 4, \"event\": \"compaction_started\", \"files_L0\": [213, 191], \"files_L1\": [179], \"score\": 1, \"input_data_size\": 600797} \n2016-04-16 16:18:24,496 [Thread-15] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904495899, \"job\": 6, \"event\": \"compaction_started\", \"files_L0\": [216, 197], \"files_L1\": [177], \"score\": 1, \"input_data_size\": 608122} \n2016-04-16 16:18:24,496 [Thread-14] FATAL (RocksLoggerAdapter.java [log]:53) - \n** Compaction Stats [default] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [XML_DOM_STORE] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L1      3/0     157.49   0.3      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      3/0     157.49   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [SYMBOL_STORE] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      1/0       0.00   0.5      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n  L1      3/0       0.03   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      4/0       0.03   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [COLLECTION_STORE] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [NAME_ID_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0       0.12   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     30.2         0         1    0.002       0      0\n  L1      1/0       0.45   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      3/0       0.57   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     30.2         0         1    0.002       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     30.2         0         1    0.002       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [NAME_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0     127.37   1.0      0.0     0.0      0.0       0.1      0.1       0.0   0.0      0.0     30.7         4         2    2.075       0      0\n  L1      1/0      63.68   0.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      3/0     191.05   0.0      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0     30.7         4         2    2.075       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0     30.7         4         2    2.075       0      0\nFlush(GB): cumulative 0.124, interval 0.124\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [SORT_NAME_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      1/0       0.46   0.5      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     19.9         0         1    0.023       0      0\n  L1      1/0       0.46   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      2/0       0.92   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     19.9         0         1    0.023       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     19.9         0         1    0.023       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [SORT_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [NGRAM_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [METADATA_STORE] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L1      1/0       0.49   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      1/0       0.49   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [METADATA_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L1      1/0       0.19   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      1/0       0.19   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [UUID_STORE] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      1/0       0.00   0.5      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.5         0         1    0.001       0      0\n  L1      1/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      2/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.5         0         1    0.001       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.5         0         1    0.001       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [UUID_URI_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0       0.13   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     24.1         0         1    0.003       0      0\n  L1      1/0       0.45   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      3/0       0.58   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     24.1         0         1    0.003       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     24.1         0         1    0.003       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** Compaction Stats [URI_UUID_INDEX] **\nLevel    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0       0.06   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     13.0         0         1    0.002       0      0\n  L1      1/0       0.21   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0      0\n Sum      3/0       0.27   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     13.0         0         1    0.002       0      0\n Int      0/0       0.00   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     13.0         0         1    0.002       0      0\nFlush(GB): cumulative 0.000, interval 0.000\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** DB Stats **\nUptime(secs): 44.5 total, 44.5 interval\nCumulative writes: 0 writes, 0 keys, 0 batches, 0.0 writes per batch, ingest: 0.00 GB, 0.00 MB/s\nCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nInterval writes: 0 writes, 0 keys, 0 batches, 0.0 writes per batch, ingest: 0.00 MB, 0.00 MB/s\nInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\n\n2016-04-16 16:18:24,499 [Thread-16] INFO  (RocksLoggerAdapter.java [log]:53) - [URI_UUID_INDEX] [JOB 3] Compacting 2@0 + 1@1 files to L1, score 1.00 \n2016-04-16 16:18:24,499 [main] INFO  (RocksDatabase.java [open]:121) - Opened RocksDB: /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks \n2016-04-16 16:18:24,499 [Thread-17] INFO  (RocksLoggerAdapter.java [log]:53) - [URI_UUID_INDEX] Compaction start summary: Base version 41 Base level 0, inputs: [217(30KB) 198(31KB)], [178(214KB)]\n\n2016-04-16 16:18:24,500 [Thread-18] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904500096, \"job\": 3, \"event\": \"compaction_started\", \"files_L0\": [217, 198], \"files_L1\": [178], \"score\": 1, \"input_data_size\": 282942} \n2016-04-16 16:18:24,507 [Thread-19] INFO  (RocksLoggerAdapter.java [log]:53) - [UUID_URI_INDEX] [JOB 6] Generated table #222: 4340 keys, 475188 bytes \n2016-04-16 16:18:24,508 [Thread-20] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904507889, \"cf_name\": \"UUID_URI_INDEX\", \"job\": 6, \"event\": \"table_file_creation\", \"file_number\": 222, \"file_size\": 475188, \"table_properties\": {\"data_size\": 471554, \"index_size\": 3484, \"filter_size\": 0, \"raw_key_size\": 104160, \"raw_average_key_size\": 24, \"raw_value_size\": 356981, \"raw_average_value_size\": 82, \"num_data_blocks\": 117, \"num_entries\": 4340, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,508 [Thread-22] INFO  (RocksLoggerAdapter.java [log]:53) - [UUID_URI_INDEX] [JOB 6] Compacted 2@0 + 1@1 files to L1 => 475188 bytes \n2016-04-16 16:18:24,508 [Thread-23] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904508537, \"cf_name\": \"NAME_ID_INDEX\", \"job\": 4, \"event\": \"table_file_creation\", \"file_number\": 221, \"file_size\": 474145, \"table_properties\": {\"data_size\": 468917, \"index_size\": 5051, \"filter_size\": 0, \"raw_key_size\": 165514, \"raw_average_key_size\": 38, \"raw_value_size\": 368110, \"raw_average_value_size\": 84, \"num_data_blocks\": 110, \"num_entries\": 4342, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,509 [Thread-24] INFO  (RocksLoggerAdapter.java [log]:53) - [NAME_ID_INDEX] [JOB 4] Compacted 2@0 + 1@1 files to L1 => 474145 bytes \n2016-04-16 16:18:24,510 [Thread-25] INFO  (RocksLoggerAdapter.java [log]:53) - (Original Log Time 2016/04/16-16:18:24.509723) [UUID_URI_INDEX] compacted to: base level 1 max bytes base 536870912 files[0 1 0 0 0 0 0] max score 0.00, MB/sec: 50.3 rd, 39.3 wr, level 1, files in(2, 1) out(1) MB in(0.1, 0.5) out(0.5), read-write-amplify(8.1) write-amplify(3.6) OK, records in: 5778, records dropped: 1438\n\n2016-04-16 16:18:24,510 [Thread-26] INFO  (RocksLoggerAdapter.java [log]:53) - (Original Log Time 2016/04/16-16:18:24.510125) [NAME_ID_INDEX] compacted to: base level 1 max bytes base 536870912 files[0 1 0 0 0 0 0] max score 0.00, MB/sec: 46.9 rd, 37.0 wr, level 1, files in(2, 1) out(1) MB in(0.1, 0.5) out(0.5), read-write-amplify(8.5) write-amplify(3.7) OK, records in: 5782, records dropped: 1440\n\n2016-04-16 16:18:24,510 [Thread-27] INFO  (RocksLoggerAdapter.java [log]:53) - (Original Log Time 2016/04/16-16:18:24.509780) EVENT_LOG_v1 {\"time_micros\": 1460837904509738, \"job\": 6, \"event\": \"compaction_finished\", \"compaction_time_micros\": 12101, \"output_level\": 1, \"num_output_files\": 1, \"total_output_size\": 475188, \"num_input_records\": 5778, \"num_output_records\": 4340, \"num_subcompactions\": 1, \"lsm_state\": [0, 1, 0, 0, 0, 0, 0]} \n2016-04-16 16:18:24,510 [Thread-28] INFO  (RocksLoggerAdapter.java [log]:53) - (Original Log Time 2016/04/16-16:18:24.510167) EVENT_LOG_v1 {\"time_micros\": 1460837904510139, \"job\": 4, \"event\": \"compaction_finished\", \"compaction_time_micros\": 12808, \"output_level\": 1, \"num_output_files\": 1, \"total_output_size\": 474145, \"num_input_records\": 5782, \"num_output_records\": 4342, \"num_subcompactions\": 1, \"lsm_state\": [0, 1, 0, 0, 0, 0, 0]} \n2016-04-16 16:18:24,511 [Thread-29] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 6] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000216.sst type=2 #216 -- OK\n\n2016-04-16 16:18:24,511 [Thread-30] INFO  (RocksLoggerAdapter.java [log]:53) - [URI_UUID_INDEX] [JOB 3] Generated table #224: 4340 keys, 220047 bytes \n2016-04-16 16:18:24,511 [Thread-31] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 4] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000213.sst type=2 #213 -- OK\n\n2016-04-16 16:18:24,511 [Thread-32] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904511615, \"job\": 6, \"event\": \"table_file_deletion\", \"file_number\": 216} \n2016-04-16 16:18:24,511 [Thread-33] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904511680, \"cf_name\": \"URI_UUID_INDEX\", \"job\": 3, \"event\": \"table_file_creation\", \"file_number\": 224, \"file_size\": 220047, \"table_properties\": {\"data_size\": 214261, \"index_size\": 5384, \"filter_size\": 0, \"raw_key_size\": 391701, \"raw_average_key_size\": 90, \"raw_value_size\": 69440, \"raw_average_value_size\": 16, \"num_data_blocks\": 54, \"num_entries\": 4340, \"filter_policy_name\": \"\", \"kDeletedKeys\": \"0\"}} \n2016-04-16 16:18:24,512 [Thread-34] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904512025, \"job\": 4, \"event\": \"table_file_deletion\", \"file_number\": 213} \n2016-04-16 16:18:24,512 [Thread-35] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 6] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000197.sst type=2 #197 -- OK\n\n2016-04-16 16:18:24,512 [Thread-37] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 4] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000191.sst type=2 #191 -- OK\n\n2016-04-16 16:18:24,513 [Thread-38] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904512838, \"job\": 6, \"event\": \"table_file_deletion\", \"file_number\": 197} \n2016-04-16 16:18:24,513 [Thread-39] INFO  (RocksLoggerAdapter.java [log]:53) - (Original Log Time 2016/04/16-16:18:24.513273) [URI_UUID_INDEX] compacted to: base level 1 max bytes base 536870912 files[0 1 0 0 0 0 0] max score 0.00, MB/sec: 24.3 rd, 18.9 wr, level 1, files in(2, 1) out(1) MB in(0.1, 0.2) out(0.2), read-write-amplify(7.9) write-amplify(3.5) OK, records in: 5778, records dropped: 1438\n\n2016-04-16 16:18:24,514 [Thread-40] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904513751, \"job\": 4, \"event\": \"table_file_deletion\", \"file_number\": 191} \n2016-04-16 16:18:24,514 [Thread-42] INFO  (RocksLoggerAdapter.java [log]:53) - (Original Log Time 2016/04/16-16:18:24.513460) EVENT_LOG_v1 {\"time_micros\": 1460837904513287, \"job\": 3, \"event\": \"compaction_finished\", \"compaction_time_micros\": 11662, \"output_level\": 1, \"num_output_files\": 1, \"total_output_size\": 220047, \"num_input_records\": 5778, \"num_output_records\": 4340, \"num_subcompactions\": 1, \"lsm_state\": [0, 1, 0, 0, 0, 0, 0]} \n2016-04-16 16:18:24,514 [Thread-45] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904514731, \"job\": 6, \"event\": \"table_file_deletion\", \"file_number\": 177} \n2016-04-16 16:18:24,515 [Thread-43] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 4] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000179.sst type=2 #179 -- OK\n\n2016-04-16 16:18:24,515 [Thread-44] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 3] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000217.sst type=2 #217 -- OK\n\n2016-04-16 16:18:24,515 [Thread-46] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904515312, \"job\": 4, \"event\": \"table_file_deletion\", \"file_number\": 179} \n2016-04-16 16:18:24,515 [Thread-47] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904515506, \"job\": 3, \"event\": \"table_file_deletion\", \"file_number\": 217} \n2016-04-16 16:18:24,516 [Thread-48] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 3] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000198.sst type=2 #198 -- OK\n\n2016-04-16 16:18:24,518 [Thread-49] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904516713, \"job\": 3, \"event\": \"table_file_deletion\", \"file_number\": 198} \n2016-04-16 16:18:24,519 [Thread-50] DEBUG (RocksLoggerAdapter.java [log]:53) - [JOB 3] Delete /Users/aretter/code/exist-rocks/webapp/WEB-INF/data/rocks/000178.sst type=2 #178 -- OK\n\n2016-04-16 16:18:24,519 [Thread-51] INFO  (RocksLoggerAdapter.java [log]:53) - EVENT_LOG_v1 {\"time_micros\": 1460837904519668, \"job\": 3, \"event\": \"table_file_deletion\", \"file_number\": 178} \n```\n\nIs the recovery process deleting my data somehow?\n",
	"number": 1082,
	"title": "Is this a corrupt database?"
}, {
	"body": "when my rocksdb instance run a few time , i find some error in rocksdb LOG,\n\n2016/04/14-15:46:14.280843 7f68a4bff700 Move log file ./db_data/rt_mc_1_11/089425.log to ./db_data/rt_mc_1_11/archive/089425.log -- OK\n2016/04/14-15:46:14.280885 7f68a4bff700 Move log file ./db_data/rt_mc_1_11/089426.log to ./db_data/rt_mc_1_11/archive/089412.log -- OK\n2016/04/14-15:46:14.280931 7f68a4bff700 Move log file ./db_data/rt_mc_1_11//089425.log to ./db_data/rt_mc_1_11/archive/089427.log -- IO error: ./db_data/rt_m\nc_1_11//089427.log: No such file or directory\n2016/04/14-15:46:14.280953 7f68a4bff700 Move log file ./db_data/rt_mc_1_11//089426.log to ./db_data/rt_mc_1_11/archive/089426.log -- IO error: ./db_data/rt_m\nc_1_11//089426.log: No such file or directory\n\nthis situation only happen in PurgeObsoleteFiles()---->wal_manager_.ArchiveWALFile(fname, number),\ni am confused that why this happen twice? is this error very bad ?\n",
	"number": 1079,
	"title": "rockstable run error : IO error No such file or directory"
}, {
	"body": "Currently RocksDB \"leaks\" memory on exit.\n\nThis is not a serious problem in itself as since the executable is exiting, it doesn't matter if not all resources are freed.\n\nHowever it has the disadvantage of triggering a lot of false alerts (noise) in tools that track memory leaks.\n\nIt would therefore be really nice if RocksDB could capture the main thread exit and properly deallocate all structures. That problem is especially true on the Windows port where the main thread exit is ignored.\n\nIt may also require adding \"UnsafeCleanXXX\" function in the environement as you may want to manually destroy some object when you know your application is exiting.\n",
	"number": 1077,
	"title": "Leaks on unloading"
}, {
	"body": "Hello,\n\nThere is already the matter of the environment not being destroyed on termination of the program, we solved that with an `Env::UnsafeDeallocate()` function that we call right before exiting the application.\n\nBut there is a tougher problem with the usage tracking of RocksDB (thread_local.cc).\n\nThe problem is that it's extremly POSIXey on one side and leaks memory on exit on the other side. I'm aware that the leaked memory isn't problematic in itself, except that it adds noise when you are tracking memory usage in your application.\n\nCurrently in RocksDB we have [linker directives](https://github.com/facebook/rocksdb/blob/master/util/thread_local.cc#L83) to place hooks on thread termination, but there are two problems:\n- The main thread exit isn't captured\n- If the code is within a library, the linker directives will be ignored because it's not invoked in the first place. The user has to add the /INCLUDE manually when linking the library to an exe or a dll (I tested this on Visual Studio 2015 update 2, I might have done something wrong)\n\nI think the best would be to have a facade for threads that allows the addition of an \"on_exit\" functor, not unlike Boost.Thread. That would also solve a lot of issues regarding deletion order with the StaticMeta object.\n\nWhat do you think?\n",
	"number": 1070,
	"title": "StaticMeta issues"
}, {
	"body": "Hi!\n\nI found a bug here (db/table_cache.cc:95). RocksDB v.4.4.1\n\n```\nStatus s = ioptions_.env->NewRandomAccessFile(fname, &file, env_options);\nif (sequential_mode && ioptions_.compaction_readahead_size > 0) {\n  file = NewReadaheadRandomAccessFile(std::move(file),\n                                      ioptions_.compaction_readahead_size);\n}\n```\n\nIf the file 'fname' does not exist, and the condition in the next line is true,\nthe app crashes inside of NewReadaheadRandomAccessFile because 'file' is null.\n\nThis code works fine:\n\n```\nStatus s = ioptions_.env->NewRandomAccessFile(fname, &file, env_options);\nif (s.ok() && sequential_mode && ioptions_.compaction_readahead_size > 0) {\n  file = NewReadaheadRandomAccessFile(std::move(file),\n                                      ioptions_.compaction_readahead_size);\n}\n```\n",
	"number": 1065,
	"title": "Crash during compaction if some file is not found"
}, {
	"body": "After initial crash process using debug build of rocks 4.2.0 consistently fails with the error below. Please let me know what additional information you would like me to provide. I don't have core dump for initial crash to confirm that this failure caused the first crash but subsequent restarts result in the following assert failure included in this message.\n\nThank you,\nDmitry\n#0  0x00007f3bdfc3ccc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\n#1  0x00007f3bdfc400d8 in __GI_abort () at abort.c:89\n#2  0x00007f3bdfc35b86 in __assert_fail_base (fmt=0x7f3bdfd86830 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=assertion@entry=0x7f3b96983588 \"search_left_bound == (int32_t)level_files.size() || search_right_bound == -1\",\n\n```\nfile=file@entry=0x7f3b969834d8 \"db/forward_iterator.cc\", line=line@entry=321, function=function@entry=0x7f3b96985260 \"void rocksdb::ForwardIterator::SeekInternal(const rocksdb::Slice&, bool)\") at assert.c:92\n```\n#3  0x00007f3bdfc35c32 in __GI___assert_fail (assertion=0x7f3b96983588 \"search_left_bound == (int32_t)level_files.size() || search_right_bound == -1\", file=0x7f3b969834d8 \"db/forward_iterator.cc\", line=321,\n\n```\nfunction=0x7f3b96985260 \"void rocksdb::ForwardIterator::SeekInternal(const rocksdb::Slice&, bool)\") at assert.c:101\n```\n#4  0x00007f3b9672ca05 in rocksdb::ForwardIterator::SeekInternal(rocksdb::Slice const&, bool) () from /tmp/librocksdbjni1082100708791379795..so\n#5  0x00007f3b9672c0e1 in rocksdb::ForwardIterator::Seek(rocksdb::Slice const&) () from /tmp/librocksdbjni1082100708791379795..so\n#6  0x00007f3b96717134 in rocksdb::DBIter::Seek(rocksdb::Slice const&) () from /tmp/librocksdbjni1082100708791379795..so\n#7  0x00007f3b968b38b0 in rocksdb::TtlIterator::Seek(rocksdb::Slice const&) () from /tmp/librocksdbjni1082100708791379795..so\n#8  0x00007f3b96639899 in Java_org_rocksdb_RocksIterator_seek0 () from /tmp/librocksdbjni1082100708791379795..so\n",
	"number": 1057,
	"title": "subsequent restarts after initial crash produce assert failure in debug build of rocksdb 4.2.0"
}, {
	"body": "In quasardb to make profiling easier we added the following method in Env:\n\n``` cpp\nvoid Env::UnsafeDeallocate()\n{\n    delete envptr;\n    envptr = nullptr;\n}\n```\n\nThe goal of this function is to deallocate all structures before exiting the program to make hunting for memory leaks easier. It's unsafe in the sense it cannot be called at any other moment but before all RocksDB threads and objects have been terminated.\n\nWould you like us to do a PR? It's still not 100% working on Windows because of the way the threads are hooked.\n",
	"number": 1051,
	"title": "Add void Env::UnsafeDeallocate()?"
}, {
	"body": "",
	"number": 1043,
	"title": "Making HistogramImpl Configurable through CreateDBStatistics()"
}, {
	"body": "both #seekToFirst() and #seek(byte[] key) work just fine, however when I use #seekToLast() instead the iterator is always invalid.  \n\nAlso, this might be expected/desired behavior, but if I continue to iterate forward on a valid iterator with #next() until #isValid() returns false I am then unable to use #prev() to go back to a valid state.  The iterator will remain invalid once it has completed its forward traversal.\n",
	"number": 1034,
	"title": "seekToLast() Broken For rocksdbjni Library?"
}, {
	"body": "I just noticed there is a cuckoo hash based memtable https://github.com/facebook/rocksdb/commit/9d9d2965cb4df6163624a31d2b5dd7364813f27f, which seems to be very good for point lookup. But I do not see wiki mentioning it. Curious is it of production quality? If so, could someone please add it to wikis:\nhttps://github.com/facebook/rocksdb/wiki/Hash-based-memtable-implementations\nhttps://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide#custom-memtable-and-table-format\n\nThanks!\n",
	"number": 1033,
	"title": "Add wiki about cuckoo-hash based memtable."
}, {
	"body": "Hi,\n\nif i disable this option, does that mean RocksDB do directIO for file read?\n\nThanks,\nSheng\n",
	"number": 1032,
	"title": "allow_os_buffer option"
}, {
	"body": "On a freshly cloned copy of rocksdb, on a Mac:\n\n==15626== Memcheck, a memory error detector\n==15626== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.\n==15626== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info\n==15626== Command: ./db_test\n==15626== \n--15626-- run: /usr/bin/dsymutil \"./db_test\"\n[==========] Running 251 tests from 5 test cases.\n[----------] Global test environment set-up.\n[----------] 175 tests from DBTest\n[ RUN      ] DBTest.MockEnvTest\n--15626-- UNKNOWN mach_msg unhandled MACH_SEND_TRAILER option\n--15626-- UNKNOWN mach_msg unhandled MACH_SEND_TRAILER option (repeated 2 times)\n--15626-- UNKNOWN mach_msg unhandled MACH_SEND_TRAILER option (repeated 4 times)\n[       OK ] DBTest.MockEnvTest (1257 ms)\n[ RUN      ] DBTest.MemEnvTest\n[       OK ] DBTest.MemEnvTest (150 ms)\n[ RUN      ] DBTest.WriteEmptyBatch\n==15626== Conditional jump or move depends on uninitialised value(s)\n==15626==    at 0x1002C152F: rocksdb::BlockBasedTableBuilder::BlockBasedTableBuilder(rocksdb::ImmutableCFOptions const&, rocksdb::BlockBasedTableOptions const&, rocksdb::InternalKeyComparator const&, std::__1::vector<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> >, std::__1::allocator<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> > > > const_, unsigned int, rocksdb::WritableFileWriter_, rocksdb::CompressionType, rocksdb::CompressionOptions const&, bool) (block_based_table_builder.cc:525)\n==15626==    by 0x1002C16ED: rocksdb::BlockBasedTableBuilder::BlockBasedTableBuilder(rocksdb::ImmutableCFOptions const&, rocksdb::BlockBasedTableOptions const&, rocksdb::InternalKeyComparator const&, std::__1::vector<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> >, std::__1::allocator<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> > > > const_, unsigned int, rocksdb::WritableFileWriter_, rocksdb::CompressionType, rocksdb::CompressionOptions const&, bool) (block_based_table_builder.cc:523)\n==15626==    by 0x1002C77A0: rocksdb::BlockBasedTableFactory::NewTableBuilder(rocksdb::TableBuilderOptions const&, unsigned int, rocksdb::WritableFileWriter_) const (block_based_table_factory.cc:73)\n==15626==    by 0x1001A13E0: rocksdb::BuildTable(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, rocksdb::Env_, rocksdb::ImmutableCFOptions const&, rocksdb::EnvOptions const&, rocksdb::TableCache_, rocksdb::InternalIterator_, rocksdb::FileMetaData_, rocksdb::InternalKeyComparator const&, std::__1::vector<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> >, std::__1::allocator<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> > > > const_, unsigned int, std::__1::vector<unsigned long long, std::__1::allocator<unsigned long long> >, unsigned long long, rocksdb::CompressionType, rocksdb::CompressionOptions const&, bool, rocksdb::InternalStats_, rocksdb::Env::IOPriority, rocksdb::TableProperties_) (builder.cc:47)\n==15626==    by 0x1001F7BBD: rocksdb::DBImpl::WriteLevel0TableForRecovery(int, rocksdb::ColumnFamilyData_, rocksdb::MemTable_, rocksdb::VersionEdit_) (db_impl.cc:1414)\n==15626==    by 0x1001F713F: rocksdb::DBImpl::RecoverLogFiles(std::__1::vector<unsigned long long, std::__1::allocator<unsigned long long> > const&, unsigned long long_, bool) (db_impl.cc:1347)\n==15626==    by 0x1001F5C2E: rocksdb::DBImpl::Recover(std::__1::vector<rocksdb::ColumnFamilyDescriptor, std::__1::allocator<rocksdb::ColumnFamilyDescriptor> > const&, bool, bool) (db_impl.cc:1036)\n==15626==    by 0x10021021C: rocksdb::DB::Open(rocksdb::DBOptions const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<rocksdb::ColumnFamilyDescriptor, std::__1::allocator<rocksdb::ColumnFamilyDescriptor> > const&, std::__1::vector<rocksdb::ColumnFamilyHandle*, std::__1::allocator<rocksdb::ColumnFamilyHandle*> >_, rocksdb::DB__) (db_impl.cc:5387)\n==15626==    by 0x100190BEB: rocksdb::DBTestBase::TryReopenWithColumnFamilies(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<rocksdb::Options, std::__1::allocator<rocksdb::Options> > const&) (db_test_util.cc:428)\n==15626==    by 0x1001910D2: rocksdb::DBTestBase::TryReopenWithColumnFamilies(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, rocksdb::Options const&) (db_test_util.cc:435)\n==15626==    by 0x1000056BA: rocksdb::DBTest_WriteEmptyBatch_Test::TestBody() (db_test.cc:245)\n==15626==    by 0x10041BDCB: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test_, void (testing::Test::_)(), char const_) (gtest-all.cc:3822)\n==15626== \n",
	"number": 1030,
	"title": "valgrind uninit value on mac"
}, {
	"body": "hi\uff0c i am trying to use rocksdb in java. the performance drop from 100+ms/10000 queries to 200+s/10000 queries, Anyone know why?, the test code is as following\n\n```\n  public static void main(String args[]) throws RocksDBException {\n    Options options = PredefinedOptions.DEFAULT.createOptions()\n        .setCreateIfMissing(true).setWriteBufferSize(128 * 1204 * 1024);\n    File rocksDbPath = new File(\"db\");\n    final RocksDB db = RocksDB.open(options, rocksDbPath.getAbsolutePath());\n    Thread th = new Thread(new Runnable() {\n      private int round = 0;\n      private Random x = new Random();\n      @Override\n      public void run() {\n        for (int j = 0; j < 100; j++) {\n          long start = System.currentTimeMillis();\n          for (long i = 0; i < 100000L; i++) {\n            try {\n              db.put(Bytes.toBytes(j * 100000L + i), Bytes.toBytes(RandomWord.get()));\n            } catch (RocksDBException e) {\n              e.printStackTrace();\n            }\n          }\n          logger.info(\"init round {}, step {}, put cost {} ms\", round, j,\n              System.currentTimeMillis() - start);\n        }\n          long startTime = System.currentTimeMillis();\n          File localBackupPath = new File(\"backup-\" + round);\n          localBackupPath.mkdirs();\n          try (BackupEngine backupEngine = BackupEngine.open(Env.getDefault(),\n              new BackupableDBOptions(localBackupPath.getAbsolutePath()))) {\n            backupEngine.createNewBackup(db);\n          } catch (RocksDBException e) {\n            e.printStackTrace();\n          }\n          long endTime = System.currentTimeMillis();\n          logger.info(\"round {} new backup cost {}\", round, endTime - startTime);\n        while (true) {\n          round++;\n          long start = System.currentTimeMillis();\n          logger.info(\"start round {}\", round);\n          for (long i = 0; i < 10000L; i++) {\n            try {\n              db.get(Bytes.toBytes((long) x.nextInt(100) * 100000 + x.nextInt(100000)));\n            } catch (RocksDBException e) {\n              e.printStackTrace();\n            }\n          }\n          logger.info(\"round {}, 10000 get cost {} ms\", round, System.currentTimeMillis() - start);\n        }\n      }\n    });\n    th.start();\n    try {\n      th.join();\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n  }\n```\n",
	"number": 1029,
	"title": "Get performance dropping dramatically after backup"
}, {
	"body": "Currently C bindings only support flush the default column family explicitly. There should be a method to flush specified column family explicitly too.\n",
	"number": 1028,
	"title": "C bindings: missing flush method that accept column family as parameter"
}, {
	"body": "hi, guys\n\nI have a problem that may need your help .\n\nI have 1 billion tags can be seen as read-only after initial bulk-load, so I want to know if there are any useful tricks to optimize the *_random read speed *_ in this particular case ?\n\ndo you have any good ideas?\n\nThanks ~\n",
	"number": 1027,
	"title": "how to optimize the configuration parameters to improve the random read speed of RocksDB? "
}, {
	"body": "Hi,\n\ni am trying to profiling rocksdb performance, basically i want to know the latency for a single PUT, GET operation spend at software part of the I/O path which excludes the real I/O transfer time.\n\nis there a way to at least know the part spend at rocksDB layer?\n",
	"number": 1024,
	"title": "is there a way to measure software latency of PUT, GET exclude the I/O transfer time "
}, {
	"body": "Hi,\n\ni am trying to evaluate the hit rate of block cache in rocksDB, is there a way to do this?\n\nThanks\n",
	"number": 1023,
	"title": "how to check block cache hit rate"
}, {
	"body": "Hello.\nI use next config for java+rocksdb\n1. java 8\n2. rocksdb 3.1.13\n3. CentOS 6.5\n\nI start 4 data bases inside one jvm (all are in read only mode). Each data base ~ 8 Gb on disk.\nAfter start everything looks fine. But after 1-2 hours db is going to increase CPU usage.\nCheck screenshot: [here](http://postimg.org/image/49uhiy71l/)\nIf I restart JVM everything looks fine next 1-2 hours.\n\nHW config:\n1. CPU E5-2430\n2. 72Gb of RAM\n\nAll db files are in memory + I use blocks cache.\n",
	"number": 1021,
	"title": "rocksdb + java jni performance degradation"
}, {
	"body": "hi.I find an empty rocksdb's database need 145 MB of space.I use \"du -h\" to find the phenomenon.Why and can I fix it.\n",
	"number": 1017,
	"title": "Initializes the space occupied"
}, {
	"body": "Hi, I'm new to rocksdb and we have a userspace NVMe driver that can access raw SSD device (see https://github.com/MicronSSD/unvme).  I would like to port rocksdb to run on this driver and have a couple questions:\n\nDoes rocksdb access raw device or rely on a filesystem (i.e. like mysql)?\nIs there a device access layer in rocksdb that I can replace with a userspace driver?\n\nThanks,\nDeyoung\n",
	"number": 1016,
	"title": "Support for userspace driver"
}, {
	"body": "Hi,\n\nI am working on using Rocksdb as SSD cache, i.e. build an memcache interface on top of Rocksdb. There will be only point lookup and the read access pattern will be pretty random.\n\nThe main things I am trying to solve is are \n1) have low read amplification\n2) have TTL support\n3) have way to eviction items when disk becomes full.\n\nMy current ideas are:\n1) Use level compaction, while caching all the indexes and bloom filters in memory(maybe not the last level, which will depend on how much memory are already used). For the rest memory, leave to block cache/page cache. With these, I hope to reduce keep read amplification to ~1.\n2) TTL support is easy, by suffixing TTL in the value.\n3) I saw the recent introduction of SstFileManager, which is great for preventing out of disk space. The basic idea of eviction I have in mind is in a background thread, to keep track of all the SST files, and their last modified time. When disk space usage reaches threshold1(e.g. 60%), I will compact the oldest SST files(by calling CompactFiles() API). Hopefully, we can remove some expired items to save space. When disk space usage reaches threshold2(e.g. 80%), I will delete oldest SST files(by calling DeleteFile() API). So such eviction is not LRU, but basically my assumption is the oldest file handles the lowest number of hits now.\n\nWhat do you guys think about this approach?\n\nSome questions I have are:\n1) How to know how many bytes are reduced after calling CompactFiles()? Is there a way to know after manual compaction ends, what's the generated new files and their size? In some cases if there are no expired items which causes compaction not able to free disk space, I might just want to stop doing compaction and wait for it reaches threshold2 and then delete files.\n2) To monitor SST files, I am thinking about inheriting SstFileManager, and keep track of the timestamps of all the SST files. I only want to delete the oldest files in the last level to prevent consistency issue(if I delete a file in a lower level, it might remove keys of higher version). Is there any easy way to know what level an SST file is at?\n3) Would running multiple rocksdb instances improve performance?\n\nThanks!\n",
	"number": 1014,
	"title": "Questions related to use Rocksdb as SSD cache"
}, {
	"body": "- Modified the code to make it work on Visual Studio too\n- Now using fast crc for the first and the last bytes of the data (STEP1)\n",
	"number": 1010,
	"title": "Fast CRC32C support for windows apps (SSE4.2)"
}, {
	"body": "",
	"number": 1008,
	"title": "Allow the use of use_mmap_reads in Windows environment"
}, {
	"body": "There is an assertion check in the constructor \nWinRandomAccessFile():\n assert(!options.use_mmap_reads);\n",
	"number": 1007,
	"title": "What is the problem with using \"use_mmap_reads\" in WinRandomAccessFile"
}, {
	"body": "https://github.com/facebook/rocksdb/blob/master/Makefile#L1065\n\n`*.h => \"*.h\"`\n\n```\n    for header in `find \"include/rocksdb\" -type f -name *.h`; do \\\n        install -C -m 644 $$header $(INSTALL_PATH)/$$header; \\\n    done\n```\n",
	"number": 1005,
	"title": "install-headers bug"
}, {
	"body": " can sst_dump  dump   data from  a runtime db?\n",
	"number": 995,
	"title": "what does the sst_dump_tool use for? "
}, {
	"body": "Can i delete the   files of  ColumnFamily ? \n\nstep1. \n  virtual Status DropColumnFamily(ColumnFamilyHandle\\* column_family);\n\nstep2. \n   virtual void GetColumnFamilyMetaData(ColumnFamilyHandle\\* column_family,ColumnFamilyMetaData\\* metadata)\n\nstep3.\n   DeleteFile(std::string name)\n",
	"number": 994,
	"title": "drop ColumnFamily and delete it's files"
}, {
	"body": "terminate called after throwing an instance of 'std::bad_alloc'\n  what():  std::bad_alloc.\n\nReceived signal 6 (Aborted)\n#0   /lib64/libc.so.6(gsignal+0x35) [0x7eff70feb625]\n#1   /lib64/libc.so.6(abort+0x175) [0x7eff70fece05]\n#2   /usr/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x12d) [0x7eff718a5a5d]\n#3   /usr/lib64/libstdc++.so.6(+0xbcbe6) [0x7eff718a3be6]\n#4   /usr/lib64/libstdc++.so.6(+0xbcc13) [0x7eff718a3c13]\n#5   /usr/lib64/libstdc++.so.6(+0xbcd0e) [0x7eff718a3d0e]\n#6   /usr/lib64/libstdc++.so.6(_Znwm+0x7d) [0x7eff718a40fd]\n#7   ./db_bench() [0x5620a2]\n#8   ./db_bench() [0x58b785]\n#9   ./db_bench() [0x565faa]\n#10  ./db_bench() [0x548416]\n#11  ./db_bench() [0x47dcb7]\n#12  ./db_bench() [0x4ea630]\n#13  ./db_bench() [0x4ed23d]\n#14  ./db_bench() [0x4b6b10]\n#15  ./db_bench() [0x4b735a]\n#16  ./db_bench() [0x4cc578]\n#17  ./db_bench() [0x5950ae]\n#18  /lib64/libpthread.so.0(+0x79d1) [0x7eff725429d1]\n#19  /lib64/libc.so.6(clone+0x6d) [0x7eff710a18fd]\n\ncould anyone let  me know what exactly is happening.\n\nRegards\n",
	"number": 988,
	"title": "Got core dump while running a benchmark test"
}, {
	"body": "Currently, `MemTableIterator::Seek` uses the prefix extractor only if memtable bloom filters are enabled. If a non-total-order seek is specified, the prefix extractor could also be used to limit the keys to those which contain the same prefix as the seek key. Note that the user can manually accomplish the same effect by setting `ReadOptions::iterate_upper_bound`, this would just be a way to automatically provide that optimization.\n",
	"number": 987,
	"title": "use prefix_extractor in non-total-order memtable seeks"
}, {
	"body": "Hi everyone, \n\nI was trying to do some benchmark testing on centos system which has 2 SSD of 800 GB.  How do I configure my SSD's so that the rockdb DB is on SSD so that I can do puts and gets from rocksdb which is on SSD.  I didn't see any SSD configuration document on rocksdb.org. Any help is appreciated.\n\nRegards\n",
	"number": 986,
	"title": "Configuring SSD's"
}, {
	"body": "I'm new to rocksdb and I compiled the examples and tried running the the c_simple_example from the example folder, but it fails with the following error \n`Assertion failed: (!err), function main, file c_simple_example.c, line 28.\nAbort trap: 6` \nWhat might be going wrong ? How to fix it ?\n",
	"number": 982,
	"title": "Error running c_simple_example OSX"
}, {
	"body": "Currently with the Java API, you have to explicitly free any `ColumnFamilyHandle` before you free the `RocksDB` object. This provides for a symmetrical open/close pattern, i.e.:\n\n``` java\nfinal List<ColumnFamilyHandle> cfHandles = new ArrayList<>();\ntry (final RocksDB db = RocksDB.open(options, dbFolder.getRoot().getAbsolutePath(), cfDescriptors, cfHandles)) {\n  try {\n    /* do stuff... */\n  } finally {\n    for (final ColumnFamilyHandle columnFamilyHandle : cfHandles) {\n      columnFamilyHandle.close();\n    }\n  }\n}\n```\n\nA more desirable symmetrical pattern would involved freeing any `ColumnFamilyHandle` after the `RocksDB` object. This seems to be possible using the C++ example given in https://github.com/facebook/rocksdb/blob/master/examples/column_families_example.cc#L31. The Java approach would then look like:\n\n``` java\nfinal List<ColumnFamilyHandle> cfHandles = new ArrayList<>();\ntry (final RocksDB db = RocksDB.open(options, dbFolder.getRoot().getAbsolutePath(), cfDescriptors, cfHandles)) {\n    /* do stuff... */\n} finally {\n  for (final ColumnFamilyHandle columnFamilyHandle : cfHandles) {\n    columnFamilyHandle.close();\n  }\n}\n```\n\nPerhaps an even nicer approach for the Java API would be to associate any `ColumnFamilyHandle` with the `RocksDB` object, and then closing the `RocksDB` object would also take care of freeing any `ColumnFamilyHandle` object too. Which would leave us with the easiest approach for Java users, e.g.:\n\n``` java\nfinal List<ColumnFamilyHandle> cfHandles = new ArrayList<>();\ntry (final RocksDB db = RocksDB.open(options, dbFolder.getRoot().getAbsolutePath(), cfDescriptors, cfHandles)) {\n    /* do stuff... */\n}\n```\n\n@yhchiang Any comments?\n",
	"number": 974,
	"title": "ColumnFamilyHandle must be free'd before RocksDB in Java API"
}, {
	"body": "Hi,\n\nI built RocksDB on AIX including JNI. It required many changes in sources and also has prerequisite for my other pull requests. I will wait until others will be merged until I will create another pull request.\n\nI just created issue to track the progress and discussion. First of all I would like to ask if RocksDB team wants to merge AIX build?\n\nThanks,\nTomas\n",
	"number": 966,
	"title": "AIX support"
}, {
	"body": "More context:\nWe are building a replication library for RocksDB. Slaves read updates from Master, and apply them locally. Currently WriteBatch takes std::string as input, which means we will have to do a string copy for all replicated data before applying it to Slave db. It will be more efficient if we can save this copy.\n",
	"number": 958,
	"title": "Can we add a constructor taking IOBuf or unique_ptr<IOBuf> to WriteBatch?"
}, {
	"body": "Hi,\n\nWorking on the erlang binding of rocksdb I tried to compile it on SmartOS but got the error reported here in leo-project/erocksdb#12 . Is there a recommended version of gcc/clang to build rocksdb?\n\n Any idea on how to compile rocksdb on old versions?\n",
	"number": 956,
	"title": "Error building on SmartOS"
}, {
	"body": "I have been testing RocksDB (both Master and v4.1 branches) in a Fedora 23\nPPC64LE environment using the stock GCC 5.1.1 compilers. During \"make check\" one\nand only one test fails, ldb_cmd_test:\n\n```\n[bcbrock@ppc64le]$ ./ldb_cmd_test\n[==========] Running 2 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 2 tests from LdbCmdTest\n[ RUN      ] LdbCmdTest.HexToString\nutil/ldb_cmd_test.cc:22: Failure\nValue of: static_cast<int>(actual[i])\n  Actual: 255\nExpected: expected[i]\nWhich is: -1\n[  FAILED  ] LdbCmdTest.HexToString (0 ms)\n[ RUN      ] LdbCmdTest.HexToStringBadInputs\nInvalid hex input 123.  Must start with 0x\nInvalid hex input Ox12.  Must start with 0x\n[       OK ] LdbCmdTest.HexToStringBadInputs (0 ms)\n[----------] 2 tests from LdbCmdTest (1 ms total)\n\n[----------] Global test environment tear-down\n[==========] 2 tests from 1 test case ran. (2 ms total)\n[  PASSED  ] 1 test.\n[  FAILED  ] 1 test, listed below:\n[  FAILED  ] LdbCmdTest.HexToString\n\n 1 FAILED TEST\n```\n\nSimply exporting\n\n```\nEXTRA_CFLAGS=-fsigned-char EXTRA_CXXFLAGS=-fsigned-char\n```\n\nprior to the build solves the problem. However, given that this is the only\ntest that fails I wonder if the solution is actually to modify how the test\nchecks the result of HexToString?\n",
	"number": 950,
	"title": "Test failure on PPC64LE: ldb_cmd_test requires/assumes signed character type"
}, {
	"body": "The optional  \"std::vector<DbPath> db_paths\" is  supported now?\n",
	"number": 948,
	"title": "The optional  \"std::vector<DbPath> db_paths\" is  supported now?"
}, {
	"body": "Hi.\n\nI'm trying to migrate our system from LevelDB to RocksDB.\n\nThe development environment is VS2015 and the current RocksDB code is from commit 1477dcb.\n\nWhenever I try to read old data, I get an assertion from this line:\n\n[assert(smallest_seqno <= largest_seqno);](https://github.com/facebook/rocksdb/blob/1477dcb37d0b60c8e242e35847ee32bcfe84f6f4/db/version_edit.h#L188)\n\nCall Stack:\n\n```\nlib.dll!rocksdb::VersionEdit::AddFile(int level, unsigned __int64 file, unsigned int file_path_id, unsigned __int64 file_size, const rocksdb::InternalKey & smallest, const rocksdb::InternalKey & largest, const unsigned __int64 & smallest_seqno, const unsigned __int64 & largest_seqno, bool marked_for_compaction) Line 188   C++\nlib.dll!rocksdb::VersionSet::WriteSnapshot(rocksdb::log::Writer * log) Line 3047    C++\nlib.dll!rocksdb::VersionSet::LogAndApply(rocksdb::ColumnFamilyData * column_family_data, const rocksdb::MutableCFOptions & mutable_cf_options, rocksdb::VersionEdit * edit, rocksdb::InstrumentedMutex * mu, rocksdb::Directory * db_directory, bool new_descriptor_log, const rocksdb::ColumnFamilyOptions * new_cf_options) Line 2165 C++\nlib.dll!rocksdb::DBImpl::RecoverLogFiles(const std::vector<unsigned __int64,std::allocator<unsigned __int64> > & log_numbers, unsigned __int64 * max_sequence, bool read_only) Line 1359    C++\nlib.dll!rocksdb::DBImpl::Recover(const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, bool read_only, bool error_if_log_file_exist) Line 1026 C++\nlib.dll!rocksdb::DB::Open(const rocksdb::DBOptions & db_options, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & dbname, const std::vector<rocksdb::ColumnFamilyDescriptor,std::allocator<rocksdb::ColumnFamilyDescriptor> > & column_families, std::vector<rocksdb::ColumnFamilyHandle *,std::allocator<rocksdb::ColumnFamilyHandle *> > * handles, rocksdb::DB * * dbptr) Line 5300  C++\nlib.dll!rocksdb::DB::Open(const rocksdb::Options & options, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & dbname, rocksdb::DB * * dbptr) Line 5227   C++\n```\n\nThe constructor of `FileMetaData` initializes those members with initial data in order to be replaced later by actual data; but it seems that it doesn't happen when reading old log files.\nHowever, the code still arrives to a function which asserts they are exists and set correctly.\n\nAm I missing anything?\n",
	"number": 946,
	"title": "Assertion when opening old LevelDB files - Windows"
}, {
	"body": "I am trying to install RocksDB in Ubuntu 15.04 but i am getting the below error??? \n\nRocksDB/rocksdb-4.1# make all\n  GEN      util/build_version.cc\n  CC       util/crc32c.o\nutil/crc32c.cc: In function \u2018void rocksdb::crc32c::Fast_CRC32(uint64_t_, const uint8_t__)\u2019:\nutil/crc32c.cc:319:39: error: \u2018_mm_crc32_u64\u2019 was not declared in this scope\n   *l = _mm_crc32_u64(_l, LE_LOAD64(_p));\n                                       ^\nMakefile:1133: recipe for target 'util/crc32c.o' failed\nmake: *_\\* [util/crc32c.o] Error 1\n\nHelp me to get out of this error to install Rocksdb\n",
	"number": 944,
	"title": "Error in RocksDB Installation"
}, {
	"body": "2015/11/24-23:00:48.341789 ffecc40080 [WARN] ------- DUMPING STATS -------\n2015/11/24-23:00:48.341930 ffecc40080 [WARN]\n*\\* Compaction Stats [default] **\n## Level    Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) Stall(cnt)  KeyIn KeyDrop\n\n  L0     59/32        29   6.8      0.0     0.0      0.0     102.6    102.6       0.0   0.0      0.0      4.4     23990     99566    0.241      19946       0      0\n  L1     33/24       307   0.3   1646.0   102.6   1543.4    1642.7     99.2       0.0  16.0      6.5      6.5    260502     19141   13.610          0    132M  3438K\n  L2    220/0       1147   1.0     48.8    17.1     31.7      48.1     16.4      81.8   2.8      6.6      6.5      7545      1460    5.168          0     18M   959K\n  L3   1289/0      13813   1.0     71.9    34.5     37.5      71.2     33.8      62.6   2.1      6.7      6.6     11042      1864    5.924          0     44M   915K\n  L4   5870/0      82686   0.0    122.9    63.0     59.9     120.7     60.9      19.9   1.9      6.6      6.5     18999      2611    7.276          0     81M  2790K\nSum   7471/56     97981   0.0   1889.6   217.2   1672.5    1985.3    312.9     164.3  19.3      6.0      6.3    322078    124642    2.584      19946    278M  8103K\nInt      0/0          0   0.0     16.9     1.7     15.2      17.0      1.9       0.6  39.6      3.5      3.5      4960      1029    4.820        654   2016K   315K\nFlush(GB): cumulative 102.611, interval 0.430\nStalls(count): 19437 level0_slowdown, 509 level0_numfiles, 0 memtable_compaction, 0 leveln_slowdown_soft, 0 leveln_slowdown_hard\n\nBased on the above stats, we are seeing a total of 56 threads currently doing compaction across all the levels.\nWe have set max_background_compactions = 4.\nCan you clarify how is max_background_compactions taken into account ? \nThanks!\n",
	"number": 939,
	"title": "Total threads in the DB doing concurrent compaction exceed Max Background Compactions value"
}, {
	"body": "The slice concept is extremely powerful and significant performance boost - to the point the c++ standard libraries will include soon std::string_view http://en.cppreference.com/w/cpp/experimental/basic_string_view. Most of the compilers already offer an experimental version of it and more and more projects commit to it.\n\nWhat about adding support for stl string_view, as part of the current interfaces or even migrate Slice to match the standard and use the stl version directly when available.\n",
	"number": 936,
	"title": "support for std::experimental::string_view"
}, {
	"body": "I tried to build master branch of rocksdb on Windows 7.\nI think these files should not be included in release build:\n\n```\ntable/mock_table.cc\nutil/mock_env.cc\nutil/testharness.cc\nutil/testutil.cc\n```\n",
	"number": 935,
	"title": "Unexpected mock and test support files in Windows release build"
}, {
	"body": "I tried to build master branch of rocksdb on Windows 7.\nI found that \"tools/ldb_cmd.cc\" denpends on \"tools/sst_dump_tool_imp.h\", \nand \"tools/sst_dump_tool_imp.h\" depends on \"util/testharness.h\", \nand \"util/testharness.h\" depends on 'gtest/gtest.h'.\n\nI just tried to build a release version.\nI think it should not denpend on 'gtest/gtest.h'.\nI just comment out #include \"util/testharness.h\" in \"tools/sst_dump_tool_imp.h\", and found the compilation is OK.\nAm I do it right?\n",
	"number": 934,
	"title": "Unexpected 'gtest/gtest.h' dependency in release build"
}, {
	"body": "",
	"number": 933,
	"title": "Why ColumnFamilyOptions.compaction_filter is not a std::shared_ptr like others ?"
}, {
	"body": "Rocksdb open call aborts when DB is full. Looks like DB open is trying to dump the configuration to LOG file and fails with no space. DB should be allowed to be opened even if there is no space left on device. Looks like a bug. \nComments?\n\nStack:\n\n```\nProgram terminated with signal 6, Aborted.\n#0 0x0000007b6d6d8568 in __GI_raise (sig=<optimized out>) at ../nptl/sysdeps/unix/sysv/linux/raise.c:66\n66 ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.\n(gdb) bt\n#0 0x0000007b6d6d8568 in __GI_raise (sig=<optimized out>) at ../nptl/sysdeps/unix/sysv/linux/raise.c:66\n#1 0x0000007b6d6d9fc4 in __GI_abort () at abort.c:90\n#2 0x0000007b6d6d0450 in __assert_fail_base (fmt=0x7b6d7efd00 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=0x7b6e232930 \"sz == write_size\",\nfile=0x7b6e232918 \"./util/posix_logger.h\", line=<optimized out>, function=<optimized out>) at assert.c:92\n#3 0x0000007b6d6d04fc in _GI__assert_fail (assertion=0x7b6e232930 \"sz == write_size\", file=0x7b6e232918 \"./util/posix_logger.h\", line=<optimized out>,\nfunction=0x7b6e232c88 \"virtual void rocksdb::PosixLogger::Logv(const char*, va_list)\") at assert.c:101\n#4 0x0000007b6e154414 in ?? () from /usr/lib64/librocksdb.so.3.11\n#5 0x0000007b6e13db8c in ?? () from /usr/lib64/librocksdb.so.3.11\n#6 0x0000007b6e143edc in rocksdb::Log(rocksdb::Logger*, char const*, ...) () from /usr/lib64/librocksdb.so.3.11\n#7 0x0000007b6e1e3c8c in rocksdb::DBOptions::Dump(rocksdb::Logger*) const () from /usr/lib64/librocksdb.so.3.11\n#8 0x0000007b6e0649e8 in rocksdb::DBImpl::DBImpl(rocksdb::DBOptions const&, std::string const&) () from /usr/lib64/librocksdb.so.3.11\n#9 0x0000007b6e0711b4 in rocksdb::DB::Open(rocksdb::DBOptions const&, std::string const&, std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> >, rocksdb::DB*) () from /usr/lib64/librocksdb.so.3.11\n#10 0x0000007b6e072208 in rocksdb::DB::Open(rocksdb::Options const&, std::string const&, rocksdb::DB**) () from /usr/lib64/librocksdb.so.3.11\n#11 0x0000007b6e01de98 in rocksdb_open () from /usr/lib64/librocksdb.so.3.11\n```\n\nCode Base: rocksdb 3.11.0\n\nThanks !\n",
	"number": 931,
	"title": "Rocksdb open call aborts when DB is full"
}, {
	"body": "I am trying to limit the size of the Database using the rocksdb options.\nFollowing is the configuration:\nNo. of Memtables = 4\nSize of memtable = 4M\nNo. of Levels = 4\nTarget file size base = 4M\nTarget file size multiplier = 1\nMax bytes for level base = 32M\nMax bytes for level multiplier = 4\nlevel0_file_num_compaction_trigger = 4\nlevel0_slowdown_writes_trigger = 48\nlevel0_stop_writes_trigger = 64\nBlock cache size = 128MB\nCompression = No\nBlock Size = 16KB\n\nBased on this, total size of my Database should not exceed 1GB.\nI am observing that DB grows beyond this limit and uses the entire partition.\nIn this experiment, a partition of 2GB has been given to DB and I am trying to limit the DB size to 1GB.\nAny suggestions on how to achieve this?  Is this a bug?\n\nNOTE: rocskdb version 3.13.0 is used for this experiment.\n\nThanks!\n",
	"number": 930,
	"title": "Number of Levels does not limit the size of the RocksDB ?"
}, {
	"body": "After inserting about a billion keys (rough estimate), I am hitting the maximum file size supported by `PlainTableReader`. In my logs it says:\n\n``` json\n2016/01/06-22:41:30.390947 7f52427fc700 [WARN] Compaction error: Not implemented: File is too large for PlainTableReader!\n2016/01/06-22:41:30.390964 7f52427fc700 (Original Log Time 2016/01/06-22:41:30.390396) [default] compacted to: files[10 0 0 0 0 0 2264] max score 2.00, MB/sec: 5.3 rd, 5.3 wr, level\n 0, files in(0, 3) out(1) MB in(0.0, 2177.6) out(2160.9), read-write-amplify(inf) write-amplify(inf) Not implemented: File is too large for PlainTableReader!, records in: 103049259,\n records dropped: 0\n2016/01/06-22:41:30.390968 7f52427fc700 (Original Log Time 2016/01/06-22:41:30.390462) EVENT_LOG_v1 {\"time_micros\": 1452120090390429, \"job\": 18890, \"event\": \"compaction_finished\", \"\noutput_level\": 0, \"num_output_files\": 1, \"total_output_size\": 2265820389, \"num_input_records\": 102019867, \"num_output_records\": 102019867, \"lsm_state\": [10, 0, 0, 0, 0, 0, 2264]}\n2016/01/06-22:41:30.390974 7f52427fc700 [ERROR] Waiting after background compaction error: Not implemented: File is too large for PlainTableReader!, Accumulated background error counts: 1\n```\n\nThe part `\"total_output_size\": 2265820389` already shows the error - `PlainTableIndex::kMaxFileSize` is 2Gb (minus 1 byte), `total_output_size` is about 2.1Gb. So far so clear.\n\nWhat's not clear is why this error happens. I used the configuration values from https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide#in-memory-prefix-database, with some parameters from https://github.com/facebook/rocksdb/wiki/RocksDB-In-Memory-Workload-Performance-Benchmarks#test-1-point-lookup :\n\n``` cpp\n  // Plaintable\n  rocksdb::PlainTableOptions plain_table_options;\n  plain_table_options.user_key_len = 8;\n  plain_table_options.bloom_bits_per_key = 10;\n  plain_table_options.hash_table_ratio = 0.75;\n  options.table_factory.reset(rocksdb::NewPlainTableFactory(plain_table_options));\n\n  options.allow_mmap_reads = true;\n  options.allow_mmap_writes = false;\n  options.compression = rocksdb::kNoCompression;\n\n  // create prefix extractor over entire key\n  options.prefix_extractor.reset(rocksdb::NewFixedPrefixTransform(8));\n\n  // Use hash link list memtable to change binary search to hash lookup in mem table\n  // in write mode, use vector memtable\n  if(mode == Mode::WriteOnly) {\n    options.memtable_factory.reset(new rocksdb::VectorRepFactory);    \n  } else {\n    options.memtable_factory.reset(rocksdb::NewHashSkipListRepFactory(1024*1024));\n  }\n\n  // Enable bloom filter for hash table to reduce memory accesses (usually means\n  // CPU cache misses) when reading from mem table to one, for the case where key\n  // is not found in mem tables\n  options.memtable_prefix_bloom_bits = 10000000;\n  options.memtable_prefix_bloom_probes = 6;\n\n  // Tune compaction so that, a full compaction is kicked off as soon as we have\n  // two files. We hack the parameter of universal compaction:\n  options.compaction_style = rocksdb::kCompactionStyleUniversal;\n  options.compaction_options_universal.size_ratio = 10;\n  options.compaction_options_universal.min_merge_width = 2;\n  options.compaction_options_universal.max_size_amplification_percent = 1;\n  options.level0_file_num_compaction_trigger = 1;\n  options.level0_slowdown_writes_trigger = 16;\n  options.level0_stop_writes_trigger = 24;\n\n  // Tune bloom filter to minimize memory accesses:\n  options.bloom_locality = 1;\n\n  // Use one mem table at one time. Its size is determined by the full compaction\n  // interval we want to pay. We tune compaction such that after every flush, a\n  // full compaction will be triggered, which costs CPU. The larger the mem table\n  // size, the longer the compaction interval will be, and at the same time, we\n  // see less memory efficiency, worse query performance and longer recovery time\n  // when restarting the DB.\n  options.write_buffer_size = 32 << 20;  // 32Mb\n  options.max_write_buffer_number = 2;\n  options.min_write_buffer_number_to_merge = 1;\n\n  // Multiple DBs sharing the same compaction pool of `num_threads`.\n  // we have one thread for messaging, one thread for flushing, and\n  // at least one thread for compacting. If num_threads > 3, put all \n  // remaining threads into compaction.\n  int num_background_threads = num_threads > 3 ? num_threads - 2 : 1;\n  options.max_background_compactions = num_background_threads;\n  options.max_background_flushes = 1;\n  options.env->SetBackgroundThreads(1, rocksdb::Env::Priority::HIGH);\n  options.env->SetBackgroundThreads(num_background_threads, rocksdb::Env::Priority::LOW);\n\n  // Settings for WAL logs:\n  options.disableDataSync = 1;\n  options.bytes_per_sync = 2 << 20;\n```\n\n(yes, this is the same configuration as in my other bug report today).\n\nIf it's something trivial, I would suggest adding this information to the documentation.\n",
	"number": 927,
	"title": "hitting PlainTableIndex::kMaxFileSize - but reason for that is not obvious"
}, {
	"body": "```\n  // Current Requirements:\n  // (1) Memtable is empty.\n  // (2) All existing files (if any) have sequence number = 0.\n  // (3) Key range in loaded table file don't overlap with existing\n  //     files key ranges.\n  // (4) No other writes happen during AddFile call, otherwise\n  //     DB may get corrupted.\n  // (5) Database have at least 2 levels.\n```\n\n items 1 and 4 apply to the destination column family, not the whole db, correct?\n",
	"number": 925,
	"title": "AddFile documentation clarification"
}, {
	"body": "The merge_operator.h states that the \"new_value\" parameter passed to MergeOperator::FullMerge interface would be a empty string. But DBIter does not follow the rule:\n\n``` c++\n// db_iter:562 DBIter::FindValueForCurrentKeyUsingSeek()\nuser_merge_operator_->FullMerge(saved_key_.GetKey(), nullptr, operands, &saved_value_, logger_);\n```\n\nThe \"saved_value_\" is sometimes not empty while iterating the db. Consequence is that the value() returned from iterator maybe wrong.\n\nCodes below shows how data could be inconsistent.\n[tests.tar.gz](https://github.com/facebook/rocksdb/files/79396/tests.tar.gz)\ntest output:\n\n```\nRunning main() from gtest_main.cc\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from BytesPendingMerger\n[ RUN      ] BytesPendingMerger.Merge\nBytesPendingMerger_unittest.cpp:65: Failure\nValue of: it->value().ToString()\n  Actual: \"abccddefthebrownfoxjumps=.=\"\nExpected: util::str::Join(ikey->second.begin(), ikey->second.end(), \"\")\nWhich is: \"thebrownfoxjumps=.=\"\nkey2\nBytesPendingMerger_unittest.cpp:65: Failure\nValue of: it->value().ToString()\n  Actual: \"abccddefthebrownfoxjumps=.=12233\"\nExpected: util::str::Join(ikey->second.begin(), ikey->second.end(), \"\")\nWhich is: \"12233\"\nkey1\n[  FAILED  ] BytesPendingMerger.Merge (30 ms)\n[----------] 1 test from BytesPendingMerger (30 ms total)\n\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (30 ms total)\n[  PASSED  ] 0 tests.\n[  FAILED  ] 1 test, listed below:\n[  FAILED  ] BytesPendingMerger.Merge\n```\n",
	"number": 920,
	"title": "FullMerge invoked with non-empty \"new_value\" parameter"
}, {
	"body": "Hi,\n\nI'm writing a clojure wrapper for RocksJava, and am trying to automatically release snapshots in a finalizer with `releaseSnapshot`. However this causes the jvm to segfault reproducibly.\nReleasing the snapshot outside the finalizer works fine.\nI've tested this on rocksdbjni 3.13.1 and 4.0, both have the same problem.\nThe system this is running on is OS X.\nDoes anybody have any ideas or pointers what is happening here?\n\nI attached the segfault log.\n[hs_err_pid50805.log.txt](https://github.com/facebook/rocksdb/files/74804/hs_err_pid50805.log.txt)\n",
	"number": 912,
	"title": "[RocksJava] Segfault when calling releaseSnapshot from finalizer."
}, {
	"body": "Unless I'm missing something, the new \"Full\" bloom filter doesn't seem to be available from the C API.  `NewBloomFilterPolicy(int)` was extended with a `bool use_block_based_builder = true` argument, but the C API still has only `rocksdb_filterpolicy_create_bloom(int)`.\n\nI can submit a pull request for this.  My strategy will be to create another C function for creating full bloom filters (so as not to break compatibility with clients).  Let me know if there's a more preferred way.\n",
	"number": 909,
	"title": "New \"Full\" Bloom Filter Format not createable from C API"
}, {
	"body": "I left a debian folder containing the needed files to generate a librocksdb4 and a librocksdb4-dev packages.\n",
	"number": 905,
	"title": "Added Debian package build dir"
}, {
	"body": "hi.\nToday, I find my disk used 90%.So I use \"delete\" API to delete old dates.I only delete 30 days ago dates.After 9 hours \uff0cmy program core dump. When it restart ,rocksdb open fail.I find \"sst\" files lost.If sst file's num  is more than max_open_files,  rocksdb will delete db?\n\nLOG has error info:\n\n2015/12/28-14:44:05.055091 7f52b51b7700 [ERROR] [JOB 2534] Failed to delete /media/ssd/sentryd/dataV3/171720.sst type=2 #171720 -- IO error: /media/ssd/sentryd/dataV3/171720.sst: No such file or directory\n2015/12/28-14:44:05.055110 7f52b51b7700 EVENT_LOG_v1 {\"time_micros\": 1451285045055104, \"job\": 2534, \"event\": \"table_file_deletion\", \"file_number\": 171720, \"status\": \"IO error: /media/ssd/sentryd/dataV3/171720.sst: No such file or directory\"}\n2015/12/28-14:44:05.059226 7f52b51b7700 [ERROR] [JOB 2534] Failed to delete /media/ssd/sentryd/dataV3/171719.sst type=2 #171719 -- IO error: /media/ssd/sentryd/dataV3/171719.sst: No such file or directory\n2015/12/28-14:44:05.059248 7f52b51b7700 EVENT_LOG_v1 {\"time_micros\": 1451285045059241, \"job\": 2534, \"event\": \"table_file_deletion\", \"file_number\": 171719, \"status\": \"IO error: /media/ssd/sentryd/dataV3/171719.sst: No such file or directory\"}\n2015/12/28-14:44:05.063744 7f52b51b7700 [ERROR] [JOB 2534] Failed to delete /media/ssd/sentryd/dataV3/171718.sst type=2 #171718 -- IO error: /media/ssd/sentryd/dataV3/171718.sst: No such file or directory\n",
	"number": 904,
	"title": "Data loss"
}, {
	"body": "https://ci.appveyor.com/project/Facebook/rocksdb/build/1.0.389\n\nIt passes though it shows a test failure:\n\nDBTest.HardLimit State: Completed\nNote: Google Test filter = DBTest.HardLimit\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from DBTest\n[ RUN      ] DBTest.HardLimit\nc:\\projects\\rocksdb\\db\\db_test.cc(9153): error: Expected: (callback_count.load()) >= (1), actual: 0 vs 1\n[  FAILED  ] DBTest.HardLimit (3719 ms)\n[----------] 1 test from DBTest (3719 ms total)\n\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (3719 ms total)\n[  PASSED  ] 0 tests.\n[  FAILED  ] 1 test, listed below:\n[  FAILED  ] DBTest.HardLimit\n\n 1 FAILED TEST\n\n@yuslepukhin any idea why?\n",
	"number": 901,
	"title": "Appveyor CI passes even when there is a unit test failure"
}, {
	"body": "It would be nice if RocksDB came with a default merge operator for appending bytes,\nanalogous to `stringappend`.\n\nA common use case for a merge operation is simply storing additional values under that key.\nAt the same time many binary serialisation formats can read object collections when their byte-strings are simply concatenated (to support streaming applications).\n\n`stringappend` almost handles this case, but always inserts a separating character (\",\" by default).\nSo introducing this would merely require a copy of `stringappend` with the line introducing the delimiter removed and the names updated.\n\nI know that it seems trivial to implement this on an application level, however when using RocksJava, one has to jump through quite a few hoops to do this.\n\nI'd be willing to create a patch for this.\n\nThanks :), JP\n",
	"number": 891,
	"title": "Additional default merge operator 'byteappend'."
}, {
	"body": "Here is the error message. \nput or merge error: Not implemented: Provide a merge_operator when opening DB\ntcmalloc: large alloc 1100156043264 bytes == (nil) @  0x3fffb5709754 0x3fffb564bb14 0x3fffb564d4f8 0x3fffb564e33c 0x101dde44 0x101de084 0x101de57c 0x101acfd8 0x100c6ccc 0x100c8814 0x100cad38 0x100f7fa0 0x10105d18 0x10219764 0x10219920 0x3fffb57f7fe8 0x3fffb53651e4\nterminate called after throwing an instance of 'std::bad_alloc'\n  what():  std::bad_alloc\nReceived signal 6 (Aborted)\n\nI used all default setting for db_bench.\nLevelDB:    version 4.3\nKeys:       16 bytes each\nValues:     100 bytes each (50 bytes after compression)\nEntries:    1000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    110.6 MB (estimated)\nFileSize:   62.9 MB (estimated)\nWrites per second: 0\nCompression: Snappy\nMemtablerep: skip_list\nPerf Level: 0\n",
	"number": 888,
	"title": "Got alloc errors(too large allocation 1TB) when doing db_bench seekrandomwhilemerging test"
}, {
	"body": "I am new to this community, wanted  to try out different stuff using rocksdb. I downloaded the rocksdb package and have installed it on my centos 6. I was successfully able to provide all these commands.\n- make all\n  - make share_lib\n  - make static_lib\n    but all these produce \".0\" files or  \".so \" or \".a\" files.  I am unsure as to how to execute or run. \n    I wanted to tinker around so thought of looking into the examples. I am not sure how to run the c example and execute the same. There is no documentation as to how to run compile these and run these examples. \n    Any insights on this is greatly appreciated. \n",
	"number": 887,
	"title": "Compiling and running c_sample program."
}, {
	"body": "Using the following options:\n\n``` cpp\n  options_.info_log_level = rocksdb::ERROR_LEVEL;\n  options_.log_file_time_to_roll = 0;\n  options_.keep_log_file_num = 10;\n  options_.max_log_file_size = 1024 * 1024 * 1;\n  options_.stats_dump_period_sec = 0;\n```\n\nI try to limit the logfile size. With an update from 3.10 to 4.1 (release) the LOG includes the options/column options dump.\n\nThe 3.10 implementation used `Log(Logger* info_log, const char* format, ...)` while the 4.1 implementation uses `Header(Logger* info_log, const char* format, ...)`.\n\nThe change occurred by @yhchiang: https://github.com/facebook/rocksdb/commit/f21c7415a7c77edf1158fc8503c41ca6a04696f7\n\nFrom `./util/env.cc`:\n\n``` cpp\nvoid Log(Logger* info_log, const char* format, ...) {\n  if (info_log && info_log->GetInfoLogLevel() <= InfoLogLevel::INFO_LEVEL) {\n    va_list ap;\n    va_start(ap, format);\n    info_log->Logv(InfoLogLevel::INFO_LEVEL, format, ap);\n    va_end(ap);\n  }\n}\n\n[...]\n\nvoid Header(Logger* info_log, const char* format, ...) {\n  if (info_log) {\n    va_list ap;\n    va_start(ap, format);\n    info_log->LogHeader(format, ap);\n    va_end(ap);\n  }\n}\n```\n\nWhen moving from pre 3.10 I noticed this went from INFO to WARN. What is the thought behind escalating these verbose logs? And is there a way to completely remove them in the 4.1 release?\n\nThey should be reverted back to using `Log`.\n",
	"number": 884,
	"title": "With 4.x LOG files include frequent options Dump (leads to large LOG files)"
}, {
	"body": "rocksdb 4.1\nos mac 10.11.1\n\nar: creating archive librocksdb.a\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(xfunc.o) has no symbols\n",
	"number": 875,
	"title": "ar: creating archive librocksdb.a /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(xfunc.o) has no symbols"
}, {
	"body": "I'm hitting the following assertion in `DBImpl::MarkLogsSynced()`:\n    `assert(logs_.empty() || (logs_.size() == 1 && !logs_[0].getting_synced));`\n\nThis appears to be due to a `DBImpl::SyncWAL()` call that sets exactly 0 logs to `getting_synced = true`, because the loop breaks immediately from condition `it->number <= current_log_number` failing. The mutex is unlocked, during which time another `DBImpl::SyncWAL()` is called that marks the first entry with `getting_synced = true`. The first call reacquires the mutex, then enters `DBImple::MarkLogSynced()`, tripping on the assertion.\n\nI would appreciate someone verifying the scenario is valid and performing the appropriate fix.\n",
	"number": 872,
	"title": "Bad assertion in `DBImpl::MarkLogsSynced()`"
}, {
	"body": "",
	"number": 870,
	"title": "avoid unnecessary switch of memtable when do flush"
}, {
	"body": "The Windows porting uses certain API which is only available on Windows 8 or later.\n\nGetSystemTimePreciseAsFileTime is one of such APIs.\n\nI am not sure if there are other such APIs too...\n\nCan we eliminate using of such kind of APIs so the rockdb.dll can be used on Windows 7 too?\n",
	"number": 869,
	"title": "Windows 7 support"
}, {
	"body": "Adding the following asserts makes the second one fail in reduce_levels_test\n\nreduce_levels_test_je State: Completed\n[==========] Running 3 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 3 tests from ReduceLevelTest\n[ RUN      ] ReduceLevelTest.Last_Level\nreduce_levels_test_je.exe : Assertion failed: level < num_levels_, file c:\\dev\\rocksdb\\rocksdb\\db\\version_set.h, line 227\n\n  const std::vector<FileMetaData*>& LevelFiles(int level) const {\n    assert(level >= 0);\n    assert(level < num_levels_);\n    assert(files_ != nullptr);\n     return files_[level];\n   }\n",
	"number": 864,
	"title": "Adding assertion for the LevelFiles(int level) reveales out of bounds access "
}, {
	"body": "Hi.I use the interface Merge operation. I set TTL is 10 days and open database with \"DBWithTTL::Open\". But I find datas not be deleted 10 days later.   I don't  know why and how to do. My email is \"bingfeng198878@163.com\". Please help me.   Thank you!\n",
	"number": 861,
	"title": "TTL and Merge use"
}, {
	"body": "hi, i want to know what does disable_delete_obsolete_files_ exactly mean?\n\ncase is :\n1. set WAL_ttl_seconds  = 6000\n2. turn options_.delete_scheduler on\n\nif i set disable_delete_obsolete_files_  = true, then rocksdb will not delete obsolete file\uff08LOG.old_, archieve/_, trash sstable in some dir\uff09 any more? is this right?\n",
	"number": 859,
	"title": "disable_delete_obsolete_files_, what does it exactly mean?"
}, {
	"body": "After calling `BlockBasedTable::GenerateCachePrefix`, `compressed_cache_key_prefix_size` may be changed to 30. \n\n``` c++\n    BlockBasedTable::GenerateCachePrefix(\n        table_options.block_cache_compressed.get(), file->writable_file(),\n        &rep_->compressed_cache_key_prefix[0],\n        &rep_->compressed_cache_key_prefix_size);\n  }\n```\n\nAnd in `BlockBasedTableBuilder::InsertBlockInCache`, trys to write an `uint64_t`  without checking, this may cause buffer overflow.\n\n``` c++\n    char* end = EncodeVarint64(\n                  r->compressed_cache_key_prefix +\n                  r->compressed_cache_key_prefix_size,\n                  handle->offset());\n```\n",
	"number": 854,
	"title": "Write to compressed_cache_key_prefix in InsertBlockInCache may cause overflow"
}, {
	"body": "## Why my librocksdb.so file size is 60MB?it's too big,how to optimization the size\uff1fthanks!\n\n```\n-rw-r--r--  1 root root 196853756 Nov 21 06:55 librocksdb.a\nlrwxrwxrwx  1 root root        19 Nov 21 07:00 librocksdb.so -> librocksdb.so.4.1.0\nlrwxrwxrwx  1 root root        19 Nov 21 07:00 librocksdb.so.4 -> librocksdb.so.4.1.0\nlrwxrwxrwx  1 root root        19 Nov 21 07:00 librocksdb.so.4.1 -> librocksdb.so.4.1.0\n-rwxr-xr-x  1 root root  60865048 Nov 21 07:00 librocksdb.so.4.1.0\n```\n",
	"number": 852,
	"title": "Why my librocksdb.so file size is 60MB?it's too big,how to optimization the size\uff1f"
}, {
	"body": "## How can I delete the LOG.old.\\* files?\n## thank you!\n\n```\n-rw-r--r-- 1 root root   434319 Nov 20 07:59 116280.sst\n-rw-r--r-- 1 root root   432979 Nov 20 07:59 116282.sst\n-rw-r--r-- 1 root root   434843 Nov 20 07:59 116284.sst\n-rw-r--r-- 1 root root   433350 Nov 20 07:59 116286.sst\n-rw-r--r-- 1 root root  3487174 Nov 20 07:59 116287.log\n-rw-r--r-- 1 root root   434405 Nov 20 07:59 116288.sst\n-rw-r--r-- 1 root root       16 Nov 20 07:41 CURRENT\n-rw-r--r-- 1 root root       37 Nov 20 03:40 IDENTITY\n-rw-r--r-- 1 root root        0 Nov 20 03:40 LOCK\n-rw-r--r-- 1 root root 19358719 Nov 20 07:59 LOG\n-rw-r--r-- 1 root root  3795394 Nov 20 03:45 LOG.old.1447991334341079\n-rw-r--r-- 1 root root 17880614 Nov 20 04:13 LOG.old.1447993690696363\n-rw-r--r-- 1 root root 45883143 Nov 20 05:20 LOG.old.1447997019485462\n-rw-r--r-- 1 root root  7028662 Nov 20 05:29 LOG.old.1448002770120232\n-rw-r--r-- 1 root root 39349306 Nov 20 07:34 LOG.old.1448004850881430\n-rw-r--r-- 1 root root  1910508 Nov 20 07:41 LOG.old.1448005278376423\n-rw-r--r-- 1 root root  1018567 Nov 20 07:59 MANIFEST-099716\n```\n",
	"number": 849,
	"title": "How can I delete the LOG.old.1447991334341079 file?"
}, {
	"body": "in my case\uff0cif turn options_.rate_limiter on, when close rocksdb by kill -9\uff0crocksdb can not start again\uff0cit is hanged~~~\ngdb attach the process, find the stack like that:\n#0  0x00007f9b785d1a0e in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x0000000000586412 in rocksdb::port::CondVar::TimedWait (this=Unhandled dwarf expression opcode 0xf3\n\n) at port/port_posix.cc:100\n#2  0x0000000000501dc3 in rocksdb::GenericRateLimiter::Request (this=0x18fff70, bytes=Unhandled dwarf expression opcode 0xf3\n\n) at util/rate_limiter.cc:107\n#3  0x00000000004e7953 in rocksdb::WritableFileWriter::RequestToken (this=0x190d7d0, bytes=16343) at util/file_reader_writer.cc:223\n#4  0x00000000004e7a47 in rocksdb::WritableFileWriter::Flush (this=0x190d7d0) at util/file_reader_writer.cc:120\n#5  0x0000000000588849 in rocksdb::BlockBasedTableBuilder::Flush (this=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_builder.cc:594\n#6  0x0000000000588a00 in rocksdb::BlockBasedTableBuilder::Add (this=0x190d9f0, key=..., value=...) at table/block_based_table_builder.cc:556\n#7  0x000000000059810f in rocksdb::BuildTable (dbname=Unhandled dwarf expression opcode 0xf3\n\n) at db/builder.cc:100\n#8  0x0000000000531515 in rocksdb::DBImpl::WriteLevel0TableForRecovery (this=0x1903230, job_id=1, cfd=0x1909030, mem=Unhandled dwarf expression opcode 0xf3\n\n) at db/db_impl.cc:1305\n#9  0x0000000000532dc0 in rocksdb::DBImpl::RecoverLogFiles (this=Unhandled dwarf expression opcode 0xf3\n\n) at db/db_impl.cc:1179\n#10 0x00000000005339e6 in rocksdb::DBImpl::Recover (this=0x1903230, column_families=Unhandled dwarf expression opcode 0xf3\n\n) at db/db_impl.cc:993\n#11 0x0000000000534a2e in rocksdb::DB::Open (db_options=..., dbname=\"/home/wangfaqiang/testdb\",\n\n```\ncolumn_families=std::vector of length 1, capacity 1 = {...}, handles=0x7ffcd956b110, dbptr=0x7ffcd956afd8) at db/db_impl.cc:4402\n```\n#12 0x00000000004eb5d5 in rocksdb::DBWithTTL::Open (db_options=..., dbname=\"/home/wangfaqiang/testdb\", column_families=Unhandled dwarf expression opcode 0xf3\n\ni have to clear all files which in db dir to start rocksdb~~~\nis there anyway i can solve this problem?\n",
	"number": 847,
	"title": "close rocksdb by kill -9\uff0cand rocksdb can not start again!!"
}, {
	"body": "I am wondering if even when we set new columns using the column family options, the default column  is still existing on a newly created db, and if we can use it. The doc is not clear about it.\n",
	"number": 845,
	"title": "does the default column co-exists with others?"
}, {
	"body": "Currently the C-API does not allow a caller to pass custom CompactRangeOptions. The signature of rocksdb_compact_range is \n\n``` c\nextern ROCKSDB_LIBRARY_API void rocksdb_compact_range(rocksdb_t* db,\n                                                      const char* start_key,\n                                                      size_t start_key_len,\n                                                      const char* limit_key,\n                                                      size_t limit_key_len);\n\nextern ROCKSDB_LIBRARY_API void rocksdb_compact_range_cf(\n    rocksdb_t* db, rocksdb_column_family_handle_t* column_family,\n    const char* start_key, size_t start_key_len, const char* limit_key,\n    size_t limit_key_len);\n```\n",
	"number": 840,
	"title": "C API: Expose CompactRangeOptions and allow rocksdb_compact_range to accept options"
}, {
	"body": "Hi,\ndoes rocksdb using cpplint?\n\nI just run it for one file and got:\n\n```\n\\rocksdb\\util\\auto_roll_logger_test.cc(8):  Include \"cmath\" not in alphabetical order  [build/include_alpha] [4]\n\\rocksdb\\util\\auto_roll_logger_test.cc(16):  Found C system header after other header. Should be: auto_roll_logger_test.h, c system, c++ system, other.  [build/include_order] [4]\n\\rocksdb\\util\\auto_roll_logger_test.cc(19):  Do not use namespace using-directives.  Use using-declarations instead.  [build/namespaces] [5]\n\\rocksdb\\util\\auto_roll_logger_test.cc(36):  Consider using ASSERT_EQ instead of ASSERT_TRUE(a == b)  [readability/check] [2]\n\\rocksdb\\util\\auto_roll_logger_test.cc(212):  Extra space after ( in function call  [whitespace/parens] [4]\n\\rocksdb\\util\\auto_roll_logger_test.cc(217):  Missing username in TODO; it should look like \"// TODO(my_username): Stuff.\"  [readability/todo] [2]\n\\rocksdb\\util\\auto_roll_logger_test.cc(354):  Redundant blank line at the start of a code block should be deleted.  [whitespace/blank_line] [2]\n```\n\nmay be better use cpplint in pre commit hook?\n",
	"number": 830,
	"title": "cpplint not using"
}, {
	"body": "Some times ,we write bad data into db and need rollback it \u3002\nFor reader,  we can assigned the  snapshot with seqid=200. But evently  the data  with seq between seq 201 and 9800 must be deleted.  Can we  use   compaction filter  to filter them?\n\nkey: type: put seqID: 1 \n\nkey: B type: put seqID: 10000 \nkey: B type: put seqID: 9900<=====  make snapshot with seqID 9900\nkey: B type: put seqID: 9800               ||\n...                                                         bad data  \n                                                            bad data\n                                                            || \n\nkey: B type: put seqID: 300\nkey: B type: put seqID: 200<=====  make snapshot with seqID 200\n... \nkey: B type: put seqID: 2 \n",
	"number": 825,
	"title": "can we delete/filter  a key/recode  by  key-seq?  "
}, {
	"body": "I see the following crash in my application that is using rocksdb 3.10.2\n\n(gdb) info threads\n  Id   Target Id         Frame\n  2    Thread 0x7ff96484db60 (LWP 4427) 0x0000003f12878395 in _int_free () from /lib64/libc.so.6\n- 1    Thread 0x7ff9646a3700 (LWP 4428) 0x0000003f12832625 in raise () from /lib64/libc.so.6\n  (gdb) bt\n  #0  0x0000003f12832625 in raise () from /lib64/libc.so.6\n  #1  0x0000003f12833e05 in abort () from /lib64/libc.so.6\n  #2  0x0000003f1282b74e in __assert_fail_base () from /lib64/libc.so.6\n  #3  0x0000003f1282b810 in __assert_fail () from /lib64/libc.so.6\n  #4  0x00007ff9658c9e82 in rocksdb::CuckooTableBuilder::Add (this=0x7ff95c01a790, key=..., value=...) at table/cuckoo_table_builder.cc:112\n  #5  0x00007ff9658a2add in rocksdb::CompactionJob::ProcessKeyValueCompaction (this=this@entry=0x7ff9646a1ae0, imm_micros=imm_micros@entry=0x7ff9646a1748, input=input@entry=\n  0x7ff95c004630, is_compaction_v2=is_compaction_v2@entry=false) at db/compaction_job.cc:818\n  #6  0x00007ff9658a5d17 in rocksdb::CompactionJob::Run (this=this@entry=0x7ff9646a1ae0) at db/compaction_job.cc:312\n  #7  0x00007ff9658061b6 in rocksdb::DBImpl::BackgroundCompaction (this=this@entry=0xc30d00, madeProgress=madeProgress@entry=0x7ff9646a1f2e,\n  job_context=job_context@entry=0x7ff9646a1f50, log_buffer=log_buffer@entry=0x7ff9646a20b0) at db/db_impl.cc:2282\n  #8  0x00007ff965811a3e in rocksdb::DBImpl::BackgroundCallCompaction (this=this@entry=0xc30d00) at db/db_impl.cc:2014\n  #9  0x00007ff965811ea5 in rocksdb::DBImpl::BGWorkCompaction (db=0xc30d00) at db/db_impl.cc:1883\n  #10 0x00007ff965873f5f in BGThread (thread_id=<optimized out>, this=0xc2f480) at util/env_posix.cc:1696\n  #11 rocksdb::(anonymous namespace)::PosixEnv::ThreadPool::BGThreadWrapper (arg=<optimized out>) at util/env_posix.cc:1720\n  #12 0x0000003fef2079d1 in start_thread () from /lib64/libpthread.so.0\n  #13 0x0000003f128e88fd in clone () from /lib64/libc.so.6\n\nthis is the other thread\n(gdb) bt\n#0  0x0000003f12878395 in _int_free () from /lib64/libc.so.6\n#1  0x00007ff965941979 in _M_dispose (__a=..., this=<optimized out>) at /home/shanbin/gcc-bld/x86_64-redhat-linux/libstdc++-v3/include/bits/basic_string.h:229\n#2  std::string::assign (this=0x7fffe150f540, __str=...) at /home/shanbin/gcc-bld/x86_64-redhat-linux/libstdc++-v3/include/bits/basic_string.tcc:250\n#3  0x00007ff965480c07 in operator= (__str=..., this=<optimized out>)\n\n```\nat /nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_x86_64/tools/gcc/4.1.1/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.1.1/../../../../include/c++/4.1.1/bits/basic_string.h:486\n```\n#4  log4cxx::helpers::CharMessageBuffer::str (this=0x7fffe150f540) at messagebuffer.cpp:75\n#5  0x00007ff96579d2b5 in libnqss::RocksDBDataStore::init (this=0xc2ccc8, identifier=..., userGuids=..., flags=<optimized out>)\n\n```\nat /home/hyadava/git/fb/libnqss/src/datastore/RocksDBDataStore.cc:233\n```\n#6  0x00007ff9657a5284 in libnqss::DataStoreFactory::createDataStore (type=type@entry=libnqss::kInMemLocal, provider=provider@entry=libnqss::kRocksDB, location=..., userGuids=...,\n\n```\nflags=flags@entry=12) at /home/hyadava/git/fb/libnqss/src/datastore/DataStoreFactory.cc:21\n```\n#7  0x00000000004155e7 in main (argc=17, argv=0x7fffe1528e78) at /home/hyadava/git/fb/libnqss/src/tools/RocksDBDataLoader.cc:784\n\nSome information about my application\n\nrocksdb version : 3.10.2\nOS: Linux XXX.XXX.XXX 2.6.32-431.23.3.el6.XXXXX.20140804.x86_64 #1 SMP Mon Aug 4 04:44:59 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\n\nMy application creates 1400 column families. I see this crash if I try to re-insert the same set of records in some of the column families. Is there an upper limit on the number of column families? I could not find anything in the documentation.\n",
	"number": 819,
	"title": "Crash in rocksdb::CuckooTableBuilder::Add"
}, {
	"body": "There is no document for how to compile rocksdb with lz4 support, I work on it for whole afternoon, failed.\n",
	"number": 814,
	"title": "How to compile rocksdb with lz4 support?"
}, {
	"body": "Tried several times on an up-to-date Debian/Sid amd64 environment. It consistently fails:\n[----------] 15 tests from EnvPosixTest\n[...]\n[ RUN      ] EnvPosixTest.DecreaseNumBgThreads\n[       OK ] EnvPosixTest.DecreaseNumBgThreads (7303 ms)\n[ RUN      ] EnvPosixTest.RandomAccessUniqueID\nSegmentation fault\n\ngdb bt shows:\n[ RUN      ] EnvPosixTest.RandomAccessUniqueID\n\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff65a9c9a in strlen () from /lib/x86_64-linux-gnu/libc.so.6\n(gdb) bt\n#0  0x00007ffff65a9c9a in strlen () from /lib/x86_64-linux-gnu/libc.so.6\n#1  0x00007ffff6f0322c in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const ()\n\n   from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#2  0x0000000000456aaf in std::operator==<char, std::char_traits<char>, std::allocator<char> > (__rhs=\"/var/tmp\", __lhs=0x0)\n\n```\nat /usr/include/c++/5/bits/basic_string.h:4925\n```\n#3  rocksdb::IoctlFriendlyTmpdir::IoctlFriendlyTmpdir (this=0x7fffffffe100)\n\n```\nat util/env_test.cc:556\n```\n#4  0x00000000004522cc in rocksdb::EnvPosixTest_RandomAccessUniqueID_Test::TestBody (this=0x9a1370) at util/env_test.cc:590\n#5  0x0000000000655ce7 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (location=0x69ddb8 \"the test body\",\n\n```\nmethod=<optimized out>, object=<optimized out>)\nat third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3822\n```\n#6  testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>\n\n```\n(object=object@entry=0x9a1370, \nmethod=(void (testing::Test::*)(testing::Test * const)) 0x452290 <rocksdb::EnvPosixTest_RandomAccessUniqueID_Test::TestBody()>, \nlocation=location@entry=0x69ddb8 \"the test body\")\nat third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3858\n```\n\n[...]\n\nWhat should I check, how can this be fixed, what additional information may you need?\n",
	"number": 804,
	"title": "4.0 EnvPosixTest.RandomAccessUniqueID segfault"
}, {
	"body": "\"rocksdb::DB::Open \"  cause busy looping and huge memory usage, is there anyone know how to fix it?\n",
	"number": 794,
	"title": "rocksdb for iOS"
}, {
	"body": "```\n2015/10/27-10:28:11.063281 7fed7523f700 Table was constructed:\n  [basic properties]: # data blocks=459; # entries=43935; raw key size=2592165; raw average key size=59.000000; raw value size=1135703; raw average value size=25.849619; data block size=1846115; index block size=34381; filter block size=0; (estimated) table size=1880496; filter policy name=N/A;\n  [user collected properties]: kDeletedKeys=0;\n2015/10/27-10:28:11.149505 7fed7523f700 [WARN] Compaction error: IO error: /data3/bada/data/33/vtscan/597/019276.sst: Bad file descriptor\n2015/10/27-10:28:11.149520 7fed7523f700 (Original Log Time 2015/10/27-10:28:11.149236) [default] compacted to: files[0 7 59 137 0 0 0], MB/sec: 3.7 rd, 15.2 wr, level 3, files in(1, 4) out(21) MB in(2.0, 8.4) out(42.6), read-write-amplify(26.0) write-amplify(20.9) IO error: /data3/bada/data/33/vtscan/597/019276.sst: Bad file descriptor, records in: 44624, records dropped: 0\n2015/10/27-10:28:11.149528 7fed7523f700 [ERROR] Waiting after background compaction error: IO error: /data3/bada/data/33/vtscan/597/019276.sst: Bad file descriptor, Accumulated background error counts: 1\n2015/10/27-10:28:13.218197 7fed7523f700 [ERROR] Waiting after background compaction error: IO error: /data3/bada/data/33/vtscan/597/019276.sst: Bad file descriptor, Accumulated background error counts: 2\n```\n\nThis is the log of the rocksdb,  The error is happen when the compaction generating new file. After this error the db can only be read. I think the reason cause the problem is that the problem of filesystem, not the rocksdb. can you tell me the detail?  And how to prevent this kind of problem\n\nAnd I have meet this kind of problem before, the log show that \n\n```\n2015/05/21-22:15:53.332635 7f1e7eb3f700 (Original Log Time 2015/05/21-22:15:53.332618) [default] Compacting 1@1 + 8@2 files, score 1.44 slots available 0\n2015/05/21-22:15:53.332650 7f1e7eb3f700 (Original Log Time 2015/05/21-22:15:53.332627) [default] Compaction start summary: Base version 6 Base level 1, seek compaction:0, inputs: [2608(2080KB)], [2563(2085KB) 2564(2086KB) 2565(2086KB) 2566(2086KB) 2567(2087KB) 2569(2086KB) 2570(306KB) 2375(2086KB)]\n2015/05/21-22:15:53.800795 7f1e7eb3f700 [WARN] Compaction error: Corruption: block checksum mismatch\n2015/05/21-22:15:53.800813 7f1e7eb3f700 (Original Log Time 2015/05/21-22:15:53.800762) [default] compacted to: files[0 8 58 77 0 0 0], 69.2 MB/sec, level 2, files in(1, 8) out(8) MB in(2.0, 14.6) out(14.3), read-write-amplify(15.2) write-amplify(7.0) Corruption: block checksum mismatch\n2015/05/21-22:15:53.800820 7f1e7eb3f700 Waiting after background compaction error: Corruption: block checksum mismatch, Accumulated background error counts: 1\n2015/05/21-22:15:54.808946 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002625.sst type=2 #2625 -- OK\n2015/05/21-22:15:54.809575 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002624.sst type=2 #2624 -- OK\n2015/05/21-22:15:54.810245 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002623.sst type=2 #2623 -- OK\n2015/05/21-22:15:54.810978 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002622.sst type=2 #2622 -- OK\n2015/05/21-22:15:54.811609 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002621.sst type=2 #2621 -- OK\n2015/05/21-22:15:54.812442 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002620.sst type=2 #2620 -- OK\n2015/05/21-22:15:54.813127 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002619.sst type=2 #2619 -- OK\n2015/05/21-22:15:54.813987 7f1e7eb3f700 Delete /data3/bada/data/33/vtscan/751/002618.sst type=2 #2618 -- OK\n2015/05/21-22:16:12.586943 7f1e7eb3f700 Waiting after background compaction error: Corruption: block checksum mismatch, Accumulated background error counts: 2\n```\n",
	"number": 787,
	"title": "Rocksdb compaction error"
}, {
	"body": "Hi,\nwe're experiencing an odd problem in our RocksDB database: some SST files are deleted while still being listed in the manifest (it happened 4/5 times in the past month, so it's not an one-off).\n\nRocksDB log (only the lines mentioning 389425):\n\n```\n2015/10/10-23:47:12.610730 7f962b77e700 EVENT_LOG_v1 {\"time_micros\": 1444513632610720, \"job\": 2583, \"event\": \"table_file_deletion\", \"file_number\": 389425}\n2015/10/10-23:47:12.652879 7fa7f8d6c700 [WARN] Compaction error: IO error: /path/to/db/389425.sst: No such file or directory\n2015/10/10-23:47:12.652884 7fa7f8d6c700 (Original Log Time 2015/10/10-23:47:12.652829  [default] compacted to: base level 3 max bytes base 57283145 files[4 0 0 2 30 240 1727] max score 1.00, MB/sec: 1957.6 rd, 252.8 wr, level 3, files in(6, 0) out(1) MB in(123.7, 0.0) out(16.0), read-write-amplify(1.1) write-amplify(0.1) IO error: /path/to/db/389425.sst: No such file or directory, records in: 400250, records dropped: 5064\n2015/10/10-23:47:12.652890 7fa7f8d6c700 [ERROR] Waiting after background compaction error: IO error: /path/to/db/389425.sst: No such file or directory, Accumulated background error counts: 1\n```\n\nWe're using RocksDB via the Java JNI interface, in multiple clusters (for a total of > 300 boxes). The on-disk DB is ~60GB per box, and we do 10MB of writes every 20 seconds (I don't have stats about the number of written keys).\n\nThere is a nightly job deleting ~1/400th of the keys, and so far the issue has only happened during this mass-deletion.\n\nIn case it's relevant, we only have a single write and/or delete call running at a time (they are performed via Java synchronized methods).\n\nWe tried writing a stress test to reproduce the issue in a controlled environemnt, but without success.\n\nLet me know if there is anything we can do to help debugging this issue (we can run a patched RocksDB/JNI, but it will have to run in production to have any chance of catching the issue).\n\nThanks in advance,\nMattia\n",
	"number": 782,
	"title": "SST deleted from disk but not from manifest"
}, {
	"body": "Hello,\nI am looking for suggestions on my RocksDB configuration.\n\nNum of memtables = 2\nSize of memtable = 16MB\nNum of levels = 4\nTarget file size base = 64MB\nTarget file size multiplier = 1\nmax_bytes_for_level_base = 1GB\nmax_bytes_for_level_multiplier = 12\nlevel0_file_num_compaction_trigger = 40\nlevel0_slowdown_writes_trigger = 50\nlevel0_stop_writes_trigger = 64\nAverage Key size = 32 Bytes\nAverage Value size = 80 Bytes (we work with smaller values)\nBlock cache size = 128MB\nCompression = NO\nBlock Size = 16KB\n\nThe database is hosted on 128G SSD.\nExpected workload is 70:30 ( Read:Write) and maximum PUT rate is around 20K ops.\n\nCan you suggest if this configuration is optimal with respect to performance and write amplification?\n\nThanks.\n",
	"number": 772,
	"title": "Suggestion on RocksDB Configuration"
}, {
	"body": "I am currently working on an R binding for RocksDB: https://github.com/mrcsparker/rrocksdb\n",
	"number": 767,
	"title": "New R language binding for RocksDB"
}, {
	"body": "Hi guys,\n\nI am new to rocksdb. In order to avoid data loss after machine crashes, I enabled both sync and WAL for write operation, but the performance downgraded significantly.\n\nAfter adding some logs before and after this line (https://github.com/facebook/rocksdb/blob/4.1.fb/db/db_impl.cc#L3897), it shows this Sync operation takes 20-50 ms. \n\nWill try to dig into Sync operation, but it will be appreciated to get some help from here.\n\nP.S. Each rocksdb instance uses one HDD disk, including operation logs, data logs, sst files, etc, are on this disk.\n\nThanks.\n",
	"number": 762,
	"title": "Write performance downgrades significantly when enable both sync and WAL?"
}, {
	"body": "Besides compaction based on level score, leveldb can arrange to automatically compact this file after a certain number of seeks, this can reduce IO when there are duplicate datas between different levels. rocksdb without this feature? why?\n",
	"number": 757,
	"title": "file to compact based on seek stats?"
}, {
	"body": "It might be nice to have a method to delete all db contents,\nwhich can be runned while db is opened.   For example for unittesting.\n\nI do not like the idea of closing DB and running rm -rf from shell, \ndeleting every key from list of all keys is not very nice too.\n",
	"number": 755,
	"title": "add method to clear all data from DB (while having it opened)"
}, {
	"body": "See the discussion here: https://www.facebook.com/groups/rocksdb.dev/permalink/823935664371616/\n",
	"number": 742,
	"title": "Add a wiki page to explain how to change LSM config options"
}, {
	"body": "Running \"make -j\" seems to work fine until it hits the tests section, then things get sad.\n",
	"number": 739,
	"title": "Tests seem to break when running parallel compile"
}, {
	"body": "In Makefile there are lib.a builds for four compression libraries (libz, libbz2, libsnappy, liblz4), but the new experimental zstd is missing.\n",
	"number": 738,
	"title": "ZSTD Not included in JNI build"
}, {
	"body": "Hello,\n\nI'm not sure if you're already supporting Android as a platform (I'm guessing yes because a quick search shows there are precompiler directives to check `OS_ANDROID`).\n\nBasically trying to initialize the native library via `RocksDB.loadLibrary()` fails at https://github.com/facebook/rocksdb/blob/master/java/src/main/java/org/rocksdb/NativeLibraryLoader.java#L101 due to missing `Files`, `File#toPath` and `StandardCopyOption`.\n\nI'm using the [jar in Maven](http://search.maven.org/#artifactdetails%7Corg.rocksdb%7Crocksdbjni%7C3.13.1%7Cjar).\n",
	"number": 734,
	"title": "[Java] initializing the native library on Android fails"
}, {
	"body": "Allow storage backend to store private metadata\n\nSummary: This patch provides a set of helper functions that would enable\na storage backend (e.g., posix, HDFS) to store sstable private metadata\nin the manifest. This frees the storage backend from implementing its\nown metadata mechanisms by reusing the functionality already present in\nthe manifest. This same idea can be applied to the WAL and MANIFEST; if\nthe mechanism is correct, patches for these will follow.\n\nTest Plan: This patch has been tested in posix and in a new backend for\nOpenChannel SSDs [1], where the private metadata functions have been\nimplemented. The idea is that testing functions are provided by the\nstorage backend when used.\n\n[1] https://github.com/OpenChannelSSD\n",
	"number": 731,
	"title": "Allow storage backend to store metadata in manifest"
}, {
	"body": "error is:\n(builder.o), building for iOS simulator, but linking in object file built for OSX, for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
	"number": 729,
	"title": "Failed to link with Xcode7 simulator"
}, {
	"body": "`env_win.cc` calls `GetSystemTimePreciseAsFileTime` which is only supported on Windows 8 or later:\nhttps://msdn.microsoft.com/en-us/library/windows/desktop/hh706895(v=vs.85).aspx\n",
	"number": 726,
	"title": "Windows port fails on Windows 7"
}, {
	"body": "Hi I have seen that you release rocksdb 4.0\n\nwould be nice if you can mention the new 'compaction_readahead_size' option in the \"HISTORY.md\" file.\nIf found it accidentally by calling diff on the include directory between 3.13 and 4.0\n",
	"number": 715,
	"title": "Mention 'compaction_readahead_size' in 'Whats new in rocksdb 4.0.0'"
}, {
	"body": "OptimizeLevelStyleCompaction unconditionally defaulted to kSnappyCompression,\nwhich is a problem if it's not supported. Checks for and uses it, LZ4, or no\ncompression if neither are around.\n",
	"number": 710,
	"title": "fix compression choice when snappy unavailable"
}, {
	"body": "I recently downloaded and am playing around with RocksDB and was getting runtime errors when running examples/simple_example.cc. Digging into it, I found out that the kSnappyCompression option is used whether or not RocksDB was configured to use it. Would fix it myself, but I'm not sure what the alternative \"optimal\" compression settings are when snappy isn't around. No compression? zlib/lz4/etc. (if available)?\n",
	"number": 709,
	"title": "ColumnFamilyOptions::OptimizeLevelStyleCompaction unconditionally uses snappy"
}, {
	"body": "Latest maven RocksDB Java jar (http://mvnrepository.com/artifact/org.rocksdb/rocksdbjni/3.10.1) only appears to support Linux and MacOs. Anyone know how to use RocksDB Java library in Windows Development environment? \n",
	"number": 703,
	"title": "RocksDB Java Windows Development Environment"
}, {
	"body": "We are experiencing PUT request hang on 3.11.0.\n## Below is the stack trace:\n\nThread 58 (Thread 0xfff126f080 (LWP 1332)):\n#0  __pthread_cond_wait (cond=0x1010e048, mutex=0x1010e020) at pthread_cond_wait.c:158\n#1  0x00000062882e76d8 in ?? () from /usr/lib64/librocksdb.so.3.11\n#2  0x00000062879cba4c in start_thread (arg=0xfff126f080) at pthread_create.c:310\n#3  0x00000062878fa21c in __thread_start () from /lib64/libc.so.6\n\nThread 57 (Thread 0xfff0a6f080 (LWP 1333)):\n#0  __pthread_cond_wait (cond=0x1010e048, mutex=0x1010e020) at pthread_cond_wait.c:158\n#1  0x00000062882e76d8 in ?? () from /usr/lib64/librocksdb.so.3.11\n#2  0x00000062879cba4c in start_thread (arg=0xfff0a6f080) at pthread_create.c:310\n#3  0x00000062878fa21c in __thread_start () from /lib64/libc.so.6\n\nThread 56 (Thread 0xffebfff080 (LWP 1334)):\n#0  __pthread_cond_wait (cond=0x1010e048, mutex=0x1010e020) at pthread_cond_wait.c:158\n#1  0x00000062882e76d8 in ?? () from /usr/lib64/librocksdb.so.3.11\n#2  0x00000062879cba4c in start_thread (arg=0xffebfff080) at pthread_create.c:310\n#3  0x00000062878fa21c in __thread_start () from /lib64/libc.so.6\n\nThread 55 (Thread 0xffeb7ff080 (LWP 1335)):\n#0  __pthread_cond_wait (cond=0x1010e048, mutex=0x1010e020) at pthread_cond_wait.c:158\n#1  0x00000062882e76d8 in ?? () from /usr/lib64/librocksdb.so.3.11\n#2  0x00000062879cba4c in start_thread (arg=0xffeb7ff080) at pthread_create.c:310\n#3  0x00000062878fa21c in __thread_start () from /lib64/libc.so.6\n\nThread 52 (Thread 0xffdcffe080 (LWP 1368)):\n#0  __pthread_cond_wait (cond=0xffdcffcd08, mutex=0x1013a888) at pthread_cond_wait.c:158\n#1  0x0000006288296d08 in rocksdb::port::CondVar::Wait() () from /usr/lib64/librocksdb.so.3.11\n#2  0x00000062882ffce0 in rocksdb::InstrumentedCondVar::WaitInternal() () from /usr/lib64/librocksdb.so.3.11\n#3  0x00000062882ffef4 in rocksdb::InstrumentedCondVar::Wait() () from /usr/lib64/librocksdb.so.3.11\n#4  0x00000062882957b8 in rocksdb::WriteThread::EnterWriteThread(rocksdb::WriteThread::Writer*, unsigned long) () from /usr/lib64/librocksdb.so.3.11\n#5  0x000000628820cca8 in rocksdb::DBImpl::Write(rocksdb::WriteOptions const&, rocksdb::WriteBatch*) () from /usr/lib64/librocksdb.so.3.11\n#6  0x00000062881fd030 in rocksdb::DB::Put(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, rocksdb::Slice const&) () from /usr/lib65/librocksdb.so.3.11\n#7  0x00000062881fd0c0 in rocksdb::DBImpl::Put(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, rocksdb::Slice const&) () from /usr/lib64/librocksdb.so.3.11\n#8  0x0000006288214678 in ?? () from /usr/lib64/librocksdb.so.3.11\n#9  0x00000062881bea70 in rocksdb_put () from /usr/lib64/librocksdb.so.3.11\n\nThread 8 (Thread 0xff76fff080 (LWP 1419)):\n#0  __pthread_cond_wait (cond=0xff76ffddf8, mutex=0x1013a888) at pthread_cond_wait.c:158\n#1  0x0000006288296d08 in rocksdb::port::CondVar::Wait() () from /usr/lib64/librocksdb.so.3.11\n#2  0x00000062882ffce0 in rocksdb::InstrumentedCondVar::WaitInternal() () from /usr/lib64/librocksdb.so.3.11\n#3  0x00000062882ffef4 in rocksdb::InstrumentedCondVar::Wait() () from /usr/lib64/librocksdb.so.3.11\n#4  0x00000062882957b8 in rocksdb::WriteThread::EnterWriteThread(rocksdb::WriteThread::Writer*, unsigned long) ()  from /usr/lib64/librocksdb.so.3.11\n#5  0x000000628820cca8 in rocksdb::DBImpl::Write(rocksdb::WriteOptions const&, rocksdb::WriteBatch*) () from /usr/lib64/librocksdb.so.3.11\n#6  0x00000062881fd030 in rocksdb::DB::Put(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, rocksdb::Slice const&) () from /usr/lib64/librocksdb.so.3.11\n#7  0x00000062881fd0c0 in rocksdb::DBImpl::Put(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, rocksdb::Slice const&) () from /usr/lib64/librocksdb.so.3.11\n#8  0x0000006288214678 in ?? () from /usr/lib64/librocksdb.so.3.11\n#9  0x00000062881bea70 in rocksdb_put () from /usr/lib64/librocksdb.so.3.11\n\nThread 6 (Thread 0xff75fff080 (LWP 1421)):\n#0  __pthread_cond_wait (cond=0xff75ffdd08, mutex=0x1013a888) at pthread_cond_wait.c:158\n#1  0x0000006288296d08 in rocksdb::port::CondVar::Wait() () from /usr/lib64/librocksdb.so.3.11\n#2  0x00000062882ffce0 in rocksdb::InstrumentedCondVar::WaitInternal() () from /usr/lib64/librocksdb.so.3.11\n#3  0x00000062882ffef4 in rocksdb::InstrumentedCondVar::Wait() () from /usr/lib64/librocksdb.so.3.11\n#4  0x00000062882957b8 in rocksdb::WriteThread::EnterWriteThread(rocksdb::WriteThread::Writer*, unsigned long) () from /usr/lib64/librocksdb.so.3.11\n#5  0x000000628820cca8 in rocksdb::DBImpl::Write(rocksdb::WriteOptions const&, rocksdb::WriteBatch*) () from /usr/lib64/librocksdb.so.3.11\n#6  0x00000062881fd030 in rocksdb::DB::Put(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, rocksdb::Slice const&) () from /usr/lib64/librocksdb.so.3.11\n#7  0x00000062881fd0c0 in rocksdb::DBImpl::Put(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, rocksdb::Slice const&) () from /usr/lib64/librocksdb.so.3.11\n#8  0x0000006288214678 in ?? () from /usr/lib64/librocksdb.so.3.11\n#9  0x00000062881bea70 in rocksdb_put () from /usr/lib64/librocksdb.so.3.11\n\nNote: All other threads have stack trace similar to Thread 6 or Thread 58.\n## Configuration details are as below:\n\n2015/08/19-23:56:47.814093 fff1d93000 RocksDB version: 3.11.0\n2015/08/19-23:56:47.814288 fff1d93000 Git sha rocksdb_build_git_sha:812c461c96869ebcd8e629da8f01e1cea01c00ca\n2015/08/19-23:56:47.814315 fff1d93000 Compile date Jul 30 2015\n2015/08/19-23:56:47.814326 fff1d93000 DB SUMMARY\n2015/08/19-23:56:47.815267 fff1d93000 CURRENT file:  CURRENT\n2015/08/19-23:56:47.815290 fff1d93000 IDENTITY file:  IDENTITY\n2015/08/19-23:56:47.815399 fff1d93000 MANIFEST file:  MANIFEST-000804 size: 1578 Bytes\n2015/08/19-23:56:47.815414 fff1d93000 SST files in /rocksdb/user-container-1 dir, Total Num: 7, files: 000184.sst 000186.sst 000187.sst 000189.sst 000191.sst 000192.sst 000194.sst\n2015/08/19-23:56:47.815428 fff1d93000 Write Ahead Log file in /rocksdb/user-container-1: 000805.log size: 0\n2015/08/19-23:56:47.815529 fff1d93000          Options.error_if_exists: 0\n2015/08/19-23:56:47.815543 fff1d93000        Options.create_if_missing: 0\n2015/08/19-23:56:47.815553 fff1d93000          Options.paranoid_checks: 1\n2015/08/19-23:56:47.815564 fff1d93000                      Options.env: 0x62884704e0\n2015/08/19-23:56:47.815574 fff1d93000                 Options.info_log: 0x10117010\n2015/08/19-23:56:47.815585 fff1d93000           Options.max_open_files: 5000\n2015/08/19-23:56:47.815595 fff1d93000       Options.max_total_wal_size: 16777192\n2015/08/19-23:56:47.815606 fff1d93000        Options.disableDataSync: 0\n2015/08/19-23:56:47.815616 fff1d93000              Options.use_fsync: 0\n2015/08/19-23:56:47.815627 fff1d93000      Options.max_log_file_size: 0\n2015/08/19-23:56:47.815637 fff1d93000 Options.max_manifest_file_size: 18446744073709551615\n2015/08/19-23:56:47.815648 fff1d93000      Options.log_file_time_to_roll: 0\n2015/08/19-23:56:47.815658 fff1d93000      Options.keep_log_file_num: 1000\n2015/08/19-23:56:47.815668 fff1d93000        Options.allow_os_buffer: 1\n2015/08/19-23:56:47.815679 fff1d93000       Options.allow_mmap_reads: 0\n2015/08/19-23:56:47.815689 fff1d93000      Options.allow_mmap_writes: 0\n2015/08/19-23:56:47.815700 fff1d93000          Options.create_missing_column_families: 0\n2015/08/19-23:56:47.815710 fff1d93000                              Options.db_log_dir:\n2015/08/19-23:56:47.815721 fff1d93000                                 Options.wal_dir: /rocksdb/user-container-1\n2015/08/19-23:56:47.815731 fff1d93000                Options.table_cache_numshardbits: 4\n2015/08/19-23:56:47.815742 fff1d93000     Options.delete_obsolete_files_period_micros: 21600000000\n2015/08/19-23:56:47.815752 fff1d93000              Options.max_background_compactions: 4\n2015/08/19-23:56:47.815763 fff1d93000                  Options.max_background_flushes: 1\n2015/08/19-23:56:47.815773 fff1d93000                         Options.WAL_ttl_seconds: 0\n2015/08/19-23:56:47.815784 fff1d93000                       Options.WAL_size_limit_MB: 0\n2015/08/19-23:56:47.815815 fff1d93000             Options.manifest_preallocation_size: 4194304\n2015/08/19-23:56:47.815826 fff1d93000                          Options.allow_os_buffer: 1\n2015/08/19-23:56:47.815836 fff1d93000                         Options.allow_mmap_reads: 0\n2015/08/19-23:56:47.815847 fff1d93000                        Options.allow_mmap_writes: 0\n2015/08/19-23:56:47.815857 fff1d93000                      Options.is_fd_close_on_exec: 1\n2015/08/19-23:56:47.815867 fff1d93000                    Options.stats_dump_period_sec: 3600\n2015/08/19-23:56:47.815878 fff1d93000                    Options.advise_random_on_open: 1\n2015/08/19-23:56:47.815888 fff1d93000                     Options.db_write_buffer_size: 0\n2015/08/19-23:56:47.815899 fff1d93000          Options.access_hint_on_compaction_start: NORMAL\n2015/08/19-23:56:47.815910 fff1d93000                       Options.use_adaptive_mutex: 0\n2015/08/19-23:56:47.815920 fff1d93000                             Options.rate_limiter: (nil)\n2015/08/19-23:56:47.815931 fff1d93000                           Options.bytes_per_sync: 0\n2015/08/19-23:56:47.815942 fff1d93000                       Options.wal_bytes_per_sync: 0\n2015/08/19-23:56:47.816004 fff1d93000                   Options.enable_thread_tracking: 0\n2015/08/19-23:56:47.816022 fff1d93000 Compression algorithms supported:\n2015/08/19-23:56:47.816032 fff1d93000   Snappy supported: 1\n2015/08/19-23:56:47.816042 fff1d93000   Zlib supported: 1\n2015/08/19-23:56:47.816053 fff1d93000   Bzip supported: 1\n2015/08/19-23:56:47.816063 fff1d93000   LZ4 supported: 0\n2015/08/19-23:56:47.816334 fff1d93000 Recovering from manifest file: MANIFEST-000804\n2015/08/19-23:56:47.816448 fff1d93000 --------------- Options for column family [default]:\n2015/08/19-23:56:47.816475 fff1d93000          Options.error_if_exists: 0\n2015/08/19-23:56:47.816485 fff1d93000        Options.create_if_missing: 0\n2015/08/19-23:56:47.816496 fff1d93000          Options.paranoid_checks: 1\n2015/08/19-23:56:47.816506 fff1d93000                      Options.env: 0x62884704e0\n2015/08/19-23:56:47.816517 fff1d93000                 Options.info_log: 0x10117010\n2015/08/19-23:56:47.816527 fff1d93000           Options.max_open_files: 5000\n2015/08/19-23:56:47.816537 fff1d93000       Options.max_total_wal_size: 16777192\n2015/08/19-23:56:47.816548 fff1d93000        Options.disableDataSync: 0\n2015/08/19-23:56:47.816558 fff1d93000              Options.use_fsync: 0\n2015/08/19-23:56:47.816569 fff1d93000      Options.max_log_file_size: 0\n2015/08/19-23:56:47.816579 fff1d93000 Options.max_manifest_file_size: 18446744073709551615\n2015/08/19-23:56:47.816590 fff1d93000      Options.log_file_time_to_roll: 0\n2015/08/19-23:56:47.816600 fff1d93000      Options.keep_log_file_num: 1000\n2015/08/19-23:56:47.816611 fff1d93000        Options.allow_os_buffer: 1\n2015/08/19-23:56:47.816621 fff1d93000       Options.allow_mmap_reads: 0\n2015/08/19-23:56:47.816631 fff1d93000      Options.allow_mmap_writes: 0\n2015/08/19-23:56:47.816642 fff1d93000          Options.create_missing_column_families: 0\n2015/08/19-23:56:47.816652 fff1d93000                              Options.db_log_dir:\n2015/08/19-23:56:47.816662 fff1d93000                                 Options.wal_dir: /rocksdb/user-container-1\n2015/08/19-23:56:47.816673 fff1d93000                Options.table_cache_numshardbits: 4\n2015/08/19-23:56:47.816683 fff1d93000     Options.delete_obsolete_files_period_micros: 21600000000\n2015/08/19-23:56:47.816694 fff1d93000              Options.max_background_compactions: 4\n2015/08/19-23:56:47.816704 fff1d93000                  Options.max_background_flushes: 1\n2015/08/19-23:56:47.816715 fff1d93000                         Options.WAL_ttl_seconds: 0\n2015/08/19-23:56:47.816725 fff1d93000                       Options.WAL_size_limit_MB: 0\n2015/08/19-23:56:47.816757 fff1d93000             Options.manifest_preallocation_size: 4194304\n2015/08/19-23:56:47.816767 fff1d93000                          Options.allow_os_buffer: 1\n2015/08/19-23:56:47.816777 fff1d93000                         Options.allow_mmap_reads: 0\n2015/08/19-23:56:47.816788 fff1d93000                        Options.allow_mmap_writes: 0\n2015/08/19-23:56:47.816798 fff1d93000                      Options.is_fd_close_on_exec: 1\n2015/08/19-23:56:47.816808 fff1d93000                    Options.stats_dump_period_sec: 3600\n2015/08/19-23:56:47.816819 fff1d93000                    Options.advise_random_on_open: 1\n2015/08/19-23:56:47.816830 fff1d93000                     Options.db_write_buffer_size: 0\n2015/08/19-23:56:47.816840 fff1d93000          Options.access_hint_on_compaction_start: NORMAL\n2015/08/19-23:56:47.816851 fff1d93000                       Options.use_adaptive_mutex: 0\n2015/08/19-23:56:47.816861 fff1d93000                             Options.rate_limiter: (nil)\n2015/08/19-23:56:47.816872 fff1d93000                           Options.bytes_per_sync: 0\n2015/08/19-23:56:47.816882 fff1d93000                       Options.wal_bytes_per_sync: 0\n2015/08/19-23:56:47.816892 fff1d93000                   Options.enable_thread_tracking: 0\n2015/08/19-23:56:47.816903 fff1d93000               Options.comparator: rocksdb.InternalKeyComparator:internal\n2015/08/19-23:56:47.816914 fff1d93000           Options.merge_operator: None\n2015/08/19-23:56:47.816925 fff1d93000        Options.compaction_filter: None\n2015/08/19-23:56:47.816936 fff1d93000        Options.compaction_filter_factory: DefaultCompactionFilterFactory\n2015/08/19-23:56:47.816947 fff1d93000        Options.compaction_filter_factory_v2: DefaultCompactionFilterFactoryV2\n2015/08/19-23:56:47.817011 fff1d93000         Options.memtable_factory: SkipListFactory\n2015/08/19-23:56:47.817028 fff1d93000            Options.table_factory: BlockBasedTable\n2015/08/19-23:56:47.817077 fff1d93000            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x101101c0)\n  cache_index_and_filter_blocks: 1\n  index_type: 0\n  hash_index_allow_collision: 1\n  checksum: 1\n  no_block_cache: 0\n  block_cache: 0x1011bd18\n  block_cache_size: 100000\n  block_cache_compressed: (nil)\n  block_size: 16384\n  block_size_deviation: 10\n  block_restart_interval: 8\n  filter_policy: nullptr\n  format_version: 0\n2015/08/19-23:56:47.817092 fff1d93000        Options.write_buffer_size: 4194304\n2015/08/19-23:56:47.817102 fff1d93000  Options.max_write_buffer_number: 3\n2015/08/19-23:56:47.817114 fff1d93000        Options.compression[0]: NoCompression\n2015/08/19-23:56:47.817125 fff1d93000        Options.compression[1]: NoCompression\n2015/08/19-23:56:47.817135 fff1d93000        Options.compression[2]: NoCompression\n2015/08/19-23:56:47.817146 fff1d93000        Options.compression[3]: NoCompression\n2015/08/19-23:56:47.817157 fff1d93000       Options.prefix_extractor: nullptr\n2015/08/19-23:56:47.817167 fff1d93000             Options.num_levels: 7\n2015/08/19-23:56:47.817178 fff1d93000        Options.min_write_buffer_number_to_merge: 1\n2015/08/19-23:56:47.817188 fff1d93000         Options.purge_redundant_kvs_while_flush: 1\n2015/08/19-23:56:47.817199 fff1d93000            Options.compression_opts.window_bits: -14\n2015/08/19-23:56:47.817209 fff1d93000                  Options.compression_opts.level: -1\n2015/08/19-23:56:47.817220 fff1d93000               Options.compression_opts.strategy: 0\n2015/08/19-23:56:47.817230 fff1d93000      Options.level0_file_num_compaction_trigger: 8\n2015/08/19-23:56:47.817241 fff1d93000          Options.level0_slowdown_writes_trigger: 20\n2015/08/19-23:56:47.817251 fff1d93000              Options.level0_stop_writes_trigger: 24\n2015/08/19-23:56:47.817261 fff1d93000                Options.max_mem_compaction_level: 2\n2015/08/19-23:56:47.817272 fff1d93000                   Options.target_file_size_base: 67108864\n2015/08/19-23:56:47.817282 fff1d93000             Options.target_file_size_multiplier: 1\n2015/08/19-23:56:47.817293 fff1d93000                Options.max_bytes_for_level_base: 671088640\n2015/08/19-23:56:47.817303 fff1d93000 Options.level_compaction_dynamic_level_bytes: 0\n2015/08/19-23:56:47.817314 fff1d93000          Options.max_bytes_for_level_multiplier: 10\n2015/08/19-23:56:47.817324 fff1d93000 Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2015/08/19-23:56:47.817336 fff1d93000 Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2015/08/19-23:56:47.817346 fff1d93000 Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2015/08/19-23:56:47.817357 fff1d93000 Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2015/08/19-23:56:47.817368 fff1d93000 Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2015/08/19-23:56:47.817379 fff1d93000 Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2015/08/19-23:56:47.817390 fff1d93000 Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2015/08/19-23:56:47.817401 fff1d93000       Options.max_sequential_skip_in_iterations: 8\n2015/08/19-23:56:47.817411 fff1d93000              Options.expanded_compaction_factor: 25\n2015/08/19-23:56:47.817421 fff1d93000                Options.source_compaction_factor: 1\n2015/08/19-23:56:47.817432 fff1d93000          Options.max_grandparent_overlap_factor: 10\n2015/08/19-23:56:47.817442 fff1d93000                        Options.arena_block_size: 419430\n2015/08/19-23:56:47.817453 fff1d93000                       Options.soft_rate_limit: 0.00\n2015/08/19-23:56:47.817472 fff1d93000                       Options.hard_rate_limit: 0.00\n2015/08/19-23:56:47.817484 fff1d93000       Options.rate_limit_delay_max_milliseconds: 1000\n2015/08/19-23:56:47.817495 fff1d93000                Options.disable_auto_compactions: 0\n2015/08/19-23:56:47.817505 fff1d93000          Options.purge_redundant_kvs_while_flush: 1\n2015/08/19-23:56:47.817557 fff1d93000                           Options.filter_deletes: 0\n2015/08/19-23:56:47.817572 fff1d93000           Options.verify_checksums_in_compaction: 0\n2015/08/19-23:56:47.817583 fff1d93000                         Options.compaction_style: 0\n2015/08/19-23:56:47.817593 fff1d93000  Options.compaction_options_universal.size_ratio: 1\n2015/08/19-23:56:47.817604 fff1d93000 Options.compaction_options_universal.min_merge_width: 2\n2015/08/19-23:56:47.817614 fff1d93000 Options.compaction_options_universal.max_merge_width: 4294967295\n2015/08/19-23:56:47.817625 fff1d93000 Options.compaction_options_universal.max_size_amplification_percent: 200\n2015/08/19-23:56:47.817635 fff1d93000 Options.compaction_options_universal.compression_size_percent: -1\n2015/08/19-23:56:47.817646 fff1d93000 Options.compaction_options_fifo.max_table_files_size: 1073741824\n2015/08/19-23:56:47.817657 fff1d93000                   Options.table_properties_collectors:\n2015/08/19-23:56:47.817668 fff1d93000                   Options.inplace_update_support: 0\n2015/08/19-23:56:47.817678 fff1d93000                 Options.inplace_update_num_locks: 10000\n2015/08/19-23:56:47.817689 fff1d93000               Options.min_partial_merge_operands: 2\n2015/08/19-23:56:47.817699 fff1d93000               Options.memtable_prefix_bloom_bits: 0\n2015/08/19-23:56:47.817710 fff1d93000             Options.memtable_prefix_bloom_probes: 6\n2015/08/19-23:56:47.817720 fff1d93000   Options.memtable_prefix_bloom_huge_page_tlb_size: 0\n2015/08/19-23:56:47.817730 fff1d93000                           Options.bloom_locality: 0\n2015/08/19-23:56:47.817741 fff1d93000                    Options.max_successive_merges: 0\n2015/08/19-23:56:47.817751 fff1d93000                Options.optimize_fllters_for_hits: 0\n2015/08/19-23:56:47.818983 fff1d93000 Recovered from manifest file:/rocksdb/user-container-1/MANIFEST-000804 succeeded,manifest_file_number is 804, next_file_number is 806, last_sequence is 2380539, log_number is 0,prev_log_number is 0,max_column_family is 0\n2015/08/19-23:56:47.819020 fff1d93000 Column family [default](ID 0), log number is 803\n2015/08/19-23:56:47.819935 fff1d93000 EVENT_LOG_v1 {\"time_micros\": 1440028607819914, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [805]}\n2015/08/19-23:56:47.820000 fff1d93000 Recovering log #805\n2015/08/19-23:56:47.820137 fff1d93000 Creating manifest 807\n2015/08/19-23:56:47.878036 fff1d93000 Deleting manifest 804 current manifest 807\n2015/08/19-23:56:47.878335 fff1d93000 EVENT_LOG_v1 {\"time_micros\": 1440028607878319, \"job\": 1, \"event\": \"recovery_finished\"}\n2015/08/19-23:56:47.880348 fff1d93000 [DEBUG] [JOB 2] Delete /rocksdb/user-container-1//000805.log type=0 #805 -- OK\n2015/08/19-23:56:47.909102 fff1d93000 DB pointer 0x1011bfe0\n## Last few lines of the LOG are as below:\n\n2015/08/20-01:44:13.806745 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053806729, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1097}\n2015/08/20-01:44:13.808517 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001097.sst type=2 #1097 -- OK\n2015/08/20-01:44:13.808627 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053808612, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1095}\n2015/08/20-01:44:13.810338 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001095.sst type=2 #1095 -- OK\n2015/08/20-01:44:13.810443 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053810427, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1093}\n2015/08/20-01:44:13.867749 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001093.sst type=2 #1093 -- OK\n2015/08/20-01:44:13.867893 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053867874, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1092}\n2015/08/20-01:44:13.869657 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001092.sst type=2 #1092 -- OK\n2015/08/20-01:44:13.869766 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053869751, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1090}\n2015/08/20-01:44:13.930074 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001090.sst type=2 #1090 -- OK\n2015/08/20-01:44:13.930218 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053930200, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1089}\n2015/08/20-01:44:13.932096 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001089.sst type=2 #1089 -- OK\n2015/08/20-01:44:13.932215 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053932199, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1087}\n2015/08/20-01:44:13.933964 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001087.sst type=2 #1087 -- OK\n2015/08/20-01:44:13.934084 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053934069, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1085}\n2015/08/20-01:44:13.994680 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001085.sst type=2 #1085 -- OK\n2015/08/20-01:44:13.994830 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053994812, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1084}\n2015/08/20-01:44:13.996628 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001084.sst type=2 #1084 -- OK\n2015/08/20-01:44:13.996743 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053996727, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1082}\n2015/08/20-01:44:13.998485 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001082.sst type=2 #1082 -- OK\n2015/08/20-01:44:13.998598 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035053998583, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1080}\n2015/08/20-01:44:14.060166 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001080.sst type=2 #1080 -- OK\n2015/08/20-01:44:14.060312 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035054060293, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1079}\n2015/08/20-01:44:14.062291 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001079.sst type=2 #1079 -- OK\n2015/08/20-01:44:14.062582 ffedd91080 EVENT_LOG_v1 {\"time_micros\": 1440035054062563, \"job\": 134, \"event\": \"table_file_deletion\", \"file_number\": 1077}\n2015/08/20-01:44:14.124123 ffedd91080 [DEBUG] [JOB 134] Delete /rocksdb/user-container-1/001077.sst type=2 #1077 -- OK\n",
	"number": 696,
	"title": "Put request hangs on 3.11.0"
}, {
	"body": "Using rocksjni with a Java web application running in Tomcat, the JVM crashed with this log message:\n\n```\nException in thread \"Thread-10\" java.lang.NoClassDefFoundError: org/rocksdb/InfoLogLevel\nCaused by: java.lang.ClassNotFoundException: org.rocksdb.InfoLogLevel\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\njava: ./java/./rocksjni/portal.h:743: static _jclass* rocksdb::InfoLogLevelJni::getJClass(JNIEnv*): Assertion `jclazz != nullptr' failed.\n```\n\nHere's the relevant code:\n\n```\n    // Get the java class id of org.rocksdb.WBWIRocksIterator.WriteType.\n    static jclass getJClass(JNIEnv* env) {\n      jclass jclazz = env->FindClass(\"org/rocksdb/InfoLogLevel\");\n      if (jclazz == nullptr)\n        env->ExceptionDescribe();\n      assert(jclazz != nullptr);\n      return jclazz;\n    }\n```\n\nThe class `org/rocksdb/InfoLogLevel` is certainly present, inside the rocksdbjni JAR file under `WEB-INF/lib`, so what's going on?\n\nHere's my theory: as we all know Tomcat creates separate `ClassLoader`s for each web application. A web application's Java classes and library classes are not available from the system class loader; instead, you must find them using the loader that Tomcat has created for that web application.\n\nUnfortunately, when rocksdb is setup by rocksdbjni and it wants to log something, it uses `LoggerJniCallback::Logv()`, which looks like this:\n\n```\n/**\n * Get JNIEnv for current native thread\n */\nJNIEnv* LoggerJniCallback::getJniEnv() const {\n  JNIEnv *env;\n  jint rs = m_jvm->AttachCurrentThread(reinterpret_cast<void **>(&env), NULL);\n  assert(rs == JNI_OK);\n  return env;\n}\n\n...\n\nvoid LoggerJniCallback::Logv(const InfoLogLevel log_level,\n    const char* format, va_list ap) {\n  if (GetInfoLogLevel() <= log_level) {\n    JNIEnv* env = getJniEnv();\n\n     ....\n\n    }\n    m_jvm->DetachCurrentThread();\n  }\n}\n```\n\nNote that the current thread is not associated with the JVM at all, which is why it has to put the call back into Java-land inside a `AttachCurrentThread()`/`DetachCurrentThread()` pair. But that means that the corresponding Java thread no longer has a same context class loader. So it defaults to the system class loader and so the attempt to load `org/rocksdb/InfoLogLevel` fails.\n\nSuggested solution:\n1. Upon creation of a new RocksDB instance, store a reference to the current thread's context class loader inside the native code.\n2. Whenever the native code needs to load a class, invoke `ClassLoader.findClass()` directly using the saved loader reference.\n",
	"number": 691,
	"title": "[Java] Assertion failure + JVM crash with rocksdbjni running under Tomcat"
}, {
	"body": "# The issue:\n\nRight now the 'default' build is to build with -march=native. Specified in the INSTALL.md. However, this default installation has the unexpected behavior that when you run it you get a `SIGILL`\n\nThe default compiler flags for both satic (librocksdb.a) and dynamic librocksdb.so.3.11.2 are compiled with  `-march=native`\n## Background\n\n```\ng++ -shared -Wl,-soname -Wl,librocksdb.so.3.11 -g -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -I. -I./include -std=c++11  -DROCKSDB_PLATFORM_POSIX  -DOS_LINUX -fno-builtin-memcmp -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS=google -DZLIB -DBZIP2 -DLZ4 -march=native   -isystem ./third-party/gtest-1.7.0/fused-src -O2 -fno-omit-frame-pointer -momit-leaf-frame-pointer -DNDEBUG -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -fPIC db/builder.cc db/c.cc ........\n\n```\n\nThe issue with this is that my build box CPU has instructions that my cluster CPU's do not support. \n\nSpecifically this line \n\n```\n    cache.c:474  int num_shards = 1 << num_shard_bits_; \n```\n\ncauses a `shlx` optimization. It is an AVX instruction\n\nhttps://software.intel.com/sites/default/files/4f/5b/36945\n## Debug information\n\nMy build machine CPU is a:\n\n```\nIntel(R) Core(TM) i7-5960X CPU @ 3.00GHz\n```\n\nMy cluster computer cpus are:\n\n```\nIntel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz\n```\n### GDB  bt\n\n```\n(gdb) bt\n#0  0x00007ffff144c851 in ShardedLRUCache (num_shard_bits=4, capacity=8388608, this=0x60d000003790) at util/cache.cc:474\n#1  construct<rocksdb::(anonymous namespace)::ShardedLRUCache, unsigned long&, int&> (this=<optimized out>, __p=0x60d000003790) at /usr/include/c++/4.9/ext/new_allocator.h:120\n#2  _S_construct<rocksdb::(anonymous namespace)::ShardedLRUCache, unsigned long&, int&> (__a=..., __p=0x60d000003790) at /usr/include/c++/4.9/bits/alloc_traits.h:253\n#3  construct<rocksdb::(anonymous namespace)::ShardedLRUCache, unsigned long&, int&> (__a=..., __p=0x60d000003790) at /usr/include/c++/4.9/bits/alloc_traits.h:399\n#4  _Sp_counted_ptr_inplace<unsigned long&, int&> (__a=..., this=0x60d000003780) at /usr/include/c++/4.9/bits/shared_ptr_base.h:515\n#5  construct<std::_Sp_counted_ptr_inplace<rocksdb::(anonymous namespace)::ShardedLRUCache, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache>, (__gnu_cxx::_Lock_policy)2>, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache> const, unsigned long&, int&> (this=<optimized out>, __p=<optimized out>) at /usr/include/c++/4.9/ext/new_allocator.h:120\n#6  _S_construct<std::_Sp_counted_ptr_inplace<rocksdb::(anonymous namespace)::ShardedLRUCache, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache>, (__gnu_cxx::_Lock_policy)2>, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache> const, unsigned long&, int&> (__a=..., __p=<optimized out>) at /usr/include/c++/4.9/bits/alloc_traits.h:253\n#7  construct<std::_Sp_counted_ptr_inplace<rocksdb::(anonymous namespace)::ShardedLRUCache, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache>, (__gnu_cxx::_Lock_policy)2>, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache> const, unsigned long&, int&> (__a=..., __p=<optimized out>) at /usr/include/c++/4.9/bits/alloc_traits.h:399\n#8  __shared_count<rocksdb::(anonymous namespace)::ShardedLRUCache, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache>, unsigned long&, int&> (__a=..., this=<optimized out>) at /usr/include/c++/4.9/bits/shared_ptr_base.h:619\n#9  __shared_ptr<std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache>, unsigned long&, int&> (__a=..., this=<optimized out>, __tag=...) at /usr/include/c++/4.9/bits/shared_ptr_base.h:1090\n#10 shared_ptr<std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache>, unsigned long&, int&> (__a=..., this=<optimized out>, __tag=...) at /usr/include/c++/4.9/bits/shared_ptr.h:316\n#11 allocate_shared<rocksdb::(anonymous namespace)::ShardedLRUCache, std::allocator<rocksdb::(anonymous namespace)::ShardedLRUCache>, unsigned long&, int&> (__a=...) at /usr/include/c++/4.9/bits/shared_ptr.h:588\n#12 make_shared<rocksdb::(anonymous namespace)::ShardedLRUCache, unsigned long&, int&> () at /usr/include/c++/4.9/bits/shared_ptr.h:604\n#13 rocksdb::NewLRUCache (capacity=capacity@entry=8388608, num_shard_bits=num_shard_bits@entry=4) at util/cache.cc:552\n#14 0x00007ffff144cc06 in rocksdb::NewLRUCache (capacity=capacity@entry=8388608) at util/cache.cc:545\n#15 0x00007ffff1413c19 in rocksdb::BlockBasedTableFactory::BlockBasedTableFactory (this=0x60b000009d00, table_options=...) at table/block_based_table_factory.cc:36\n#16 0x00007ffff14d3254 in rocksdb::ColumnFamilyOptions::ColumnFamilyOptions (this=0x7fffffffb738) at util/options.cc:135\n#17 0x00007ffff67224e5 in Options (this=0x7fffffffb630) at /usr/local/include/rocksdb/options.h:1024\n```\n### Disasembler\n\n```\n(gdb) disassemble\nDump of assembler code for function rocksdb::NewLRUCache(unsigned long, int):\n   0x00007ffff144c790 <+0>:     push   %rbp\n   0x00007ffff144c791 <+1>:     mov    %rsp,%rbp\n   0x00007ffff144c794 <+4>:     push   %r15\n   0x00007ffff144c796 <+6>:     push   %r14\n   0x00007ffff144c798 <+8>:     push   %r13\n   0x00007ffff144c79a <+10>:    push   %r12\n   0x00007ffff144c79c <+12>:    push   %rbx\n   0x00007ffff144c79d <+13>:    mov    %edx,%ebx\n   0x00007ffff144c79f <+15>:    sub    $0x58,%rsp\n   0x00007ffff144c7a3 <+19>:    cmp    $0x13,%edx\n   0x00007ffff144c7a6 <+22>:    mov    %rdi,-0x58(%rbp)\n   0x00007ffff144c7aa <+26>:    mov    %rsi,-0x60(%rbp)\n   0x00007ffff144c7ae <+30>:    jle    0x7ffff144c7d2 <rocksdb::NewLRUCache(unsigned long, int)+66>\n   0x00007ffff144c7b0 <+32>:    movq   $0x0,(%rdi)\n   0x00007ffff144c7b7 <+39>:    movq   $0x0,0x8(%rdi)\n   0x00007ffff144c7bf <+47>:    mov    -0x58(%rbp),%rax\n   0x00007ffff144c7c3 <+51>:    add    $0x58,%rsp\n   0x00007ffff144c7c7 <+55>:    pop    %rbx\n   0x00007ffff144c7c8 <+56>:    pop    %r12\n   0x00007ffff144c7ca <+58>:    pop    %r13\n   0x00007ffff144c7cc <+60>:    pop    %r14\n   0x00007ffff144c7ce <+62>:    pop    %r15\n   0x00007ffff144c7d0 <+64>:    pop    %rbp\n   0x00007ffff144c7d1 <+65>:    retq\n   0x00007ffff144c7d2 <+66>:    mov    $0x88,%edi\n   0x00007ffff144c7d7 <+71>:    callq  0x7ffff1313fb0 <_Znwm@plt>\n   0x00007ffff144c7dc <+76>:    mov    %rax,%r14\n   0x00007ffff144c7df <+79>:    movl   $0x1,0x8(%rax)\n   0x00007ffff144c7e6 <+86>:    movl   $0x1,0xc(%rax)\n   0x00007ffff144c7ed <+93>:    lea    0x2f5ddc(%rip),%rax        # 0x7ffff17425d0 <_ZTVSt23_Sp_counted_ptr_inplaceIN7rocksdb12_GLOBAL__N_115ShardedLRUCacheESaIS2_ELN9__gnu_cxx12_Lock_policyE2EE+16>\n   0x00007ffff144c7f4 <+100>:   xor    %esi,%esi\n   0x00007ffff144c7f6 <+102>:   mov    %rax,(%r14)\n   0x00007ffff144c7f9 <+105>:   lea    0x10(%r14),%rax\n   0x00007ffff144c7fd <+109>:   mov    %rax,-0x50(%rbp)\n   0x00007ffff144c801 <+113>:   lea    0x2f5d48(%rip),%rax        # 0x7ffff1742550 <_ZTVN7rocksdb12_GLOBAL__N_115ShardedLRUCacheE+16>\n   0x00007ffff144c808 <+120>:   mov    %rax,0x10(%r14)\n   0x00007ffff144c80c <+124>:   lea    0x20(%r14),%rax\n   0x00007ffff144c810 <+128>:   mov    %rax,%rdi\n   0x00007ffff144c813 <+131>:   mov    %rax,-0x68(%rbp)\n   0x00007ffff144c817 <+135>:   callq  0x7ffff1316db0 <_ZN7rocksdb4port5MutexC1Eb@plt>\n   0x00007ffff144c81c <+140>:   lea    0x48(%r14),%rax\n   0x00007ffff144c820 <+144>:   xor    %esi,%esi\n   0x00007ffff144c822 <+146>:   mov    %rax,%rdi\n   0x00007ffff144c825 <+149>:   mov    %rax,-0x70(%rbp)\n   0x00007ffff144c829 <+153>:   callq  0x7ffff1316db0 <_ZN7rocksdb4port5MutexC1Eb@plt>\n   0x00007ffff144c82e <+158>:   mov    -0x60(%rbp),%rax\n   0x00007ffff144c832 <+162>:   mov    %ebx,0x78(%r14)\n   0x00007ffff144c836 <+166>:   mov    $0xffffffffffffffff,%rdi\n   0x00007ffff144c83d <+173>:   movq   $0x0,0x70(%r14)\n   0x00007ffff144c845 <+181>:   mov    %rax,0x80(%r14)\n   0x00007ffff144c84c <+188>:   mov    $0x1,%eax\n=> 0x00007ffff144c851 <+193>:   shlx   %ebx,%eax,%eax\n   0x00007ffff144c856 <+198>:   mov    %eax,-0x74(%rbp)\n   0x00007ffff144c859 <+201>:   cltq\n   0x00007ffff144c85b <+203>:   mov    %rax,%rbx\n   0x00007ffff144c85e <+206>:   mov    %rax,-0x48(%rbp)\n   0x00007ffff144c862 <+210>:   movabs $0xe2000000000000,%rax\n   0x00007ffff144c86c <+220>:   cmp    %rax,%rbx\n\n```\n\nThis line: \n\n```\n=> 0x00007ffff144c851 <+193>:   shlx   %ebx,%eax,%eax\n```\n\nIs the culprit. \n## Solution\n\nDefault `PORTABLE=1` on the Makefile\n## Other debug information.\n\nI un`jar`ed the rocksdb-3.10\\* libs to inspect the instructions in them:\n\nThe JNI bindings **Do not** include these instructions as well, which just means that its built with PORTABLE=1.\n\n```\n$ objdump -d librocksdbjni-linux64.so | grep shlx\n\n```\n\nThanks again! \n\nI'm only filing an issue because my previous rocksdb build `3.5` did not behave like this \n",
	"number": 690,
	"title": "Set PORTABLE=1 the default for Makefile"
}, {
	"body": "Presently in a couple of patches; let me know if you'd like them squashed first (I didn't see guidance on this). \n",
	"number": 686,
	"title": "Initial support for column families in ldb"
}, {
	"body": "Hallo,\nI have posted this thread in stackoverflow. I am getting no answer and I hope you can help me (and others) with  the probelm.\nhttp://stackoverflow.com/questions/31872945/rocksdb-fails-to-create-more-than-one-database-instance-concurrently\nThanks.\n",
	"number": 684,
	"title": "Creating multiple instance of rocksdb with MPI"
}, {
	"body": "",
	"number": 681,
	"title": "backupable_db: use dynamic buffer when dealing with meta file; fix a buffer overrun when generating meta file."
}, {
	"body": "While making *.vcxproj on win32 configuration using cmake, it calls commands that produces additional CR-LF output. In the result, the following compilation faults.:\n\n```\n  const char* rocksdb_build_git_datetime = \"rocksdb_build_git_datetime:03.08.2015\n  The filename, directory name, or volume label syntax is incorrect.\n  The filename, directory name, or volume label syntax is incorrect.\nC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V120\\Microsoft.CppCommon.targets(170,5): error MSB6006: \"cmd.exe\" exited with code 123. [C:\\lib\\contrib\\rocksdb\\build\\GenerateBuildVersion.vcxproj]\n```\n\nThe solution is to write after the next string\n`string(CONCAT GIT_DATE_TIME ${DATE} ${TIME})`\nthese two lines\n`string(REGEX REPLACE \"\\n\" \"\" GIT_DATE_TIME ${GIT_DATE_TIME})`\n`string(REGEX REPLACE \"\\r\" \"\" GIT_DATE_TIME ${GIT_DATE_TIME})`\nin CMakeLists.txt\n",
	"number": 679,
	"title": "cmake script on win32 produce additional output"
}, {
	"body": "I tried to cross compile rocksdb, with gcc-linaro-arm-linux-gnueabihf-g++, but an error.\narm.sh content is:\n# !/bin/sh\n\nexport HOST=arm-linux-gnueabihf\nexport CC=\"${HOST}-gcc\"\nexport CXX=\"${HOST}-g++\"\nexport LD=\"${HOST}-ld\"\nexport AR=\"${HOST}-ar\"\nexport NM=\"${HOST}-nm\"\nexport AS=\"${HOST}-as\"\nexport RANLIB=\"${HOST}-ranlib\"\nexport STRIP=\"${HOST}-strip\"\nexport OBJCOPY=\"${HOST}-objcopy\"\nexport OBJDUMP=\"${HOST}-objdump\"\n\nmake\n\nmake install\n\nError message is:\nnot supported 'march=native'\n\nHow to cross compile for ARM platform?\nMy dev host OS is Ubuntu 14.04,target ARM is armv7-a\n",
	"number": 678,
	"title": "How to cross compile on ARM platform?"
}, {
	"body": "So judging from previous blog posts[1] and a conversation on twitter[2], I'm lead to believe that I should be able to read in a leveldb and begin working with it with rocks. However, I've noticed the first issue in doing this is that rocks is looking for sst files when leveldb is using ldb files.\n\n`Can't access /27799377.sst: IO error: /path/goes/here/27799377.sst: No such file or directory`\n\nThe second issue, if I just go \"Sure, i'll just rename them\" is that it generates lots of warnings processing the files[3].\n\nSo is it generally possible to migrate/move/dump/load/whatever this existing leveldb into rocksdb without having to do the heavy processing that generated the initial database.\n\n1 - http://rocksdb.org/blog/1811/migrating-from-leveldb-to-rocksdb-2/\n2 - https://twitter.com/OverlordQ/status/626887860944248832\n3 - http://pastebin.com/5quBh96f\n",
	"number": 677,
	"title": "Reading LevelDB files"
}, {
	"body": "At present the design of the Java RocksIterator is not very Java'esk. That is to say that as a Java developer you will typically hold the assumption that calling `getKey()` or `getValue()` on the iterator will be stable until you call `next()`, `prev()`, `seek...`. This is not the case; see: https://github.com/facebook/rocksdb/issues/616#issuecomment-124246253\n\nI would suggest that the Java RocksIterator should be replaced by two different types of iterator:\n1. A direct iterator which uses `ByteBuffer` to access the underlying C++ memory. This would avoid the JNI copy that is done with the current RocksIterator on every call to `getKey()` or `getValue()`. It would also be unstable like the current RocksIterator, however because of the use of ByteBuffer this would be expected by the Java developer.\n2. A non-direct iterator which uses `byte[]` just as the current RocksIterator does, however the behaviour of `getKey()` and `getValue()` would be stable until `next()` or `prev()` is called; It would also be stable for example when remove is called on a non-direct iterator of a WriteBatch. This could for example be achieved by the first call to `getKey` internally caching the value, the value would be disposed on the call to `next` or `close`, the same principle would apply to `getValue`.\n\nComments?\n",
	"number": 675,
	"title": "Ideas for changing the Java RocksIterator"
}, {
	"body": "That attach log file with error in build step. Visual Studio 2013 (Version 12) cannot find `json_document.cc` in folder of project, but  i see usually all file depend on folder such as  `E:\\rocksdb\\include\\rocksdb\\utilities\\json_document.h` and `rocksdb\\utilities\\document\\json_document.cc`. \n\nWhy VC++ 2013 represent an error cannot read file from folder when build all project? Follow an fatal error below.\n\n```\n3>e:\\workspacecpp\\rocksdb\\utilities\\document\\json_document.cc(592): fatal error C1001: An internal error has occurred in the compiler.\n```\n\nIf Rocksdblib was built only step, Project throws an error as below.\n\n```\n2>  rocksdblib.dir\\Debug\\document_db.obj \n2>  rocksdblib.dir\\Debug\\json_document.obj \n2>rocksdblib.dir\\Debug\\json_document.obj : fatal error LNK1136: invalid or corrupt file\n2>\n2>Build FAILED.\n2>\n2>Time Elapsed 00:03:35.20\n========== Build: 1 succeeded, 1 failed, 1 up-to-date, 0 skipped ==========\n```\n\n2>Time Elapsed 00:00:00.12\n3>------ Build started: Project: rocksdblib, Configuration: Debug x64 ------\n3>Build started 26/7/2558 19:37:55.\n3>InitializeBuildStatus:\n3>  Touching \"rocksdblib.dir\\Debug\\rocksdblib.tlog\\unsuccessfulbuild\".\n3>CustomBuild:\n3>  Building Custom Rule E:/workspacecpp/rocksdb/CMakeLists.txt\n3>  CMake does not need to re-run because E:\\workspacecpp\\rocksdb\\buildwin\\CMakeFiles\\generate.stamp is up-to-date.\n3>  Generating ../util/build_version.cc\n3>ClCompile:\n3>  write_batch_with_index_internal.cc\n3>  write_batch_with_index.cc\n3>  db_ttl_impl.cc\n3>  optimistic_transaction_db_impl.cc\n3>  optimistic_transaction_impl.cc\n3>  spatial_db.cc\n3>  redis_lists.cc\n3>  uint64add.cc\n3>  put.cc\n3>  stringappend2.cc\n3>  stringappend.cc\n3>  leveldb_options.cc\n3>  geodb_impl.cc\n3>  flashcache.cc\n3>  json_document_builder.cc\n3>  json_document.cc\n3>  document_db.cc\n3>  checkpoint.cc\n3>  backupable_db.cc\n3>  xxhash.cc\n3>  Generating Code...\n3>e:\\workspacecpp\\rocksdb\\utilities\\document\\json_document.cc(592): fatal error C1001: An internal error has occurred in the compiler.\n3>  (compiler file 'f:\\dd\\vctools\\compiler\\utc\\src\\p2\\ehexcept.c', line 956)\n3>   To work around this problem, try simplifying or changing the program near the locations listed above.\n3>  Please choose the Technical Support command on the Visual C++ \n3>   Help menu, or open the Technical Support help file for more information\n3>\n3>Build FAILED.\n3>\n3>Time Elapsed 00:00:31.60\n========== Build: 2 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========\n\nThank in advance, \nR.Chatsiri\n",
	"number": 674,
	"title": "Rocksdblib cannot build completed in json_document.cc source code."
}, {
	"body": "I am storing metadata about files into RocksDB, however the binary files themselves are living on the filesystem alongside my Rocks data dir.\n\nWhilst I can use BackupEngine to backup the database, I wondered if there was an elegant solution for also backing up my binary files.\n\nI know that I can scan through my metadata stored in RocksDB to find the binary files that I need to backup. However, to be consistent I would need to access RocksDB at the same sequence number as the BackupEngine when `createNewBackup` is called. Is there any mechanism for getting a snapshot or iterator on a column family which is based on the same sequence number used by `createNewBackup`?\n",
	"number": 671,
	"title": "How to Backup Rocks and linked resources"
}, {
	"body": "1, if the param new_descriptor_log is true, new manifest file number may re-use and overwrite the old one, which is an unexpected behavior.\n2, check s.ok() to avoid unnecessary invoking of v->PrepareApply().\n3, add current_file_updated flag for assert.\n4, fix incorrect log printing when deleting manifest file.\n",
	"number": 661,
	"title": "fix bugs in VersionSet::LogAndApply() to make it more robust"
}, {
	"body": "",
	"number": 660,
	"title": "make it compile on armv7l"
}, {
	"body": "Hello, I'm interested in using WBWI from Rust, which requires adding support for it to the C API.\n\nThanks!\n",
	"number": 650,
	"title": "Expose WriteBatchWithIndex to C API"
}, {
	"body": "Change Iterator Direction from Forward to Backward: Either Next() + Prev(), Seek() + Prev() needs changing iterator direction. Here are known bugs and implementation constraints about it.\n\nHow changing direction is implemented? In DBIter, we keep calling Prev() to internal iterator, until we find another user key that is smaller than the current one. In merge iterator (which is most likely the internal iterator), when changing direction, for every child, we Seek() to current key, if !Vallid(), we call SeekToLast(), and then fetch all of them into the heap for a merge sort.\n\nThere are outstanding problems in implementing it.\n### Merge operator\n\nIn forward direction, if current key is a merge key, internal iterator is placed to the next internal key after the current key. When changing direction, since current position is not for a key of current user key, we simply return, which is a wrong location. For example, if we have such following internal keys:\n\nkey: A type: merge seqID: 4 \nkey: A type: merge seqID: 3 \nkey: B type: merge seqID: 6  <==== (1)\nkey: B type: merge seqID: 2 \n\nafter seeking to A, internal key is placed to position (1) and user key is A. When we call Prev(), we see current location with key B which is not A, so we simply return.\n\nWe tried to fix it in https://github.com/facebook/rocksdb/commit/ec70fea4c4025351190eba7a02bd09bb5f083790. Basically we keep calling Prev() in internal keys until a key is less than A. However, we hit other problems.\n### prefix bloom/hash\n\nPrefix iterator makes sure that if Seek() returns !Valid(), it only means no key with that prefix that is larger than the seek key. SeekToLast() depends on implementation. In most implementation, it will place to the really last position of the iterator. When merge iterator issues the Seek() + SeekToLast() sequence, it places a key to very last key. Lots of valid keys can be there between the seek key and the position. We will either end up with iterating millions of keys to find the value, or (in current implementation), simply return a key much larger than the key we should returned.\n\nBasically, changing iterating direction is not supported in prefix bloom/hash. The problem with this limitation is that, there is no way to find a key smaller than a key in prefix-based iterator, but it is a common use case.\n### number of keys skip optimization, working with prefix bloom/hash\n\nWe have an optimization of seeking: https://github.com/facebook/rocksdb/blob/4f56632b16d8ae62b7e9dd6087e22d6c161e49a9/db/db_iter.cc#L572-L579 . The motivation of this optimization is that if there is a key updated many times, we are not comfortable with calling Prev() to iterate to the same key. Instead, we directly seek to the first key to skip them. For example, if we have keys:\n\nkey:  type: put seqID: 1 \nkey: B type: put seqID: 100 \nkey: B type: put seqID: 99\nkey: B type: put seqID: 98\n... \nkey: B type: put seqID: 3\nkey: B type: put seqID: 2  <===== (1)\n\nand we are in location (1), after several Prev(), we still find ourself iterating internal key of B, we simply Seek to (B, seqID: infinite) to skip all those keys.\n\nThis implementation combining with prefix bloom/hash, can cause not just slowness but infinite loop. Since we seek to a wrong location, we never find the value we want within max threshold before seeking, and then keep seeking.\n### data race while inserting\n\nIt is possible that new entries is inserted to underlying child iterator. Those entries will have higher sequence ID than the iterator so they should be filtered out. For example, if we are doing forward iterating in this location:\n\niterator sequence ID: 10\nkey: A type: put seqID: 1 \nkey: B type: put seqID: 2  <==== (1)\n\nwhile we are doing Seek(\"B\"), one of the child iterator is like that:\n\nkey: A type: put seqID: 1 \n\nso it is !Valid() and we need SeekToLast(). However, before SeekToLast() several more keys are inserted and the child iterator is like this:\n\nkey: A type: put seqID: 1 \nkey: Z type: put seqID: 100\nkey: Z type: put seqID: 99\n...\nkey: Z type: put seqID: 60\nkey: Z type: put seqID: 59  <==== (2)\n\nSo the location is placed in (2) in merged iterator:\n\nkey: A type: put seqID: 1 \nkey: B type: put seqID: 2\nkey: Z type: put seqID: 100\nkey: Z type: put seqID: 99\n...\nkey: Z type: put seqID: 60\nkey: Z type: put seqID: 59  <==== (2)\n\nWe keep calling Prev() to internal iterator, and then the optimization of max_skipped kicked in and we start to seek to (Z, seqID: infinite) to internal iterator. Notice we seek to Z to not only the child iterator with Z inserted, but all other child iterators. Now the position of the iterator can be far off. We still don't fully understand the result of it might be.\n\nThese problems need to be addressed. A new ReverseSeek() will eliminate most use cases of changing direction to reduce the necessity of handling this complexity. At the same time, changing direction needs to be handled correctly, or gives clear error messages.\n",
	"number": 648,
	"title": "Issue of Changing Iterator Direction from Forward to Backward"
}, {
	"body": "Let's use this issue to track the efforts to improve write throughput to mem tables.\n\nAs a discussion with @guyg8, I'll work on branch `sdong_write` and port necessary from branch `write_throughput`, which contains changes @guyg8 made. Hopefully branch `sdong_write`  can have a running codes after a week and let's see whether there can be any improvements.\n",
	"number": 642,
	"title": "Parallel Writes to mem tables, etc"
}, {
	"body": "I have got the Uint64 example to work fine using the example from https://github.com/facebook/rocksdb/blob/master/db/merge_test.cc\n\nIs there an example for List or Set? Do I have to redefine counters, merge based counters and a set add operator all over again? Is it possible to trim down the lines of code?\n\nWIP code at https://github.com/sshivaji/polyglot/blob/leveldb/src/book_make.cpp (just moved from leveldb)\n\nThanks in advance.\n",
	"number": 640,
	"title": "Merge Operator Set example and optimizations?"
}, {
	"body": "Compared to original leveldb\n    mergedb: 853.131 micros/op;    4.6 MB/s\n    leveldb: 1913.242 micros/op;    2.0 MB/s\n\nIn my strategy, size of n+1 level's data is only about twice the size of n level's data. And every compaction will compact all adjacent non-empty levels from level0 to a single level, which resulting less writes.\nSuppose a LevelDB with 7 levels, there will be about 20 levels in my new db. The idea write amplification will be 1 write to log, 1 write to level0, and about 10 write to move up to level20, that is about 12 writes compared to 68 writes in original db.\nI don't want to stop the write operation, so I compact data in a sophisticated way similar to the above, then resulting several extra writes. In all, the new writes will be about 15.\n\ndetails can be found [mergedb](https://github.com/yedf/mergedb), Is anyone interested?\n",
	"number": 637,
	"title": "another strategy to speed up writing"
}, {
	"body": "On an Ubuntu 14.04.2 32 bit (i686) system with gcc 4.8.2, after correcting #633 and hacking around #634, db_test fails with the following:\n\n```\n[==========] Running 253 tests from 6 test cases.\n[----------] Global test environment set-up.\n[----------] 202 tests from DBTest\n[ RUN      ] DBTest.Empty\ndb_test: db/memtable.cc:83: rocksdb::MemTable::MemTable(const rocksdb::InternalKeyComparator&, const rocksdb::ImmutableCFOptions&, const rocksdb::MutableCFOptions&, rocksdb::WriteBuffer*): Assertion `!should_flush_' failed.\nReceived signal 6 (Aborted)\n#0   [0xb76ebd22] ??    ??:0    \n#1   /lib/i386-linux-gnu/libc.so.6(gsignal+0x47) [0xb73b6607] ??    ??:0    \n#2   /lib/i386-linux-gnu/libc.so.6(abort+0x143) [0xb73b9a33] ?? ??:0    \n#3   /lib/i386-linux-gnu/libc.so.6(+0x27757) [0xb73af757] ??    ??:0    \n#4   /lib/i386-linux-gnu/libc.so.6(+0x27807) [0xb73af807] ??    ??:0    \n#5   ./db_test() [0x82967f9] rocksdb::MemTable::MemTable(rocksdb::InternalKeyComparator const&, rocksdb::ImmutableCFOptions const&, rocksdb::MutableCFOptions const&, rocksdb::WriteBuffer*)/home/buildbot/rocksdb/db/memtable.cc:83 (discriminator 1)  \n#6   ./db_test() [0x822ea3a] rocksdb::ColumnFamilyData::ConstructNewMemtable(rocksdb::MutableCFOptions const&)  /home/buildbot/rocksdb/db/column_family.cc:513  \n#7   ./db_test() [0x822eaac] rocksdb::ColumnFamilyData::SetMemtable(rocksdb::MemTable*) /home/buildbot/rocksdb/./db/column_family.h:228 \n#8   ./db_test() [0x82b7353] rocksdb::ColumnFamilyData::SetLogNumber(unsigned long long)    /home/buildbot/rocksdb/./db/column_family.h:192 (discriminator 1)   \n#9   ./db_test() [0x82b96f2] rocksdb::VersionSet::Recover(std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, bool)  /home/buildbot/rocksdb/db/version_set.cc:2141   \n#10  ./db_test() [0x8267548] rocksdb::DBImpl::Recover(std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, bool, bool)    /home/buildbot/rocksdb/db/db_impl.cc:855    \n#11  ./db_test() [0x82684f6] rocksdb::DB::Open(rocksdb::DBOptions const&, std::string const&, std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> >*, rocksdb::DB**)  /home/buildbot/rocksdb/db/db_impl.cc:3961   \n#12  ./db_test() [0x82695be] rocksdb::DB::Open(rocksdb::Options const&, std::string const&, rocksdb::DB**)  /home/buildbot/rocksdb/db/db_impl.cc:3896   \n#13  ./db_test() [0x81f8724] rocksdb::DBTest::DestroyAndReopen(rocksdb::Options const&) /home/buildbot/rocksdb/db/db_test.cc:782    \n#14  ./db_test() [0x812e4c3] ~Options   /home/buildbot/rocksdb/./include/rocksdb/options.h:1020 \n#15  ./db_test() [0x83c7e2c] void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)   /home/buildbot/rocksdb/third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3880    \n#16  ./db_test() [0x83ba483] testing::Test::Run()   /home/buildbot/rocksdb/third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3895    \n#17  ./db_test() [0x83ba566] testing::TestInfo::Run()   /home/buildbot/rocksdb/third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:4074    \n#18  ./db_test() [0x83ba6f7] std::vector<testing::TestInfo*, std::allocator<testing::TestInfo*> >::size() const /usr/include/c++/4.8/bits/stl_vector.h:646 (discriminator 2)    \n#19  ./db_test() [0x83baa06] testing::internal::UnitTestImpl::RunAllTests() /home/buildbot/rocksdb/third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:2183    \n#20  ./db_test() [0x83bad8a] testing::UnitTest::Run()   /home/buildbot/rocksdb/third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:5681    \n#21  ./db_test() [0x804fd1d] main   /home/buildbot/rocksdb/db/db_test.cc:12954  \n#22  /lib/i386-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0xb73a1a83] ??  ??:0    \n#23  ./db_test() [0x809bdbb] _start ??:?    \nAborted (core dumped)\n```\n\nThis may be a consequence of my hack to fix #634 (#undef **SSE4_2** at the top of util/crc32c.cc) but I doubt it.\n",
	"number": 635,
	"title": "db_test fails on 32 bit system"
}, {
	"body": "Over here https://github.com/facebook/rocksdb/blob/master/util/thread_status_impl.cc#L103 , 1LU is used instead of 1LLU leading to these errors on a 32 bit build:\n\n```\nutil/thread_status_impl.cc: In static member function \u2018static std::map<std::basic_string<char>, long long unsigned int> rocksdb::ThreadStatus::InterpretOperationProperties(rocksdb::ThreadStatus::OperationType, const uint64_t*)\u2019:\nutil/thread_status_impl.cc:103:54: error: left shift count >= width of type [-Werror]\n           {\"OutputLevel\", op_properties[i] % (1LU << 32)});\n                                                      ^\nutil/thread_status_impl.cc:103:56: error: division by zero [-Werror=div-by-zero]\n           {\"OutputLevel\", op_properties[i] % (1LU << 32)});\n                                                        ^\n```\n",
	"number": 633,
	"title": "32 bit shift of 32 bit literal"
}, {
	"body": "The size of rocksdb (RocksDB version: 3.11.0) is 300G . The memory used by rocksdb keeps going up over time while compacting. Our OOM killer kills the process when it hits 64GB.   while  optionals about compression reseted with  snappy , every thing is ok.\n\nthe stack message is\n#0 0x00000039f040e6fd in write () from /lib64/libpthread.so.0\n#1 0x000000000053bace in rocksdb::(anonymous namespace)::PosixWritableFile::Flush (this=0x5eb2a00)\n\nat util/env_posix.cc:790\n#2 0x00000000005946ba in rocksdb::BlockBasedTableBuilder::Flush (this=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_builder.cc:584\n#3 0x0000000000594870 in rocksdb::BlockBasedTableBuilder::Add (this=0x3cb9af400, key=..., value=...)\n\nat table/block_based_table_builder.cc:546\n---Type to continue, or q to quit---\n#4 0x00000000005761bd in rocksdb::CompactionJob::ProcessKeyValueCompaction (this=0x7f373ddb6d00,\n\nimm_micros=0x7f373ddb5f78, input=0x43a0ba1c0, is_compaction_v2=false) at db/compaction_job.cc:879\n#5 0x00000000005795f2 in rocksdb::CompactionJob::Run (this=0x7f373ddb6d00) at db/compaction_job.cc:360\n#6 0x00000000004c0335 in rocksdb::DBImpl::BackgroundCompaction (this=0x2a08000, madeProgress=0x7f373ddb714e,\n\njob_context=0x7f373ddb7170, log_buffer=0x7f373ddb7330) at db/db_impl.cc:2395\n#7 0x00000000004cbedf in rocksdb::DBImpl::BackgroundCallCompaction (this=0x2a08000) at db/db_impl.cc:2106\n#8 0x000000000053b60f in BGThread (arg=Unhandled dwarf expression opcode 0xf3\n\n) at util/env_posix.cc:1702\n#9 rocksdb::(anonymous namespace)::PosixEnv::ThreadPool::BGThreadWrapper (arg=Unhandled dwarf expression opcode 0xf3\n\nhere is the options\n\n[WARN] Options.create_if_missing: 1\n[WARN] Options.paranoid_checks: 1\n[WARN] Options.env: 0x839600\n[WARN] Options.info_log: 0x29f8000\n[WARN] Options.max_open_files: 5000\n[WARN] Options.max_total_wal_size: 0\n[WARN] Options.disableDataSync: 0\n[WARN] Options.use_fsync: 0\n[WARN] Options.max_log_file_size: 536870912\n[WARN] Options.max_manifest_file_size: 18446744073709551615\n[WARN] Options.log_file_time_to_roll: 0\n[WARN] Options.keep_log_file_num: 1000\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.create_missing_column_families: 0\n[WARN] Options.db_log_dir: ./log/\n[WARN] Options.wal_dir: /ssd/data/morpheus/userprofile\n[WARN] Options.table_cache_numshardbits: 4\n[WARN] Options.delete_obsolete_files_period_micros: 21600000000\n[WARN] Options.max_background_compactions: 2\n[WARN] Options.max_background_flushes: 2\n[WARN] Options.WAL_ttl_seconds: 0\n[WARN] Options.WAL_size_limit_MB: 0\n[WARN] Options.manifest_preallocation_size: 4194304\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.is_fd_close_on_exec: 1\n[WARN] Options.stats_dump_period_sec: 600\n[WARN] Options.advise_random_on_open: 1\n[WARN] Options.db_write_buffer_size: 0\n[WARN] Options.access_hint_on_compaction_start: NORMAL\n[WARN] Options.use_adaptive_mutex: 0\n[WARN] Options.rate_limiter: (nil)\n[WARN] Options.bytes_per_sync: 0\n[WARN] Options.wal_bytes_per_sync: 0\n[WARN] Options.enable_thread_tracking: 0\nCompression algorithms supported:\nSnappy supported: 1\nZlib supported: 1\nBzip supported: 1\nLZ4 supported: 1\nRecovering from manifest\n--------------- Options for\n[WARN] Options.error_if_exists: 0\n[WARN] Options.create_if_missing: 1\n[WARN] Options.paranoid_checks: 1\n[WARN] Options.env: 0x839600\n[WARN] Options.info_log: 0x29f8000\n[WARN] Options.max_open_files: 5000\n[WARN] Options.max_total_wal_size: 0\n[WARN] Options.disableDataSync: 0\n[WARN] Options.use_fsync: 0\n[WARN] Options.max_log_file_size: 536870912\n[WARN] Options.max_manifest_file_size: 18446744073709551615\n[WARN] Options.log_file_time_to_roll: 0\n[WARN] Options.keep_log_file_num: 1000\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.create_missing_column_families: 0\n[WARN] Options.db_log_dir: ./log/\n[WARN] Options.wal_dir: /ssd/data/morpheus/userprofile\n[WARN] Options.table_cache_numshardbits: 4\n[WARN] Options.delete_obsolete_files_period_micros: 21600000000\n[WARN] Options.max_background_compactions: 2\n[WARN] Options.max_background_flushes: 2\n[WARN] Options.WAL_ttl_seconds: 0\n[WARN] Options.WAL_size_limit_MB: 0\n[WARN] Options.manifest_preallocation_size: 4194304\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.is_fd_close_on_exec: 1\n[WARN] Options.stats_dump_period_sec: 600\n[WARN] Options.advise_random_on_open: 1\n[WARN] Options.db_write_buffer_size: 0\n[WARN] Options.access_hint_on_compaction_start: NORMAL\n[WARN] Options.use_adaptive_mutex: 0\n[WARN] Options.rate_limiter: (nil)\n[WARN] Options.bytes_per_sync: 0\n[WARN] Options.wal_bytes_per_sync: 0\n[WARN] Options.enable_thread_tracking: 0\n[WARN] Options.comparator: rocksdb.InternalKeyComparator:leveldb.BytewiseComparator\n[WARN] Options.merge_operator: rocks_merge\n[WARN] Options.compaction_filter: None\n[WARN] Options.compaction_filter_factory: DefaultCompactionFilterFactory\n[WARN] Options.compaction_filter_factory_v2: DefaultCompactionFilterFactoryV2\n[WARN] Options.memtable_factory: HashLinkListRepFactory\n[WARN] Options.table_factory: BlockBasedTable\n[WARN] table_factory options:\n\n[WARN] Options.write_buffer_size: 536870912\n[WARN] Options.max_write_buffer_number: 4\n[WARN] Options.compression[0]: NoCompression\n[WARN] Options.compression[1]: Snappy\n[WARN] Options.compression[2]: Snappy\n[WARN] Options.compression[3]: Zlib\n[WARN] Options.compression[4]: Zlib\n[WARN] Options.compression[5]: Zlib\n[WARN] Options.compression[6]: Zlib\n[WARN] Options.prefix_extractor: rocksdb.Noop\n[WARN] Options.num_levels: 7\n[WARN] Options.min_write_buffer_number_to_merge: 2\n[WARN] Options.purge_redundant_kvs_while_flush: 1\n[WARN] Options.compression_opts.window_bits: -14\n[WARN] Options.compression_opts.level: -1\n[WARN] Options.compression_opts.strategy: 0\n[WARN] Options.level0_file_num_compaction_trigger: 4\n[WARN] Options.level0_slowdown_writes_trigger: 20\n[WARN] Options.level0_stop_writes_trigger: 48\n[WARN] Options.max_mem_compaction_level: 2\n[WARN] Options.target_file_size_base: 134217728\n[WARN] Options.target_file_size_multiplier: 1\n[WARN] Options.max_bytes_for_level_base: 1073741824\n[WARN] Options.level_compaction_dynamic_level_bytes: 0\n[WARN] Options.max_bytes_for_level_multiplier: 10\n[WARN] Options.max_bytes_for_level_multiplier_addtl[0]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[1]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[2]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[3]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[4]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[5]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[6]: 1\n[WARN] Options.max_sequential_skip_in_iterations: 8\n[WARN] Options.expanded_compaction_factor: 25\n[WARN] Options.source_compaction_factor: 1\n[WARN] Options.max_grandparent_overlap_factor: 10\n[WARN] Options.arena_block_size: 53687091\n[WARN] Options.soft_rate_limit: 0.00\n[WARN] Options.hard_rate_limit: 0.00\n[WARN] Options.rate_limit_delay_max_milliseconds: 1000\n[WARN] Options.disable_auto_compactions: 0\n[WARN] Options.purge_redundant_kvs_while_flush: 1\n[WARN] Options.filter_deletes: 1\n[WARN] Options.verify_checksums_in_compaction: 1\n[WARN] Options.compaction_style: 0\n[WARN] Options.compaction_options_universal.size_ratio: 1\n[WARN] Options.compaction_options_universal.min_merge_width: 2\n[WARN] Options.compaction_options_universal.max_merge_width: 4294967295\n[WARN] Options.compaction_options_universal.max_size_amplification_percent: 200\n[WARN] Options.compaction_options_universal.compression_size_percent: -1\n[WARN] Options.compaction_options_fifo.max_table_files_size: 1073741824\n[WARN] Options.table_properties_collectors: \n[WARN] Options.inplace_update_support: 1\n[WARN] Options.inplace_update_num_locks: 10000\n",
	"number": 629,
	"title": "Memory leake with   zlib  compression optional."
}, {
	"body": "Are there any more details on how exactly tailing iterators behave?  The documentation says data \"is not guaranteed to be seen.\"\n\nI have a logging application where I are append to a topic (`key=topic:auto_increment`).  I'd like to tail this log.  I can keep track of the tail and when things are written.  I'm trying to find the most efficient way to tail this topic.   Am I right to assume that even if I create a tailing iterator, I cannot guarantee that I'll see the appended records even if I wait until `Put` returns to `Next`?  Is there a better approach than just using `Get` and manually iterating.\n\nDocs:\n\n> Not all new data is guaranteed to be available to a tailing iterator. Seek() or SeekToFirst() on a tailing iterator can be thought of as creating an implicit snapshot -- anything written after it may, but is not guaranteed to be seen.\n\nhttps://github.com/facebook/rocksdb/wiki/Tailing-Iterator\n",
	"number": 628,
	"title": "Clarify tailing iterator semantics"
}, {
	"body": "When compiling the rocksdb example from http://rocksdb.org/, we found the error of undefined reference to `rocksdb::ReadOptions::ReadOptions()'.\nHad someone encountered the same problem?\n- Complie env.\n  OS: Ubuntu 14.04.1 LTS, g++ 4.8.2\n- Install the rocksdb lib by cmd\n  `\n  sudo make install \n  `\n- Compile the example by:\n  `\n  g++ rocksdb_example.cc -o rocksdb_example -L/usr/local/lib -lrocksdb -std=c++11\n  `\n\nOutput:\n\n```\n g++ rocksdb_example.cc -o rocksdb_example -L/usr/local/lib -lrocksdb -std=c++11 \n/tmp/ccNymlEb.o: In function `main':\nrocksdb_example.cc:(.text+0x20b): undefined reference to `rocksdb::ReadOptions::ReadOptions()'\ncollect2: error: ld returned 1 exit status\n\n```\n\nThe code of rocksdb_example.cc:\n\n```\n#include <assert.h>\n#include \"rocksdb/db.h\"\n\nint main() {\n  rocksdb::DB* db;\n  rocksdb::Options options;\n  options.create_if_missing = true;\n  rocksdb::Status status =\n    rocksdb::DB::Open(options, \"/tmp/testdb\", &db);\n  assert(status.ok());\n\n  std::string key1 = \"key1\";\n  std::string key2 = \"key2\";\n  std::string value;\n  rocksdb::Status s = db->Get(rocksdb::ReadOptions(), key1, &value);\n  if (s.ok()) s = db->Put(rocksdb::WriteOptions(), key2, value);\n  if (s.ok()) s = db->Delete(rocksdb::WriteOptions(), key1);\n\n  /* open the db as described above */\n  /* do something with db */\n  delete db;\n}\n```\n",
	"number": 627,
	"title": "Undefined reference to `rocksdb::ReadOptions::ReadOptions()' when compiling the example."
}, {
	"body": "The size of rocksdb (RocksDB version: 3.11.0)   is  300G .  The memory used by rocksdb keeps going up over time while compacting. Our OOM killer kills the process when it hits  64GB. \n\nthe stack message is \n#0  0x00000039f040e6fd in write () from /lib64/libpthread.so.0\n#1  0x000000000053bace in rocksdb::(anonymous namespace)::PosixWritableFile::Flush (this=0x5eb2a00)\n\n```\nat util/env_posix.cc:790\n```\n#2  0x00000000005946ba in rocksdb::BlockBasedTableBuilder::Flush (this=Unhandled dwarf expression opcode 0xf3\n\n) at table/block_based_table_builder.cc:584\n#3  0x0000000000594870 in rocksdb::BlockBasedTableBuilder::Add (this=0x3cb9af400, key=..., value=...)\n\n```\nat table/block_based_table_builder.cc:546\n```\n\n---Type <return> to continue, or q <return> to quit---\n#4  0x00000000005761bd in rocksdb::CompactionJob::ProcessKeyValueCompaction (this=0x7f373ddb6d00,\n\n```\nimm_micros=0x7f373ddb5f78, input=0x43a0ba1c0, is_compaction_v2=false) at db/compaction_job.cc:879\n```\n#5  0x00000000005795f2 in rocksdb::CompactionJob::Run (this=0x7f373ddb6d00) at db/compaction_job.cc:360\n#6  0x00000000004c0335 in rocksdb::DBImpl::BackgroundCompaction (this=0x2a08000, madeProgress=0x7f373ddb714e,\n\n```\njob_context=0x7f373ddb7170, log_buffer=0x7f373ddb7330) at db/db_impl.cc:2395\n```\n#7  0x00000000004cbedf in rocksdb::DBImpl::BackgroundCallCompaction (this=0x2a08000) at db/db_impl.cc:2106\n#8  0x000000000053b60f in BGThread (arg=Unhandled dwarf expression opcode 0xf3\n\n) at util/env_posix.cc:1702\n#9  rocksdb::(anonymous namespace)::PosixEnv::ThreadPool::BGThreadWrapper (arg=Unhandled dwarf expression opcode 0xf3\n\nhere is the  options \n\n[WARN] Options.create_if_missing: 1\n[WARN] Options.paranoid_checks: 1\n[WARN] Options.env: 0x839600\n[WARN] Options.info_log: 0x29f8000\n[WARN] Options.max_open_files: 5000\n[WARN] Options.max_total_wal_size: 0\n[WARN] Options.disableDataSync: 0\n[WARN] Options.use_fsync: 0\n[WARN] Options.max_log_file_size: 536870912\n[WARN] Options.max_manifest_file_size: 18446744073709551615\n[WARN] Options.log_file_time_to_roll: 0\n[WARN] Options.keep_log_file_num: 1000\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.create_missing_column_families: 0\n[WARN] Options.db_log_dir: ./log/\n[WARN] Options.wal_dir: /ssd/data/morpheus/userprofile\n[WARN] Options.table_cache_numshardbits: 4\n[WARN] Options.delete_obsolete_files_period_micros: 21600000000\n[WARN] Options.max_background_compactions: 2\n[WARN] Options.max_background_flushes: 2\n[WARN] Options.WAL_ttl_seconds: 0\n[WARN] Options.WAL_size_limit_MB: 0\n[WARN] Options.manifest_preallocation_size: 4194304\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.is_fd_close_on_exec: 1\n[WARN] Options.stats_dump_period_sec: 600\n[WARN] Options.advise_random_on_open: 1\n[WARN] Options.db_write_buffer_size: 0\n[WARN] Options.access_hint_on_compaction_start: NORMAL\n[WARN] Options.use_adaptive_mutex: 0\n[WARN] Options.rate_limiter: (nil)\n[WARN] Options.bytes_per_sync: 0\n[WARN] Options.wal_bytes_per_sync: 0\n[WARN] Options.enable_thread_tracking: 0\nCompression algorithms supported:\nSnappy supported: 1\nZlib supported: 1\nBzip supported: 1\nLZ4 supported: 1\nRecovering from manifest\n--------------- Options for\n[WARN] Options.error_if_exists: 0\n[WARN] Options.create_if_missing: 1\n[WARN] Options.paranoid_checks: 1\n[WARN] Options.env: 0x839600\n[WARN] Options.info_log: 0x29f8000\n[WARN] Options.max_open_files: 5000\n[WARN] Options.max_total_wal_size: 0\n[WARN] Options.disableDataSync: 0\n[WARN] Options.use_fsync: 0\n[WARN] Options.max_log_file_size: 536870912\n[WARN] Options.max_manifest_file_size: 18446744073709551615\n[WARN] Options.log_file_time_to_roll: 0\n[WARN] Options.keep_log_file_num: 1000\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.create_missing_column_families: 0\n[WARN] Options.db_log_dir: ./log/\n[WARN] Options.wal_dir: /ssd/data/morpheus/userprofile\n[WARN] Options.table_cache_numshardbits: 4\n[WARN] Options.delete_obsolete_files_period_micros: 21600000000\n[WARN] Options.max_background_compactions: 2\n[WARN] Options.max_background_flushes: 2\n[WARN] Options.WAL_ttl_seconds: 0\n[WARN] Options.WAL_size_limit_MB: 0\n[WARN] Options.manifest_preallocation_size: 4194304\n[WARN] Options.allow_os_buffer: 1\n[WARN] Options.allow_mmap_reads: 0\n[WARN] Options.allow_mmap_writes: 0\n[WARN] Options.is_fd_close_on_exec: 1\n[WARN] Options.stats_dump_period_sec: 600\n[WARN] Options.advise_random_on_open: 1\n[WARN] Options.db_write_buffer_size: 0\n[WARN] Options.access_hint_on_compaction_start: NORMAL\n[WARN] Options.use_adaptive_mutex: 0\n[WARN] Options.rate_limiter: (nil)\n[WARN] Options.bytes_per_sync: 0\n[WARN] Options.wal_bytes_per_sync: 0\n[WARN] Options.enable_thread_tracking: 0\n[WARN] Options.comparator: rocksdb.InternalKeyComparator:leveldb.BytewiseComparator\n[WARN] Options.merge_operator: rocks_merge\n[WARN] Options.compaction_filter: None\n[WARN] Options.compaction_filter_factory: DefaultCompactionFilterFactory\n[WARN] Options.compaction_filter_factory_v2: DefaultCompactionFilterFactoryV2\n[WARN] Options.memtable_factory: HashLinkListRepFactory\n[WARN] Options.table_factory: BlockBasedTable\n[WARN] table_factory options:\n\n[WARN] Options.write_buffer_size: 536870912\n[WARN] Options.max_write_buffer_number: 4\n[WARN] Options.compression[0]: NoCompression\n[WARN] Options.compression[1]: Snappy\n[WARN] Options.compression[2]: Snappy\n[WARN] Options.compression[3]: Zlib\n[WARN] Options.compression[4]: Zlib\n[WARN] Options.compression[5]: Zlib\n[WARN] Options.compression[6]: Zlib\n[WARN] Options.prefix_extractor: rocksdb.Noop\n[WARN] Options.num_levels: 7\n[WARN] Options.min_write_buffer_number_to_merge: 2\n[WARN] Options.purge_redundant_kvs_while_flush: 1\n[WARN] Options.compression_opts.window_bits: -14\n[WARN] Options.compression_opts.level: -1\n[WARN] Options.compression_opts.strategy: 0\n[WARN] Options.level0_file_num_compaction_trigger: 4\n[WARN] Options.level0_slowdown_writes_trigger: 20\n[WARN] Options.level0_stop_writes_trigger: 48\n[WARN] Options.max_mem_compaction_level: 2\n[WARN] Options.target_file_size_base: 134217728\n[WARN] Options.target_file_size_multiplier: 1\n[WARN] Options.max_bytes_for_level_base: 1073741824\n[WARN] Options.level_compaction_dynamic_level_bytes: 0\n[WARN] Options.max_bytes_for_level_multiplier: 10\n[WARN] Options.max_bytes_for_level_multiplier_addtl[0]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[1]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[2]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[3]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[4]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[5]: 1\n[WARN] Options.max_bytes_for_level_multiplier_addtl[6]: 1\n[WARN] Options.max_sequential_skip_in_iterations: 8\n[WARN] Options.expanded_compaction_factor: 25\n[WARN] Options.source_compaction_factor: 1\n[WARN] Options.max_grandparent_overlap_factor: 10\n[WARN] Options.arena_block_size: 53687091\n[WARN] Options.soft_rate_limit: 0.00\n[WARN] Options.hard_rate_limit: 0.00\n[WARN] Options.rate_limit_delay_max_milliseconds: 1000\n[WARN] Options.disable_auto_compactions: 0\n[WARN] Options.purge_redundant_kvs_while_flush: 1\n[WARN] Options.filter_deletes: 1\n[WARN] Options.verify_checksums_in_compaction: 1\n[WARN] Options.compaction_style: 0\n[WARN] Options.compaction_options_universal.size_ratio: 1\n[WARN] Options.compaction_options_universal.min_merge_width: 2\n[WARN] Options.compaction_options_universal.max_merge_width: 4294967295\n[WARN] Options.compaction_options_universal.max_size_amplification_percent: 200\n[WARN] Options.compaction_options_universal.compression_size_percent: -1\n[WARN] Options.compaction_options_fifo.max_table_files_size: 1073741824\n[WARN] Options.table_properties_collectors: \n[WARN] Options.inplace_update_support: 1\n[WARN] Options.inplace_update_num_locks: 10000\n",
	"number": 626,
	"title": "Something wrong with memory  while  doing compaction."
}, {
	"body": "Hi, all.\n\nI am trying to dump info from an SST file.\nThe key-value pairs are in the form \n\"key-tttt-xxxx\" : \"val-tttt-xxxx\" (xxxx is index numbers, starting from 0)\n\n== 1 ==\nHowever, for some files, I got assertion failure when I do\n\n./sst_dump --command=raw --file=/path/to/file\n\nIt complains:\n\"sst_dump: ./db/dbformat.h:89: rocksdb::Slice rocksdb::ExtractUserKey(const rocksdb::Slice&): Assertion `internal_key.size() >= 8' failed.\"\n\nBesides, \"./sst_dump --command=scan ... \"  works fine.\n\n== 2 ==\nAlso, I tried to use SstFileReader class to dump the table, for example\n\n```\nSstFileReader sst_reader(sst_file, false, false);\nsst_reader.DumpTable(dump_file);\n```\n\nBut it had segmentation fault. With gdb, the backtrace is \n\n---\n\nToString (hex=true, this=<optimized out>) at ./include/rocksdb/slice.h:82\n82          snprintf(buf, 10, \"%02X\", (unsigned char)data_[i]);\n(gdb) bt\n#0  ToString (hex=true, this=<optimized out>) at ./include/rocksdb/slice.h:82\n#1  rocksdb::BlockBasedTable::DumpDataBlocks (this=this@entry=0x67e5a0,\n\n```\nout_file=out_file@entry=0x67d440) at table/block_based_table_reader.cc:1621\n```\n#2  0x00007ffff7ab4bea in rocksdb::BlockBasedTable::DumpTable (this=0x67e5a0,\n\n```\nout_file=0x67d440) at table/block_based_table_reader.cc:1522\n```\n#3  0x00007ffff7b6cb4d in rocksdb::SstFileReader::DumpTable (\n\n```\nthis=this@entry=0x7fffffffd4d0, out_filename=...)\nat util/sst_dump_tool.cc:105\n```\n#4  0x0000000000401ed7 in main (argc=<optimized out>, argv=<optimized out>)\n\n```\nat sst_dump.cc:24\n```\n\n---\n\nAny idea why? \nI guess I have to understand more about the internal key/value representation before figuring out the causes.\n\nThanks!\n",
	"number": 624,
	"title": "sst_dump seems having some problems"
}, {
	"body": "I have a read-bound application (aligning DNA reads to a graph reference genome). I notice that I can align about 1000 reads/second on a single CPU. However, using 32 provides only 8000 reads/second. Obviously there is some contention preventing near-linear scaling. Is it possible this is driven by rocksdb itself? Or am I seeing some kind of system-level contention?\n\nIn principle, what might be limiting the performance when I increase the number of threads? Is there any locking in read-only mode? For instance, is there a shared cache? I believe I've set up cache sharding correctly but I am not sure.\n",
	"number": 619,
	"title": "scaling is not linear per CPU"
}, {
	"body": "Is there a way to load the compressed tables into memory without using a tmpfs or ramfs?\n",
	"number": 618,
	"title": "in memory without ramfs or tmpfs"
}, {
	"body": "This pull request adds a compression field to sst_dump's show-properties report.\n\nThe reason is that there is no real way to figure out how compression is actually being used by RocksDB. The LOG specifies what compression is requested, but there are some reasons why the actual data may be compressed differently. For example, compression libraries may not be linked properly (our motivation for this patch), or compression is changed.\n\nThis patch allows a user to inspect the sst files directly and verify what compression scheme is actually used.\n\nSigned-off-by: Adam Lugowski alugowski@gmail.com\n",
	"number": 612,
	"title": "Added compression report to sst_dump"
}, {
	"body": "I am trying to atomically write 5,305,512 key/value pairs into RocksDB. The performance of this feels slow to me, at about 60 seconds. \n\nI am writing to a single WriteBatchWithIndex through calling `org.rocksdb.WriteBatchWithIndex#put(byte[], int, byte[], int, long)`, this takes some ~50 seconds. Whereas subsequently writing that batch into Rocks using `org.rocksdb.RocksDB#write(org.rocksdb.WriteOptions, org.rocksdb.WriteBatchWithIndex)` takes just ~8 seconds.\n\nIs WriteBatchWithIndex known to be slow? I was under the impression that it was all in-memory in a skip-list and so would be quite quick.\n",
	"number": 608,
	"title": "WriteBatchWithIndex seems slow"
}, {
	"body": "All  the  read  and write opertion are blocked \u3002 the  process seems deadlock.\n\nThere is  the  thread  stack info\n#0  0x0000003864a0b5bc in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x000000000050468d in rocksdb::port::CondVar::Wait (this=Unhandled dwarf expression opcode 0xf3\n\n) at port/port_posix.cc:83\n#2  0x00000000005365d5 in WaitInternal (this=0x7f6aa95e5448) at util/instrumented_mutex.cc:50\n#3  rocksdb::InstrumentedCondVar::Wait (this=0x7f6aa95e5448) at util/instrumented_mutex.cc:38\n#4  0x0000000000503855 in rocksdb::WriteThread::EnterWriteThread (this=0x11682a8, w=<value optimized out>, expiration_time=0) at db/write_thread.cc:22\n#5  0x00000000004c74b3 in rocksdb::DBImpl::Write (this=0x1168000, write_options=..., my_batch=0x7f6aa95e5688) at db/db_impl.cc:3044\n",
	"number": 607,
	"title": "Sometimes,  a deadlock occurs  when  manual compaction."
}, {
	"body": "Hello and thank you for RocksDB,\n\n  ldb (ldb_cmd.h HexToString) usage of sscanf is O(N) making ldb O(N^2).  I noticed this on RHEL7 with glibc-2.17-55.el7_0.5.x86_64 while loading some large hex values.  Here is a reproducer for a bash shell:\n\n```\n> for i in {1,2,4,8}; do rm -rf /tmp/tmp.rdb ; perl -e 'printf qq{%d ==> 0x%s\\n},1,0 x $ARGV[0]' ${i}000000 | time -v ./ldb --create_if_missing --db=/tmp/tmp.rdb --value_hex load 2>&1 | egrep 'User|Elapsed' ; done\n   User time (seconds): 9.45\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 0:09.48\n   User time (seconds): 39.79\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 0:40.14\n   User time (seconds): 174.03\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 2:55.41\n   User time (seconds): 774.66\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 13:02.68\n```\n\nJust to demonstrate the perl time and max line length:\n\n```\n> for i in {1,2,4,8}; do perl -e 'printf qq{%d ==> 0x%s\\n},1,0 x $ARGV[0]' ${i}000000 | time -v wc -L 2>&1 | egrep 'User|Elapsed|00000' ; done\n1000008\n   User time (seconds): 0.01\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.01\n2000008\n   User time (seconds): 0.02\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.02\n4000008\n   User time (seconds): 0.04\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.05\n8000008\n   User time (seconds): 0.08\n   Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.10\n```\n\nAn ltrace while ldb was running:\n\n```\n> ltrace -fcp `pidof ldb`\ntime     seconds  usecs/call     calls      function\n------ ----------- ----------- --------- --------------------\n100.00    3.481690         528      6589 sscanf\n------ ----------- ----------- --------- --------------------\n100.00    3.481690                  6589 total\n```\n\nThis is the glibc bug:\n\n  https://sourceware.org/bugzilla/show_bug.cgi?id=17577\n\nand a stackoverflow discussion:\n\n  http://stackoverflow.com/questions/23923924/why-is-glibcs-sscanf-vastly-slower-than-fscanf-on-linux\n\nCheers.\n",
	"number": 605,
	"title": "ldb (ldb_cmd.h HexToString) usage of sscanf is O(N) which makes ldb O(N^2)"
}, {
	"body": "two issues:\n1) correct command line to build for iOS is: declare -x TARGET_OS=IOS;make static_lib\n2) error occurs in file util/file_util.cc at line:\n      s = srcfile->Read(bytes_to_read, &slice, buffer);\nthe error is:\nutil/file_util.cc:43:25: error: implicit conversion loses integer precision: 'uint64_t' (aka 'unsigned long long') to 'size_t'\n      (aka 'unsigned long') [-Werror,-Wshorten-64-to-32]\n      s = srcfile->Read(bytes_to_read, &slice, buffer);\n          ~~~~~~~       ^~~~~~~~~~~~~\n\nlibrary was build with latest Xcode version 6.3.1 (6D1002)\n",
	"number": 594,
	"title": "build for iOS"
}, {
	"body": "We've observed some situations in production where all writes fail with \"Invalid column family specified in write batch\".  We're still investigating, but in an attempt to reproduce the behavior, we've noticed that once a write fails because it used a dropped column family, a subsequent write will fail as well.\n\nis this expected?  if yes, how to get writes working again?\n\nwith the code below on OSX (our observations were originally rhel6, i'll try reproduce there if needed):\n\n```\n2:51 ~/tmp ditzy& git --git-dir ~/src/rocksdb/.git log --oneline | sed 1q\n149ef19 Bump up patch to 3.10.1\n2:51 ~/tmp ditzy& c++ --version\nApple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn)\nTarget: x86_64-apple-darwin14.3.0\nThread model: posix\n2:51 ~/tmp ditzy& c++ -std=c++11 -W -Wall -Wno-unused-parameter -Werror cft4.cc -I ~/src/rocksdb/include -L ~/src/rocksdb -lrocksdb -lz -lbz2\n2:51 ~/tmp ditzy& ./a.out\n2:51 ~/tmp ditzy& ./a.out f\nfatal: cft4.cc:66 Put DEFAULT: 4 Invalid argument: Invalid column family specified in write batch\n```\n\ncft4.cc is below\n\n```\n#include <iostream>\n#include <string>\n#include <vector>\n#include <cstdlib>\n\n#include <rocksdb/db.h>\n#include <rocksdb/options.h>\n#include <rocksdb/slice.h>\n\nstatic void fatal(const char* fn, int line,\n        const std::string& what, const rocksdb::Status& s)\n{\n    std::cerr << \"fatal: \" << fn << ':' << line << ' '\n        << what << \": \" << s.code() << ' ' << s.ToString() << '\\n';\n    std::exit(1);\n}\n\n#define FATAL(w,s) fatal(__FILE__, __LINE__, (w), (s))\n\nint main(int argc, char**)\n{\n    using namespace rocksdb;\n\n    Options options;\n    options.create_if_missing = true;\n    options.create_missing_column_families = true;\n    options.error_if_exists = true;\n\n    const std::string dbname(\"cft.test.db\");\n\n    // cleanup from a previous run\n    DestroyDB(dbname, options);\n\n    DB* db = nullptr;\n    std::vector<ColumnFamilyHandle*> cfh;\n\n    const std::vector<ColumnFamilyDescriptor> cfd = {\n        { \"1\", options },\n        { kDefaultColumnFamilyName, options },\n    };\n\n    auto s = DB::Open(options, dbname, cfd, &cfh, &db);\n    if (!s.ok())\n        FATAL(\"DB::Open\", s);\n\n    if (argc != 1) {\n        s = db->DropColumnFamily(cfh[0]);\n        if (!s.ok())\n            FATAL(\"DropColumnFamily\", s);\n    }\n\n    const auto k = Slice{\"k\"};\n    const auto v = Slice{\"v\"};\n\n    // attempt to write to the dropped column family\n    s = db->Put(WriteOptions(), cfh[0], k, v);\n    if (s.ok())\n        std::exit(0);\n\n    if (!s.IsInvalidArgument())\n        FATAL(\"Put cfh[0]\", s);\n\n    // write to the default column family\n    s = db->Put(WriteOptions(), k, v);\n    if (!s.ok())\n        FATAL(\"Put DEFAULT\", s);\n\n    return 0;\n}\n```\n",
	"number": 592,
	"title": "Using a closed column family handle leads to persistent error"
}, {
	"body": "https://github.com/facebook/rocksdb/blob/master/examples/c_simple_example.c has no Facebook copyright header.\n",
	"number": 591,
	"title": "https://github.com/facebook/rocksdb/blob/master/examples/c_simple_example.c is missing Copyright header"
}, {
	"body": "I have a rocksdb DB of size ~125GB (version 3.9.1). The memory used by rocksdb keeps going up over time. Our OOM killer kills the process when it hits ~8GB (it gets there in ~1 hour). And the leak seems to be happening along the read path (both point lookups and row iteration).\n\nGoogle's tcmalloc shows that allocations along ReadFilter() and CreateIndexReader() are contributing to the leak. Both of these ultimately point to ReadBlockContents() and UncompressBlockContents(). Any idea what could be happening? The larger question is if there is a way to limit cumulative memory usage by rocksdb to a certain number?\n\nHere are the DB options we're using:\n\nwrite_buffer_size = 64MB\nblock_size = 8192;\nrocksdb::BlockBasedTableOptions table_options;\ntable_options.filter_policy.reset(rocksdb::NewBloomFilterPolicy(10, false));\ntable_options.block_cache = rocksdb::NewLRUCache(128 \\* 1024 \\* 1024);\nmin_write_buffer_number_to_merge = 3;\nmax_write_buffer_number = 5;\ntarget_file_size_base = 64MB;\nmax_bytes_for_level_base = 512MB;\nmax_bytes_for_level_multiplier = 8;\ntarget_file_size_multiplier = 3;\n\nAfter seeing leaks, I tried/considered the following:\n1) Set the block cache to 1MB. It did not help.\n2) Verified that cache_index_and_filter_blocks is false.\n3) Considered setting use_block_based_builder to true, but wanted to get to the bottom of this leak before rebuilding/compacting the DB with this option.\n\nThere are currently 4 levels in the DB, and the largest sstable is 1.9GB. Here are the current db stats:\n\n```\n** Compaction Stats [default] **\nLevel   Files   Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms)     RecordIn   RecordDrop\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0     3/0        122   0.4      0.0     0.0      0.0       0.3      0.3       0.0   0.0      0.0     28.4         9         8    1.176       0.00          0    0.00            0            0\n  L1    11/0        511   1.0      0.7     0.3      0.5       0.6      0.1       0.0   2.4     24.2     20.4        32         1   31.658       0.00          0    0.00      4258695       548196\n  L2    47/0       3956   1.0      0.6     0.1      0.5       0.6      0.1       0.0   4.5     18.8     18.3        34         2   16.779       0.00          0    0.00      4994999       212495\n  L3   100/0      32443   1.0      0.7     0.2      0.5       0.7      0.2       0.0   3.0     17.7     17.6        42         1   41.880       0.00          0    0.00     11112522        20936\n  L4    77/0      89758   0.3      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0         0         0    0.000       0.00          0    0.00            0            0\n Sum   238/0     126789   0.0      2.1     0.6      1.5       2.2      0.8       0.0   8.5     18.4     19.4       117        12    9.709       0.00          0    0.00     20366216       781627\n Int     0/0          0   0.0      1.1     0.3      0.7       1.2      0.4       0.0   9.7     17.0     18.6        63         5   12.678       0.00          0    0.00     13075943       192579\nFlush(GB): accumulative 0.261, interval 0.119\nStalls(secs): 0.000 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 0.000 leveln_slowdown_soft, 0.000 leveln_slowdown_hard\nStalls(count): 0 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 0 leveln_slowdown_soft, 0 leveln_slowdown_hard\n\n** DB Stats **\nUptime(secs): 1001.2 total, 471.2 interval\nCumulative writes: 3111 writes, 4052120 keys, 3111 batches, 1.0 writes per batch, 1.02 GB user ingest\nCumulative WAL: 3111 writes, 3111 syncs, 1.00 writes per sync, 1.02 GB written\nInterval writes: 1103 writes, 1858047 keys, 1103 batches, 1.0 writes per batch, 475.9 MB user ingest\nInterval WAL: 1103 writes, 1103 syncs, 1.00 writes per sync, 0.46 MB written\n```\n\nThank you for taking the time to read this.\n",
	"number": 588,
	"title": "Rocksdb memory leak from BlockBasedTable::ReadFilter and BlockBasedTable::CreateIndexReader"
}, {
	"body": "Apparently CentOS ext4 kernel module has changed the way it handles fallocate and ftruncate and starting version 2.6.32-358 ftruncate is not sufficient to release previously pre-allocated space.\n\nBelow are test results for different combinations of kernel and filesystems. First two show the first centos kernel that got affected. The last two show difference between ext4 and xfs on latest centos kernel. \n\n**2.6.32-279.22.1.el6.x86_64 (on ext4)**\n\nstrace:\n`63475 open(\"/ext4/store_0/548369.sst\", O_RDWR|O_CREAT|O_TRUNC, 0644) = 2783`\n`63475 fallocate(2783, 01, 0, 13841203)\u00a0 = 0`\n`63475 ftruncate(2783, 9533058)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`63475 close(2783) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`63475 open(\"/ext4/store_0/548369.sst\", O_RDONLY) = 2783`\n\n```\n[user@host076.hkg1 ~]# du -sh /ext4/store_0/548369.sst\n9.1M /ext4/store_0/548369.sst\n[user@host076.hkg1 ~]# du -sh /ext4/store_0/548369.sst --apparent-size\n9.1M /ext4/store_0/548369.sst\n```\n\n**2.6.32-358.2.1.el6.x86_64 (on ext4)**\n\nstrace:\n`23896 open(\"/ext4/store_0/409013.sst\", O_RDWR|O_CREAT|O_TRUNC, 0644) = 16512`\n`23896 fallocate(16512, 01, 0, 13841203) = 0`\n`23896 ftruncate(16512, 4862088) \u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`23896 close(16512)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`23896 open(\"/ext4/store_0/409013.sst\", O_RDONLY) = 16512`\n\n```\n[user@host075.sjc2 ~]# du -sh /ext4/store_0/409013.sst\n14M /ext4/store_0/409013.sst\n[user@host075.sjc2 ~]# du -sh /ext4/store_0/409013.sst --apparent-size\n4.7M /ext4/store_0/409013.sst\n```\n\n**2.6.32-504.12.2.el6.x86_64 (on xfs)**\n\nstrace:\n`3057\u00a0 open(\u201c/xfs/store_0/197202.sst\", O_RDWR|O_CREAT|O_TRUNC, 0644) = 20000`\n`3057\u00a0 fallocate(20000, 01, 0, 27682406) = 0`\n`3057\u00a0 ftruncate(20000, 1746917) \u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`3057\u00a0 close(20000)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`3057\u00a0 open(\"/xfs/store_0/197202.sst\", O_RDONLY) = 20000`\n\n```\n[user@host076.sjc2 ~]# du -sh /xfs/store_0/197202.sst\n1.7M /xfs/store_0/197202.sst\n[user@host076.sjc2 ~]# du -sh /xfs/store_0/197202.sst --apparent-size\n1.7M /xfs/store_0/197202.sst\n```\n\n**2.6.32-504.12.2.el6.x86_64 (on ext4)**\n\nstrace:\n\n`3057\u00a0 open(\u201c/ext4/store_0/196435.sst\", O_RDWR|O_CREAT|O_TRUNC, 0644) = 26509`\n`3057\u00a0 fallocate(26509, 01, 0, 27682406) = 0`\n`3057\u00a0 ftruncate(26509, 13813433)\u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`3057\u00a0 close(26509)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = 0`\n`3057\u00a0 open(\"/ext4/store_0/196435.sst\", O_RDONLY) = 26509`\n\n```\n[user@host076.sjc2 ~]# du -sh /ext4/store_0/196435.sst\n27M /ext4/store_0/196435.sst\n[user@host076.sjc2 ~]# du -sh /ext4/store_0/196435.sst --apparent-size\n14M /ext4/store_0/196435.sst\n```\n",
	"number": 579,
	"title": "pre-allocated space not released on ext4 partitions"
}, {
	"body": "Added a byte by byte XOR merge operator, and a corresponding test in\ndb/db_bench. Also added two bash scripts to facilitate benchmarks and document properties.\n\nSigned-off-by: Pooya Shareghi shareghi@gmail.com\n",
	"number": 575,
	"title": "Added bytes XOR merge operator"
}, {
	"body": "Has anyone had this problem when trying to run valgrind on a program built with rocksdb?\n\n``` text\nvex amd64->IR: unhandled instruction bytes: 0x8F 0xEA 0xF8 0x10 0xC9 0x3 0x1D 0x0\n==32477== valgrind: Unrecognised instruction at address 0x5b284e.\n==32477==    at 0x5B284E: rocksdb::NewLRUCache(unsigned long, int, int) (cache.cc:167)\n==32477==    by 0x5B2B0F: rocksdb::NewLRUCache(unsigned long) (cache.cc:529)\n==32477==    by 0x5826C0: rocksdb::BlockBasedTableFactory::BlockBasedTableFactory(rocksdb::BlockBasedTableOptions const&) (block_based_table_factory.cc:36)\n```\n\nI'm perplexed. Making everything worse, I can run valgrind on another system, but it reports no errors and the segfault I'm trying to trace never happens on it.\n",
	"number": 566,
	"title": "problem using valgrind"
}, {
	"body": "I   knew the wal is supported\u3002 But the wal is too big to saved for longtime\u3002 why not and when operatelog  is supported?\n",
	"number": 559,
	"title": "operatelog  is support?"
}, {
	"body": "This was crossposted to https://github.com/tecbot/gorocksdb/issues/24 and @rdallman suggest to make a definitive last key, I tried it didn't work(assuming my tests are right) https://github.com/tecbot/gorocksdb/issues/24#issuecomment-85105132 (related tests: https://github.com/yinhm/gorocksdb/commit/f90f8600b28466da6fa4b1aa37c7f6cae9bcb129)\n\nI know this maybe Go client implementation specific question, still, help me.\n\nThis has been bugs me several days,  in TestCustomSliceTransform[1], if I open the old database without Destroy it first, then do the same iteration(without Put changes), the tests fails.\n\nThe docs[2] says: \"If there is one or more keys in the database matching prefix of lookup_key, RocksDB will place the iterator to the key equal or **larger than lookup_key** of the same prefix, as for total ordering mode.\"\n\nI'm not sure is this the expect behavior, it so, how do we do SeekToLast(), isn't this make SeekToLast() useless. \n\n```\n$go test -run TestCustomSliceTransform\n...........x\nFailures:\n\n\n  * /home/yinhm/gopath/src/github.com/yinhm/gorocksdb/slice_transform_test.go \n  Line 100:\n  Expected: '3'\n  Actual:   '6'\n  (Should be equal)\n\n\n12 assertions thus far\n```\n\n[1] https://github.com/yinhm/gorocksdb/commit/db9e6ddd99916eeb6fff5372f9a2c404e85b1db2\n[2] https://github.com/facebook/rocksdb/wiki/Prefix-Seek-API-Changes\n",
	"number": 548,
	"title": "Slice transform seek inconsistent after db reopen?"
}, {
	"body": "If I seek to a position that I know should be immediately past the end of particular range, I can then use `Iterator::Prev()` only if I haven't hit the end of the db.\n\nAs a workaround, I call `Iterator::SeekToLast` if the iterator isn't valid after the first seek. However, it leads to a question: What is the safe way to get to the last element in a range, guarding against the case that \"just past\" your key of interest is another key that you can seek to? Or, how do you get something like a C++ `reverse_iterator` to the db?\n",
	"number": 544,
	"title": "safely reach the last entry in a range"
}, {
	"body": "The rocksjava jdb_bench tool does not have any performance histograms.\n",
	"number": 541,
	"title": "No histogram in rocksjava benchmarks"
}, {
	"body": "Looking at the [official benchmarks](https://github.com/facebook/rocksdb/wiki/Read-Modify-Write-Benchmarks) for the merge operator, it seems that some important parameters and some details are missing.\n## Issue with key size\n\nAlthough not mentioned on the page, by looking at the code, my conclusion is that you were using 16 byte keys. Since you are using a 64bit uniform [I believe] random number generator, the are two problems:\n1. Your keys always look like an 8byte random number followed by 8 bytes of zeros, B1...B800000000. This seems unnecessary. You could simply use an 8byte key.\n2. More importantly, the possibility of collision over the course of 50M keys is very very small. Therefore, I am not sure how many of the merge statements actually caused two values to be merged as the probability of producing the same key is so small! So, does this mean that the 20 micros/ops difference can simply be the time it took the \"update\" benchmark to do the read before the write operation? Am I missing something here?\n3. If my point regarding lack [or infrequency] of key collision is valid, I would suggest re-writing the key generator.\n## Issue with reads after merge (or lack thereof)\n\nI am still learning about the implementation details of RocksDB, so I might be wrong on this one. Let's assume that [the above mentioned issue is resolved and] the random number generator actually produces same keys often, which means the benchmarks will in fact perform \"Read-Modify-Write\" and merges using the merge operator. In the \"update\" benchmark, when we do a RMW operation, a future read on the same key would be quite cheap as it does not incur the cost of merging two values. However, in the \"merge\" benchmark, a `db.merge(k,v)` statement may not necessarily cause an immediate merge of two values. In fact, it is possible that a read will have to pay this cost. Alternatively, depending on when and how compaction is scheduled to happen, a future write of a different key may be stalled. I would suggest that it is important to improve both experiments by adding a read phase to each of them. This will help understand the performance impact of the merge operator on future read operations. For example, assuming a total of N entries for the \"merge\" benchmark, it should look like\n\n```\nfor i in 1 to N\n   generate random key\n   generate random value\n   db.merge(key, value)\nend for\nfor i in 1 to N\n   generate random key\n   db.get(key, value)\nend for\n```\n## Misc\n\nLastly, it would be great if the RMW benchmarks were executed on the same hardware config as the other official [performance benchmarks](https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks). This would make it easier to compare the times.\n",
	"number": 539,
	"title": "Details on Merge performance benchmarks?"
}, {
	"body": "Is there a known issue with random writes on RocksDb Java?  I have recently ported an application from the Java port of LevelDb and see comparable performance for sequential writes, but significantly worse performance for random writes (3-4 times worse and degrading over time).  In my scenario, I'm batch loading about 10 million rows of data that is randomly ordered (basically building an index to another RocksDB table).\n\nThe Java benchmarks you've published do not include the numbers from the random write tests: https://github.com/facebook/rocksdb/wiki/RocksJava-Performance-on-Flash-Storage. Can you elaborate (or maybe publish the data for all the benchmarks you run on C++?).\n\nI've been using 3.8 on OSX 10.9.4.\n\nthanks much.\n",
	"number": 538,
	"title": "Random Write Performance on RocksDb Java"
}, {
	"body": "If I run\n`make all`\nand then run\n`make rocksdbjava`\nI get the following error.\n\n```\nmake[1]: Leaving directory `/home/pshareghi/workspace/rocksdb-stable/java'\nrm -f ./java/target/librocksdbjni-linux64.so\ng++  -g -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -Werror -I. -I./include -std=c++11  -DROCKSDB_PLATFORM_POSIX  -DOS_LINUX -fno-builtin-memcmp -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS=google -DZLIB -DBZIP2 -march=native   -DHAVE_JEMALLOC -O2 -fno-omit-frame-pointer -momit-leaf-frame-pointer -Woverloaded-virtual -Wnon-virtual-dtor -I./java/. -I/usr/lib/jvm/jdk1.6.0_34//include/ -I/usr/lib/jvm/jdk1.6.0_34//include/linux -shared -fPIC -o ./java/target/librocksdbjni-linux64.so ./java/rocksjni/*.cc db/builder.o db/c.o db/column_family.o db/compaction.o db/compaction_job.o db/compaction_picker.o db/db_filesnapshot.o db/dbformat.o db/db_impl.o db/db_impl_debug.o db/db_impl_readonly.o db/db_iter.o db/file_indexer.o db/filename.o db/flush_job.o db/flush_scheduler.o db/forward_iterator.o db/internal_stats.o db/log_reader.o db/log_writer.o db/managed_iterator.o db/memtable_allocator.o db/memtable.o db/memtable_list.o db/merge_helper.o db/merge_operator.o db/repair.o db/table_cache.o db/table_properties_collector.o db/transaction_log_impl.o db/version_builder.o db/version_edit.o db/version_set.o db/wal_manager.o db/write_batch.o db/write_controller.o db/write_thread.o port/stack_trace.o port/port_posix.o table/adaptive_table_factory.o table/block_based_filter_block.o table/block_based_table_builder.o table/block_based_table_factory.o table/block_based_table_reader.o table/block_builder.o table/block.o table/block_hash_index.o table/block_prefix_index.o table/bloom_block.o table/cuckoo_table_builder.o table/cuckoo_table_factory.o table/cuckoo_table_reader.o table/flush_block_policy.o table/format.o table/full_filter_block.o table/get_context.o table/iterator.o table/merger.o table/meta_blocks.o table/plain_table_builder.o table/plain_table_factory.o table/plain_table_index.o table/plain_table_key_coding.o table/plain_table_reader.o table/table_properties.o table/two_level_iterator.o util/arena.o util/auto_roll_logger.o util/bloom.o util/build_version.o util/cache.o util/coding.o util/comparator.o util/crc32c.o util/db_info_dumper.o util/dynamic_bloom.o util/env.o util/env_hdfs.o util/env_posix.o util/file_util.o util/filter_policy.o util/hash.o util/hash_cuckoo_rep.o util/hash_linklist_rep.o util/hash_skiplist_rep.o util/histogram.o util/instrumented_mutex.o util/iostats_context.o utilities/backupable/backupable_db.o utilities/convenience/convenience.o utilities/checkpoint/checkpoint.o utilities/compacted_db/compacted_db_impl.o utilities/document/document_db.o utilities/document/json_document_builder.o utilities/document/json_document.o utilities/geodb/geodb_impl.o utilities/leveldb_options/leveldb_options.o utilities/merge_operators/put.o utilities/merge_operators/string_append/stringappend2.o utilities/merge_operators/string_append/stringappend.o utilities/merge_operators/uint64add.o utilities/redis/redis_lists.o utilities/spatialdb/spatial_db.o utilities/ttl/db_ttl_impl.o utilities/write_batch_with_index/write_batch_with_index.o util/event_logger.o util/ldb_cmd.o util/ldb_tool.o util/log_buffer.o util/logging.o util/memenv.o util/murmurhash.o util/mutable_cf_options.o util/options_builder.o util/options.o util/options_helper.o util/perf_context.o util/rate_limiter.o util/skiplistrep.o util/slice.o util/sst_dump_tool.o util/statistics.o util/status.o util/string_util.o util/sync_point.o util/thread_local.o util/thread_status_impl.o util/thread_status_updater.o util/thread_status_updater_debug.o util/thread_status_util.o util/thread_status_util_debug.o util/vectorrep.o util/xfunc.o util/xxhash.o -lpthread -lrt -lsnappy -lz -lbz2 \n/usr/bin/ld: db/builder.o: relocation R_X86_64_32 against `.rodata' can not be used when making a shared object; recompile with -fPIC\ndb/builder.o: error adding symbols: Bad value\ncollect2: error: ld returned 1 exit status\nmake: *** [rocksdbjava] Error 1\n```\n\nTo resolve the issue I have to run\n`make clean`\nbefore executing \n`make rocksdbjava`\nwhich means most of what `make all` generated have to be deleted and only some of them will be regenerated.\n## Suggestion\n\nSomhow the rocksdbjava target needs to find out (or know) if the object files it needs are compiled with -fPIC or not. An easy solution would be to have rocksdbjava create a file called `compiledWithfPIC` anytime it runs. The other non-java targets should delete this file when they are executed. This way, if the file does not exist, rocksdbjava target will know that it has to delete the object files and recompile them with -fPIC.\n",
	"number": 537,
	"title": "\"make rocksdbjava\" fails if I run it after executing \"make all\""
}, {
	"body": "We're doing a block cache size influence on the performance,and we found that increase the size of block cache almost has no effect on performance, even in large quantity of the data set.\n\nbecause of the influence of os page cache? What circumstances increase block cache size can obtain better performance?\n\nIn addition, I found that  Rocksdb's [Performance Benchmarks](https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks) said that the block cache size is 1G,But the block cache size is set to 1048576 of actual test, and I read the code found that the unit of size maybe byte, is that right?\n_**cs=1048576;** of=500000; si=1000000; ./db_bench --benchmarks=readrandom --disable_seek_compaction=1 --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t --value_size=$vs --block_size=$bs **--cache_size=$cs**_\n",
	"number": 519,
	"title": "can rocksdb's block improve performance?"
}, {
	"body": "Jim M, that was a really quick fix, thanks. I downloaded rocksdb-master.zip, but the making didn't go well for me, these two things need attention, i think:\n1. Add to instructions how to specify an install path or prefix other than /usr/local. I first tried 'make PREFIX=/file/path' as many others have it. Then I looked at the install: tartget and saw an INSTALL_PATH variable needed to be set. I did 'export INSTALL_PATH=/file/path; make' and that worked, but instructions ought to mention. \n2. The install target directories were filled only with .h files, nothing in lib or other directories, only in include/rocksdb. I am installing as regular user. The make process runs unexpectedly fast and there seems to be no .o files made at all. The command 'find . -name *.o' comes up empty in the main source directory .. \n\nNiels L\n",
	"number": 510,
	"title": "make problems"
}, {
	"body": "My compilation of rocksdb is failing with following errrors:\n\n/home/storagevisor/rocksdb/librocksdb.a(format.o): In function `BZip2_Uncompress':\n/home/storagevisor/rocksdb/./port/port_posix.h:374: undefined reference to`BZ2_bzDecompressInit'\n/home/storagevisor/rocksdb/./port/port_posix.h:393: undefined reference to `BZ2_bzDecompress'\n/home/storagevisor/rocksdb/./port/port_posix.h:412: undefined reference to`BZ2_bzDecompressEnd'\n/home/storagevisor/rocksdb/./port/port_posix.h:418: undefined reference to `BZ2_bzDecompressEnd'\n/home/storagevisor/rocksdb/librocksdb.a(block_based_table_builder.o): In function`BZip2_Compress':\n/home/storagevisor/rocksdb/./port/port_posix.h:321: undefined reference to `BZ2_bzCompressInit'\n/home/storagevisor/rocksdb/./port/port_posix.h:340: undefined reference to`BZ2_bzCompress'\n/home/storagevisor/rocksdb/./port/port_posix.h:356: undefined reference to `BZ2_bzCompressEnd'\n/home/storagevisor/rocksdb/./port/port_posix.h:362: undefined reference to`BZ2_bzCompressEnd'\n\nI tried adding -lrt -lz in my LIBS. It used to work but started failing again. I believe I had just recompiled the static rocksdb library when I started seeing these failures again.\n",
	"number": 496,
	"title": "compilation failing on ubuntu 14.04"
}, {
	"body": "Is there any way to stop writing LOG entries for every db access? I have a situation in which I'll have many concurrent threads reading from the same db opened in read-only mode, and the additional log entries for every concurrent thread don't help and simply add files to the database directory.\n",
	"number": 494,
	"title": "turn off writes to LOG for read-only access"
}, {
	"body": "I just spent a fair bit of time and tweaking debugging poor bulk loading performance in rocksdb. One thing that tripped me up is that [simply calling IncreaseParallelism on the options doesn't increase the number of available threads for background flushes](https://github.com/facebook/rocksdb/blob/62ad0a9b19f0be4cefa70b6b32876e764b7f3c11/util/options.cc#L581). Once I realized this and set a high maximum number of flushes I was able to quadruple my application's throughput! rocksdb is no longer the major bottleneck in my process.\n\nIs there any reason this doesn't happen by default? It seems particularly problematic for bulk loads.\n",
	"number": 490,
	"title": "Should Options::IncreaseParallelism also increase Options::max_background_flushes?"
}, {
	"body": "Fixed build issue from https://github.com/facebook/rocksdb/pull/486\n\nIn reply to @mdcallag, the original paper uses CAS(compare and swap) to synchronize updates of MappingTable(a table that maintains logical pages(nodes) to physical pages(nodes)). Since RocksDB guarantees that there is only one writer, my implementation does not need to utilize CAS to ensure that there are no multiple writers.\n\n---\n\nThis is a BW-Tree implementation of Memtable for RocksDB.\nIt was done as part of Facebook Open Academy.\nThe original paper by Microsoft Research can be viewed at http://research.microsoft.com/pubs/178758/bw-tree-icde2013-final.pdf\n",
	"number": 489,
	"title": "BW-Tree implementation for Memtable; Request for review"
}, {
	"body": "Adding LOG parser and modify some logging parts to make it easier to parse.\n\nMainly parse below 5 types:\n- CompactionEvent\n- FlushEvent\n- StatisticsEvent(from DB Stats)\n- OptionsEvent(Once, at the begging of LOG)\n- Delete Event\n\nI wonder where I should put parse_db_log.\\* , so I temporarily put these files in utilities/parse_db_log.\n",
	"number": 463,
	"title": "add DB LOG file parser"
}, {
	"body": "",
	"number": 440,
	"title": "Pull Request, Source Code Refactoring, Extracted Common Functionality"
}, {
	"body": "wiki page says rocksdb support jemalloc , how to enable it?\n",
	"number": 438,
	"title": "how to use jemalloc instead of tcmalloc"
}, {
	"body": "I am storing complex objects into Rocks which are de-composed into thousands of key value pairs. Each transaction in my system has it's own WriteBatch so that new objects are committed atomically. As well as replacing objects with new objects, it is also possible to update individual parts of an object and many updates may happen in a single transaction. A WriteBatch may contain of updates to many different objects, however it should be committed atomically or fail.\n\nI am interested in detecting the issue of Write-Write conflicts in RocksDB. For example if I have two threads A and B and they each take a snapshot of the database. Let's say they both want to update the value of key1 dependent on knowing not just it's current value but also the current value of other keys in the database. The problem I have at the moment is that the first thread that does the write is fine, and succeeds, however the second thread will perform a write of a value determined by an old view of the database. Which leads to it updating the value of key1 to something invalid.\n\nWhat I would like is a mechanism whereby when the second thread attempts to write it's WriteBatch, it realises the value of the key has changed since it took it's snapshot, and so no updates in the WriteBatch are committed. In many ways it is better explained in the first paragraph here - https://en.wikipedia.org/wiki/Snapshot_isolation#Definition\n\nI may be mistaken, but I do not think Rocks offers anything to guard against Write-Write Conflicts?\n\nI am not sure of the best way to achieve this, but one approach that I might suggest would be to create a new write function, called `WriteConditional` which takes a WriteBatch and a `Condition`. The Condition has to evaluate to `true` for the batch to be written atomically, if the condition returns false then no updates from the batch are written. Initially we would have a single Condition object perhaps called, `StableCondition` which takes a Snapshot. The purpose of StableCondition would be to make sure that the keys in the WriteBatch have not been changed in the database since the Snapshot was taken.\nI am not sure how this would be achieved in Rocks, but perhaps it is as simple as checking the sequence number in the snapshot is greater-than-or-equal to the sequence number of each key in the WriteBatch?\n\nI would also be interested to know how others are managing locking and transactions for updating complex objects in Rocks.\n",
	"number": 410,
	"title": "Write-Write Conflict Avoidance"
}, {
	"body": "Hi, I find there is no option for specifying the column family (eg: by name), then how can I access data in non-default column-family by ldb?\n",
	"number": 300,
	"title": "how to use ldb with column families?"
}, {
	"body": "Hi,\n\nWe are seeing high latencies during compaction for GET operation and were wondering if its because of memtable flush.\n\nSymptoms: In some instances when rate of bytes written during compaction go high, we see an increase in memtable miss and block cache miss. During this time, 99pct latency for multiget operation goes up as well.\n\nTheory: Before memtable flush, if user calls GET operation on a key and the key exists in memtable then it will be returned from memtable. If key is retrieved from memtable, then its not stored in block cache. When memtable gets flushed, GET operation will have to go to disk to fetch value for this key if block cache doesn't have data for the same. As lot of data is being written on disk during this time due to compaction, high latencies are observed for this GET operation.\n\nCan someone please comment if there is any way to validate these above assumptions. Also, is there an option through which data will always be written to block cache even if its retrieved from memtable. If such an option exists, then GET will just fetch data from block cache when memtable are flushed and values for these keys won't have to be retrieved from disk.\n\nPlease see below the options we are using to configure DB:\n2014/09/18-09:06:53.724138 7f0826eac700 Git sha rocksdb_build_git_sha:0b1db9e969c4d64ff34a75f173ae2e3058d2367d\n2014/09/18-09:06:53.724267 7f0826eac700 Compile time 20:01:37 Jul 25 2014\n2014/09/18-09:06:53.724284 7f0826eac700          Options.error_if_exists: 0\n2014/09/18-09:06:53.724288 7f0826eac700        Options.create_if_missing: 1\n2014/09/18-09:06:53.724371 7f0826eac700          Options.paranoid_checks: 1\n2014/09/18-09:06:53.724375 7f0826eac700                      Options.env: 0x7f089468f260\n2014/09/18-09:06:53.724379 7f0826eac700                 Options.info_log: 0x7f087c00da40\n2014/09/18-09:06:53.724382 7f0826eac700           Options.max_open_files: 5000\n2014/09/18-09:06:53.724385 7f0826eac700       Options.max_total_wal_size: 0\n2014/09/18-09:06:53.724388 7f0826eac700        Options.disableDataSync: 0\n2014/09/18-09:06:53.724435 7f0826eac700              Options.use_fsync: 0\n2014/09/18-09:06:53.724546 7f0826eac700      Options.max_log_file_size: 0\n2014/09/18-09:06:53.724566 7f0826eac700 Options.max_manifest_file_size: 18446744073709551615\n2014/09/18-09:06:53.724593 7f0826eac700      Options.log_file_time_to_roll: 0\n2014/09/18-09:06:53.724595 7f0826eac700      Options.keep_log_file_num: 1000\n2014/09/18-09:06:53.724598 7f0826eac700        Options.allow_os_buffer: 1\n2014/09/18-09:06:53.724648 7f0826eac700       Options.allow_mmap_reads: 0\n2014/09/18-09:06:53.724650 7f0826eac700      Options.allow_mmap_writes: 0\n2014/09/18-09:06:53.724652 7f0826eac700          Options.create_missing_column_families: 0\n2014/09/18-09:06:53.724711 7f0826eac700                              Options.db_log_dir: \n2014/09/18-09:06:53.724715 7f0826eac700                                 Options.wal_dir: /export/content/data/followfeed-storage-db/660\n2014/09/18-09:06:53.724735 7f0826eac700                Options.table_cache_numshardbits: 4\n2014/09/18-09:06:53.724738 7f0826eac700     Options.table_cache_remove_scan_count_limit: 16\n2014/09/18-09:06:53.724774 7f0826eac700     Options.delete_obsolete_files_period_micros: 21600000000\n2014/09/18-09:06:53.724783 7f0826eac700              Options.max_background_compactions: 5\n2014/09/18-09:06:53.724826 7f0826eac700                  Options.max_background_flushes: 1\n2014/09/18-09:06:53.724837 7f0826eac700                         Options.WAL_ttl_seconds: 0\n2014/09/18-09:06:53.724867 7f0826eac700                       Options.WAL_size_limit_MB: 0\n2014/09/18-09:06:53.724890 7f0826eac700             Options.manifest_preallocation_size: 4194304\n2014/09/18-09:06:53.724893 7f0826eac700                          Options.allow_os_buffer: 1\n2014/09/18-09:06:53.724897 7f0826eac700                         Options.allow_mmap_reads: 0\n2014/09/18-09:06:53.724912 7f0826eac700                        Options.allow_mmap_writes: 0\n2014/09/18-09:06:53.724944 7f0826eac700                      Options.is_fd_close_on_exec: 1\n2014/09/18-09:06:53.724948 7f0826eac700               Options.skip_log_error_on_recovery: 0\n2014/09/18-09:06:53.724958 7f0826eac700                    Options.stats_dump_period_sec: 3600\n2014/09/18-09:06:53.724974 7f0826eac700                    Options.advise_random_on_open: 1\n2014/09/18-09:06:53.724991 7f0826eac700          Options.access_hint_on_compaction_start: NORMAL\n2014/09/18-09:06:53.725002 7f0826eac700                       Options.use_adaptive_mutex: 0\n2014/09/18-09:06:53.725004 7f0826eac700                           Options.bytes_per_sync: 0\n2014/09/18-09:06:53.725790 7f0826eac700 Recovering from manifest file: MANIFEST-000526\n2014/09/18-09:06:53.725973 7f0826eac700 Options for column family \"default\":\n2014/09/18-09:06:53.726010 7f0826eac700               Options.comparator: rocksdb.InternalKeyComparator:leveldb.BytewiseComparator\n2014/09/18-09:06:53.726091 7f0826eac700           Options.merge_operator: None\n2014/09/18-09:06:53.726145 7f0826eac700        Options.compaction_filter_factory: DefaultCompactionFilterFactory\n2014/09/18-09:06:53.726197 7f0826eac700        Options.compaction_filter_factory_v2: DefaultCompactionFilterFactoryV2\n2014/09/18-09:06:53.726284 7f0826eac700         Options.memtable_factory: SkipListFactory\n2014/09/18-09:06:53.726288 7f0826eac700            Options.table_factory: BlockBasedTable\n2014/09/18-09:06:53.726292 7f0826eac700        Options.write_buffer_size: 67108864\n2014/09/18-09:06:53.726296 7f0826eac700  Options.max_write_buffer_number: 3\n2014/09/18-09:06:53.726300 7f0826eac700              Options.block_cache: 0x7f087c00bd48\n2014/09/18-09:06:53.726368 7f0826eac700   Options.block_cache_compressed: (nil)\n2014/09/18-09:06:53.726447 7f0826eac700         Options.block_cache_size: 524288000\n2014/09/18-09:06:53.726498 7f0826eac700               Options.block_size: 65536\n2014/09/18-09:06:53.726501 7f0826eac700   Options.block_restart_interval: 16\n2014/09/18-09:06:53.726557 7f0826eac700          Options.compression: 1\n2014/09/18-09:06:53.726561 7f0826eac700          Options.filter_policy: rocksdb.BuiltinBloomFilter\n2014/09/18-09:06:53.726590 7f0826eac700       Options.prefix_extractor: nullptr\n2014/09/18-09:06:53.726624 7f0826eac700    Options.whole_key_filtering: 1\n2014/09/18-09:06:53.726685 7f0826eac700             Options.num_levels: 7\n2014/09/18-09:06:53.726754 7f0826eac700        Options.min_write_buffer_number_to_merge: 1\n2014/09/18-09:06:53.726805 7f0826eac700         Options.purge_redundant_kvs_while_flush: 1\n2014/09/18-09:06:53.726808 7f0826eac700            Options.compression_opts.window_bits: -14\n2014/09/18-09:06:53.726812 7f0826eac700                  Options.compression_opts.level: -1\n2014/09/18-09:06:53.726884 7f0826eac700               Options.compression_opts.strategy: 0\n2014/09/18-09:06:53.726931 7f0826eac700      Options.level0_file_num_compaction_trigger: 4\n2014/09/18-09:06:53.726973 7f0826eac700          Options.level0_slowdown_writes_trigger: 20\n2014/09/18-09:06:53.726996 7f0826eac700              Options.level0_stop_writes_trigger: 24\n2014/09/18-09:06:53.727035 7f0826eac700                Options.max_mem_compaction_level: 2\n2014/09/18-09:06:53.727130 7f0826eac700                   Options.target_file_size_base: 2097152\n2014/09/18-09:06:53.727133 7f0826eac700             Options.target_file_size_multiplier: 1\n2014/09/18-09:06:53.727157 7f0826eac700                Options.max_bytes_for_level_base: 10485760\n2014/09/18-09:06:53.727161 7f0826eac700          Options.max_bytes_for_level_multiplier: 10\n2014/09/18-09:06:53.727165 7f0826eac700 Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2014/09/18-09:06:53.727168 7f0826eac700 Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2014/09/18-09:06:53.727170 7f0826eac700 Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2014/09/18-09:06:53.727174 7f0826eac700 Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2014/09/18-09:06:53.727195 7f0826eac700 Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2014/09/18-09:06:53.727226 7f0826eac700 Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2014/09/18-09:06:53.727231 7f0826eac700 Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2014/09/18-09:06:53.727235 7f0826eac700       Options.max_sequential_skip_in_iterations: 8\n2014/09/18-09:06:53.727240 7f0826eac700              Options.expanded_compaction_factor: 25\n2014/09/18-09:06:53.727276 7f0826eac700                Options.source_compaction_factor: 1\n2014/09/18-09:06:53.727279 7f0826eac700          Options.max_grandparent_overlap_factor: 10\n2014/09/18-09:06:53.727283 7f0826eac700                          Options.no_block_cache: 0\n2014/09/18-09:06:53.727287 7f0826eac700                        Options.arena_block_size: 6710886\n2014/09/18-09:06:53.727337 7f0826eac700                       Options.soft_rate_limit: 0.00\n2014/09/18-09:06:53.727377 7f0826eac700                       Options.hard_rate_limit: 0.00\n2014/09/18-09:06:53.727383 7f0826eac700       Options.rate_limit_delay_max_milliseconds: 1000\n2014/09/18-09:06:53.727409 7f0826eac700                Options.disable_auto_compactions: 0\n2014/09/18-09:06:53.727413 7f0826eac700          Options.purge_redundant_kvs_while_flush: 1\n2014/09/18-09:06:53.727416 7f0826eac700                     Options.block_size_deviation: 10\n2014/09/18-09:06:53.727420 7f0826eac700                           Options.filter_deletes: 0\n2014/09/18-09:06:53.727495 7f0826eac700           Options.verify_checksums_in_compaction: 1\n2014/09/18-09:06:53.727544 7f0826eac700                         Options.compaction_style: 1\n2014/09/18-09:06:53.727548 7f0826eac700  Options.compaction_options_universal.size_ratio: 1\n2014/09/18-09:06:53.727552 7f0826eac700 Options.compaction_options_universal.min_merge_width: 2\n2014/09/18-09:06:53.727555 7f0826eac700 Options.compaction_options_universal.max_merge_width: 4294967295\n2014/09/18-09:06:53.727559 7f0826eac700 Options.compaction_options_universal.max_size_amplification_percent: 200\n2014/09/18-09:06:53.727563 7f0826eac700 Options.compaction_options_universal.compression_size_percent: 4294967295\n2014/09/18-09:06:53.727601 7f0826eac700 Options.compaction_options_fifo.max_table_files_size: 1073741824\n2014/09/18-09:06:53.727623 7f0826eac700                   Options.table_properties_collectors: InternalKeyPropertiesCollectorFactory; \n2014/09/18-09:06:53.727627 7f0826eac700                   Options.inplace_update_support: 0\n2014/09/18-09:06:53.727631 7f0826eac700                 Options.inplace_update_num_locks: 10000\n2014/09/18-09:06:53.727635 7f0826eac700               Options.min_partial_merge_operands: 2\n2014/09/18-09:06:53.727638 7f0826eac700               Options.memtable_prefix_bloom_bits: 0\n2014/09/18-09:06:53.727641 7f0826eac700             Options.memtable_prefix_bloom_probes: 6\n2014/09/18-09:06:53.727668 7f0826eac700   Options.memtable_prefix_bloom_huge_page_tlb_size: 0\n2014/09/18-09:06:53.727671 7f0826eac700                           Options.bloom_locality: 0\n2014/09/18-09:06:53.727677 7f0826eac700                    Options.max_successive_merges: 0\n",
	"number": 298,
	"title": "Option to store data in block_cache even if value is retrieved from memtable"
}, {
	"body": "A small nit. If I do 'make check', it ends up creating a lot of test binaries in the root directory. They can possibly be moved into a 'bin' or 'bintests' kind of directory, so as not to clutter the root directory.\n",
	"number": 297,
	"title": "'make check' creates a lot of test binaries in the root directory"
}, {
	"body": "Hey all,\n            I am using RocksDB iterator to seek to beginning and then perform some deletes (around 32 deletes/second). The behavior we are noticing is that the seek call takes about a second to return and takes up almost all of the CPU. \n\n```\norg.apache.samza.storage.kv.RocksDbKeyValueStore.all() 96.95%\norg.rocksdb.RocksIterator.seekToFirst() 96.95%\norg.rocksdb.RocksIterator.seekToFirst0[native]() 96.95%\n```\n\nWe observed the iostats and we do not see any heavy IO operation being performed. We keep doing this operation repeatedly (for testing purposes), initially it doesn't take that long (takes about 100ms), but slowly creeps up and stays at 1s for a long time. When it hits this 1s threshold, RocksDB doesn't print any logs after this (nothing for 8 hours sometimes).\n![screen shot 2014-09-03 at 4 34 18 pm](https://cloud.githubusercontent.com/assets/1060611/4143045/08057686-33c3-11e4-93ff-52c5f3494232.png)\n\nThe first graph is the number of key-value pairs and the second graph is the time it takes to seek to the beginning.\n\nCan someone please explain this behavior ? Why would it take the iterator that long to seek to beginning ? Why do the logs start disappearing ?\n\nConfiguration:\n\n```\n2014/09/03-19:13:45.914404 7fb7b61dc700          Options.error_if_exists: 1\n2014/09/03-19:13:45.914408 7fb7b61dc700        Options.create_if_missing: 1\n2014/09/03-19:13:45.914410 7fb7b61dc700          Options.paranoid_checks: 1\n2014/09/03-19:13:45.914411 7fb7b61dc700                      Options.env: 0x7fb78f7cf340\n2014/09/03-19:13:45.914413 7fb7b61dc700                 Options.info_log: 0x7fb7b14696d0\n2014/09/03-19:13:45.914414 7fb7b61dc700           Options.max_open_files: 5000\n2014/09/03-19:13:45.914416 7fb7b61dc700       Options.max_total_wal_size: 0\n2014/09/03-19:13:45.914418 7fb7b61dc700        Options.disableDataSync: 0\n2014/09/03-19:13:45.914419 7fb7b61dc700              Options.use_fsync: 0\n2014/09/03-19:13:45.914421 7fb7b61dc700      Options.max_log_file_size: 0\n2014/09/03-19:13:45.914422 7fb7b61dc700 Options.max_manifest_file_size: 18446744073709551615\n2014/09/03-19:13:45.914424 7fb7b61dc700      Options.log_file_time_to_roll: 0\n2014/09/03-19:13:45.914425 7fb7b61dc700      Options.keep_log_file_num: 1000\n2014/09/03-19:13:45.914427 7fb7b61dc700        Options.allow_os_buffer: 1\n2014/09/03-19:13:45.914435 7fb7b61dc700       Options.allow_mmap_reads: 0\n2014/09/03-19:13:45.914436 7fb7b61dc700      Options.allow_mmap_writes: 0\n2014/09/03-19:13:45.914438 7fb7b61dc700          Options.create_missing_column_families: 0\n2014/09/03-19:13:45.914439 7fb7b61dc700                              Options.db_log_dir: \n2014/09/03-19:13:45.914441 7fb7b61dc700                                 Options.wal_dir: /Partition 12\n2014/09/03-19:13:45.914443 7fb7b61dc700                Options.table_cache_numshardbits: 4\n2014/09/03-19:13:45.914445 7fb7b61dc700     Options.table_cache_remove_scan_count_limit: 16\n2014/09/03-19:13:45.914446 7fb7b61dc700     Options.delete_obsolete_files_period_micros: 21600000000\n2014/09/03-19:13:45.914448 7fb7b61dc700              Options.max_background_compactions: 1\n2014/09/03-19:13:45.914450 7fb7b61dc700                  Options.max_background_flushes: 1\n2014/09/03-19:13:45.914451 7fb7b61dc700                         Options.WAL_ttl_seconds: 0\n2014/09/03-19:13:45.914453 7fb7b61dc700                       Options.WAL_size_limit_MB: 0\n2014/09/03-19:13:45.914454 7fb7b61dc700             Options.manifest_preallocation_size: 4194304\n2014/09/03-19:13:45.914456 7fb7b61dc700                          Options.allow_os_buffer: 1\n2014/09/03-19:13:45.914458 7fb7b61dc700                         Options.allow_mmap_reads: 0\n2014/09/03-19:13:45.914459 7fb7b61dc700                        Options.allow_mmap_writes: 0\n2014/09/03-19:13:45.914461 7fb7b61dc700                      Options.is_fd_close_on_exec: 1\n2014/09/03-19:13:45.914462 7fb7b61dc700               Options.skip_log_error_on_recovery: 0\n2014/09/03-19:13:45.914464 7fb7b61dc700                    Options.stats_dump_period_sec: 3600\n2014/09/03-19:13:45.914465 7fb7b61dc700                    Options.advise_random_on_open: 1\n2014/09/03-19:13:45.914467 7fb7b61dc700          Options.access_hint_on_compaction_start: NORMAL\n2014/09/03-19:13:45.914468 7fb7b61dc700                       Options.use_adaptive_mutex: 0\n2014/09/03-19:13:45.914470 7fb7b61dc700                           Options.bytes_per_sync: 0\n2014/09/03-19:13:45.914614 7fb7b61dc700 Creating manifest 1 \n2014/09/03-19:13:48.241838 7fb7b61dc700 Recovering from manifest file: MANIFEST-000001\n2014/09/03-19:13:48.241928 7fb7b61dc700 Options for column family \"default\":\n2014/09/03-19:13:48.241936 7fb7b61dc700               Options.comparator: rocksdb.InternalKeyComparator:leveldb.BytewiseComparator\n2014/09/03-19:13:48.241939 7fb7b61dc700           Options.merge_operator: None\n2014/09/03-19:13:48.241941 7fb7b61dc700        Options.compaction_filter_factory: DefaultCompactionFilterFactory\n2014/09/03-19:13:48.241944 7fb7b61dc700        Options.compaction_filter_factory_v2: DefaultCompactionFilterFactoryV2\n2014/09/03-19:13:48.241945 7fb7b61dc700         Options.memtable_factory: SkipListFactory\n2014/09/03-19:13:48.241947 7fb7b61dc700            Options.table_factory: BlockBasedTable\n2014/09/03-19:13:48.241949 7fb7b61dc700        Options.write_buffer_size: 67108864\n2014/09/03-19:13:48.241951 7fb7b61dc700  Options.max_write_buffer_number: 3\n2014/09/03-19:13:48.241953 7fb7b61dc700              Options.block_cache: 0x7fb7b1467758\n2014/09/03-19:13:48.241955 7fb7b61dc700   Options.block_cache_compressed: (nil)\n2014/09/03-19:13:48.241957 7fb7b61dc700         Options.block_cache_size: 134217728\n2014/09/03-19:13:48.241959 7fb7b61dc700               Options.block_size: 4096\n2014/09/03-19:13:48.241961 7fb7b61dc700   Options.block_restart_interval: 16\n2014/09/03-19:13:48.241963 7fb7b61dc700          Options.compression: 1\n2014/09/03-19:13:48.241965 7fb7b61dc700          Options.filter_policy: rocksdb.BuiltinBloomFilter\n2014/09/03-19:13:48.241967 7fb7b61dc700       Options.prefix_extractor: nullptr\n2014/09/03-19:13:48.241968 7fb7b61dc700    Options.whole_key_filtering: 1\n2014/09/03-19:13:48.241970 7fb7b61dc700             Options.num_levels: 7\n2014/09/03-19:13:48.241972 7fb7b61dc700        Options.min_write_buffer_number_to_merge: 1\n2014/09/03-19:13:48.241974 7fb7b61dc700         Options.purge_redundant_kvs_while_flush: 1\n2014/09/03-19:13:48.241975 7fb7b61dc700            Options.compression_opts.window_bits: -14\n2014/09/03-19:13:48.241977 7fb7b61dc700                  Options.compression_opts.level: -1\n2014/09/03-19:13:48.241979 7fb7b61dc700               Options.compression_opts.strategy: 0\n2014/09/03-19:13:48.241980 7fb7b61dc700      Options.level0_file_num_compaction_trigger: 4\n2014/09/03-19:13:48.241982 7fb7b61dc700          Options.level0_slowdown_writes_trigger: 20\n2014/09/03-19:13:48.241984 7fb7b61dc700              Options.level0_stop_writes_trigger: 24\n2014/09/03-19:13:48.241986 7fb7b61dc700                Options.max_mem_compaction_level: 2\n2014/09/03-19:13:48.241987 7fb7b61dc700                   Options.target_file_size_base: 2097152\n2014/09/03-19:13:48.241989 7fb7b61dc700             Options.target_file_size_multiplier: 1\n2014/09/03-19:13:48.241991 7fb7b61dc700                Options.max_bytes_for_level_base: 10485760\n2014/09/03-19:13:48.241992 7fb7b61dc700          Options.max_bytes_for_level_multiplier: 10\n2014/09/03-19:13:48.241992 7fb7b61dc700          Options.max_bytes_for_level_multiplier: 10\n2014/09/03-19:13:48.241994 7fb7b61dc700 Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2014/09/03-19:13:48.241996 7fb7b61dc700 Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2014/09/03-19:13:48.241998 7fb7b61dc700 Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2014/09/03-19:13:48.242000 7fb7b61dc700 Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2014/09/03-19:13:48.242001 7fb7b61dc700 Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2014/09/03-19:13:48.242003 7fb7b61dc700 Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2014/09/03-19:13:48.242005 7fb7b61dc700 Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2014/09/03-19:13:48.242006 7fb7b61dc700       Options.max_sequential_skip_in_iterations: 8\n2014/09/03-19:13:48.242008 7fb7b61dc700              Options.expanded_compaction_factor: 25\n2014/09/03-19:13:48.242010 7fb7b61dc700                Options.source_compaction_factor: 1\n2014/09/03-19:13:48.242012 7fb7b61dc700          Options.max_grandparent_overlap_factor: 10\n2014/09/03-19:13:48.242013 7fb7b61dc700                          Options.no_block_cache: 0\n2014/09/03-19:13:48.242035 7fb7b61dc700                        Options.arena_block_size: 6710886\n2014/09/03-19:13:48.242038 7fb7b61dc700                       Options.soft_rate_limit: 0.00\n2014/09/03-19:13:48.242044 7fb7b61dc700                       Options.hard_rate_limit: 0.00\n2014/09/03-19:13:48.242047 7fb7b61dc700       Options.rate_limit_delay_max_milliseconds: 1000\n2014/09/03-19:13:48.242048 7fb7b61dc700                Options.disable_auto_compactions: 0\n2014/09/03-19:13:48.242050 7fb7b61dc700          Options.purge_redundant_kvs_while_flush: 1\n2014/09/03-19:13:48.242052 7fb7b61dc700                     Options.block_size_deviation: 10\n2014/09/03-19:13:48.242053 7fb7b61dc700                           Options.filter_deletes: 0\n2014/09/03-19:13:48.242055 7fb7b61dc700           Options.verify_checksums_in_compaction: 1\n2014/09/03-19:13:48.242057 7fb7b61dc700                         Options.compaction_style: 1\n2014/09/03-19:13:48.242058 7fb7b61dc700  Options.compaction_options_universal.size_ratio: 1\n2014/09/03-19:13:48.242060 7fb7b61dc700 Options.compaction_options_universal.min_merge_width: 2\n2014/09/03-19:13:48.242061 7fb7b61dc700 Options.compaction_options_universal.max_merge_width: 4294967295\n2014/09/03-19:13:48.242063 7fb7b61dc700 Options.compaction_options_universal.max_size_amplification_percent: 200\n2014/09/03-19:13:48.242065 7fb7b61dc700 Options.compaction_options_universal.compression_size_percent: 4294967295\n2014/09/03-19:13:48.242066 7fb7b61dc700 Options.compaction_options_fifo.max_table_files_size: 1073741824\n2014/09/03-19:13:48.242070 7fb7b61dc700                   Options.table_properties_collectors: InternalKeyPropertiesCollectorFactory; \n2014/09/03-19:13:48.242072 7fb7b61dc700                   Options.inplace_update_support: 0\n2014/09/03-19:13:48.242073 7fb7b61dc700                 Options.inplace_update_num_locks: 10000\n2014/09/03-19:13:48.242075 7fb7b61dc700               Options.min_partial_merge_operands: 2\n2014/09/03-19:13:48.242076 7fb7b61dc700               Options.memtable_prefix_bloom_bits: 0\n2014/09/03-19:13:48.242078 7fb7b61dc700             Options.memtable_prefix_bloom_probes: 6\n2014/09/03-19:13:48.242079 7fb7b61dc700   Options.memtable_prefix_bloom_huge_page_tlb_size: 0\n2014/09/03-19:13:48.242081 7fb7b61dc700                           Options.bloom_locality: 0\n2014/09/03-19:13:48.242082 7fb7b61dc700                    Options.max_successive_merges: 0\n```\n\nThe last few minutes before RocksDB stops printing log:\n\n```\n2014/09/03-20:46:31.916208 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916168) [default] Universal: Possible candidate file 353[0].\n2014/09/03-20:46:31.916209 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916169) [default] Universal: Skipping file 353[0] with size 45765666 (compensated size 70532946) 0\n2014/09/03-20:46:31.916210 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916170) [default] Universal: Possible candidate file 351[1].\n2014/09/03-20:46:31.916212 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916170) [default] Universal: Skipping file 351[1] with size 274767888 (compensated size 408189024) 0\n2014/09/03-20:46:31.916213 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916171) [default] Universal: Possible candidate file 334[2].\n2014/09/03-20:46:31.916215 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916173) [default] Universal: Skipping file 334[2] with size 735666259 (compensated size 738294227) 0\n2014/09/03-20:46:31.916216 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916173) [default] Universal: Possible candidate file 300[3].\n2014/09/03-20:46:31.916217 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916174) [default] Universal: Skipping file 300[3] with size 782344698 (compensated size 785612346) 0\n2014/09/03-20:46:31.916219 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916175) [default] Universal: Possible candidate file 252[4].\n2014/09/03-20:46:31.916220 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916175) [default] Universal: Skipping file 252[4] with size 1015153406 (compensated size 1023345694) 0\n2014/09/03-20:46:31.916221 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916176) [default] Universal: Possible candidate file 206[5].\n2014/09/03-20:46:31.916223 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916176) [default] Universal: Skipping file 206[5] with size 4294684948 (compensated size 4294684948) 0\n2014/09/03-20:46:31.916224 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916177) [default] Universal: Possible candidate file 353[0].\n2014/09/03-20:46:31.916226 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916181) [default] Universal: Picking file 353[0] with size 45765666 (compensated size 70532946)\n2014/09/03-20:46:31.916227 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916182) [default] Universal: Picking file 351[1] with size 274767888 (compensated size 408189024)\n2014/09/03-20:46:31.916228 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916183) [default] Universal: compacting for file num\n2014/09/03-20:46:31.916230 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916188) [default] Compacting 2@0 + 0@0 files, score 1.50 slots available 0\n2014/09/03-20:46:31.916231 7fb779c1c700 (Original Log Time 2014/09/03-20:46:31.916194) [default] Compaction start summary: Base version 194 Base level 0, seek compaction:0, inputs: [353(43MB) 351(262MB)]\n2014/09/03-20:46:36.894985 7fb779c1c700 Table was constructed:\n  [basic properties]: # data blocks=107682; # entries=467217; raw key size=5606604; raw average key size=12.000000; raw value size=442350465; raw average value size=946.777333; data block size=316861421; index block size=3138463; filter block size=1703550; (estimated) table size=321703434; filter policy name=rocksdb.BuiltinBloomFilter; \n  [user collected properties]: kDeletedKeys=94272; \n2014/09/03-20:46:41.655969 7fb779c1c700 [default] Generated table #354: 467217 keys, 320540837 bytes\n2014/09/03-20:46:41.656873 7fb779c1c700 (Original Log Time 2014/09/03-20:46:41.656388) [default] Compacted 2@0 + 0@0 files => 320540837 bytes\n2014/09/03-20:46:41.656877 7fb779c1c700 (Original Log Time 2014/09/03-20:46:41.656834) [default] compacted to: files[5 0 0 0 0 0 0], 65.8 MB/sec, level 0, files in(2, 0) out(1) MB in(305.7, 0.0) out(305.7), read-write-amplify(2.0) write-amplify(1.0) OK\n2014/09/03-20:46:41.677848 7fb779c1c700 Delete /Partition 12/000349.log type=0 #349 -- OK\n2014/09/03-20:46:41.690663 7fb779c1c700 Delete /Partition 12/000353.sst type=2 #353 -- OK\n2014/09/03-20:46:41.783902 7fb779c1c700 Delete /Partition 12/000351.sst type=2 #351 -- OK\n```\n",
	"number": 261,
	"title": "iterator.seekToFirst() high latency"
}, {
	"body": "Trying to manually (offline) compact a fairly large database, I ran into 2 issues:\n\nFirst: compaction has no indication about how far along it is (I could take seconds, minutes, hours or days to run this I guess?)\n\nSecond: Not all database options are exposed, neither are options for running compactions with as many threads as possible. Maybe some of these options could be actually read from the database itself instead of me having to specify it explicitly. I feel like I'm risking to mess with the application that uses the database afterwards, which might for example expect a slightly different file size and then will again have to re-organize everything killing all the benefits of compacting offline.\n\nThe reason I did this in the first place was that it seemed the application could not catch up with compactions (I increased the file size and it took/takes forever to migrate to this different format) so I thought of splitting the task a bit, letting the database optimize itself first and then start writing to it again when everything is fine.\n",
	"number": 192,
	"title": "ldb tool gives no progress indication when compacting"
}, {
	"body": "RocksDB has a very big number of different configuration options that we added to make it flexible and suitable for various workloads and environments (in-memory, SSD, point lookups, etc). Configuring all these options to get the most out of RocksDB is not easy and requires very high familiarity with RocksDB internals.\n\nOn the other hand, RocksDB's default options are inherited from LevelDB and not suitable for any high-performance server workload.\n\nWe should invest some time in making RocksDB's options easier to configure and understand. For starting proposals, see: https://github.com/facebook/rocksdb/wiki/Proposals-on-Improving-Rocksdb's-Options\n\nWe're welcoming all contributions and thoughts in this effort.\n",
	"number": 172,
	"title": "Simplify RocksDB configuration"
}, {
	"body": "Hi,\n\nIt would be great to have a binary distribution available, that would include multi-platform builds (linux, macosx) and bindings (java, etc).\n\nEarlier, I have made an attempt on an automated build VM environment setup using Vagrant / Ansible: https://github.com/phraktle/rocksdb-build \u2013 this might serve as useful starting point, but I don't have time to maintain this.\n\nThanks,\n  Viktor\n",
	"number": 144,
	"title": "binary distributions"
}, {
	"body": "**This PR probably isn't ready to merge yet** (see FIXMEs) but I wanted to solicit any early comments/feedback.\n\nAdds a read option for RocksDB iterators to schedule background thread tasks to \"prefetch\" upcoming data blocks. This can significantly speed up forward sequential scans by overlapping disk I/O and decompression with the reader's activities. The benefit is quite marked on large, universally-compacted databases residing on rotating disk(s), since scanning these incurs a lot of seeks that currently block iterators. The new behavior must be enabled by creating the iterator with ReadOptions.prefetch = true.\n",
	"number": 63,
	"title": "Background data block prefetching in database iterators"
}]